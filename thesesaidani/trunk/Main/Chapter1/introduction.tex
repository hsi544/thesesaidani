%Les logiciels ont été conçus historiquement pour une exécution séquentielle. Les programmes devaient s'exécuter sur une seule machine, contenant une seule unité de traitement centrale (\emph{CPU}) \nomenclature{CPU}{Central Processing Unit} et le problème est décomposé en une suite d'instructions qui sont exécutées les unes après les autres. Ainsi, une seule instruction peut être exécutée à la fois. Le calcul parallèle est par opposition à la précédente approche, l'utilisation simultanée de plusieurs ressources de calcul pour résoudre un problème. Un logiciel peut ainsi s'exécuter sur plusieurs \emph{CPU}s. Le problème est décomposé en plusieurs parties qui peuvent être résolues de manière concurrente. Ces parties sont à leur tour décomposées en plusieurs instructions et chaque paquet d'instructions s'exécute de manière indépendante l'un de l'autre. Les ressources de calcul incluent une seule machine avec plusieurs processeurs, un nombre arbitraire de machines connectées via un réseau ou alors une combinaison des deux. Une bonne partie des problèmes de calcul intensif possèdent certaines caractéristiques qui en font de bons candidats à la parallélisation. Parmi ces caractéristiques : la possibilités de les décomposer en plusieurs sous-problèmes qui peuvent être résolus simultanément et la possibilités d'être résolus en moins de temps avec plusieurs ressources qu'avec une seule. Le calcul parallèle était auparavant, réservé exclusivement à la modélisation de problèmes et de phénomènes scientifiques provenant de la réalité tels que l'environnement, la physique nucléaire, les biotechnologies, la géologie et les mathématiques. A ce jour, le calcul parallèle s'est démocratisé grâce notamment à l'évolution fulgurante de la technologie des semi-conducteurs qui a rendu les plate-formes haute performance plus accessibles. On peut citer des applications comme les bases de données, la prospection pétrolière, les moteurs de recherche, la modélisation financière, les technologies de diffusion multimédia et les applications graphiques et de réalité augmentée.
%\section*{Concepts généraux}
%\subsection*{Architecture de \emph{Von Neumann}}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=0.5\columnwidth]{Chapter1/figures/vonneumann}
%	\caption{Architecture de \emph{Von Neumann}}
%	\label{figvonneumann}
%\end{figure}
%Ce modèle fut inventé par le mathématicien hongrois \emph{John von Neumann} qui a posé les premières bases de la conception d'un ordinateur dans son papier de 1945 \cite{vonneumann}. A partir de ce moment, la majorité des ordinateurs ont été conçus sur ces bases. L'architecture \emph{von Neumann} \ref{figvonneumann} est constituée de 4 composants principaux: une mémoire, une unité de contrôle, une unité arithmétique et logique (\emph{ALU}) des entrées/sorties (\emph{I/O})\nomenclature{I/O}{Input/Output}. La mémoire à accès aléatoire (\emph{RAM}) en lecture/écriture est utilisée pour stocker les instructions ainsi que les données. L'unité de contrôle va chercher les instructions ou les données de la mémoire, décode les instructions et coordonne séquentiellement les opérations afin d'accomplir la tâche programmée. L'ALU effectue les opérations arithmétiques de base. Les I/O font l'interface avec l'utilisateur humain.
%\subsection*{Classification de Flynn des machines parallèles}
%Il existe plusieurs manières de classer les machines parallèles. Toutefois, il existe une classification qui est largement utilisée depuis 1966 et qui est celle de \emph{Flynn} \cite{flynn} (\emph{Flynn's Taxonomy}). Cette classification distingue les architectures parallèles selon deux paramètres indépendants qui sont les instructions et les données : chacun de ces deux paramètres peut avoir deux états possibles \emph{Single} ou \emph{Multiple}. Ainsi le tableau \ref{flynn} illustre la classification de Flynn.
%\begin{table}
%\centering
%\begin{tabular}{|c||c|}
%\hline
%\rowcolor{medium-gray}\textbf{SISD} &  \textbf{SIMD}\nomenclature{SIMD}{Single Instruction Multiple Data} \\
%\hline
%Single Instruction Single Data & Single Instruction Multiple Data\\
%\hline
%\hline
%\rowcolor{medium-gray}\textbf{MISD} &  \textbf{MIMD} \\
%\hline
%Multiple Instruction Single Data& Multiple Instruction Multiple Data\\
%\hline
%\end{tabular}
%\caption{Classification de \emph{Flynn} des machines parallèles}
%\label{flynn}
%\end{table}

%\subsubsection*{Single instruction, single data (SISD)}
%Une machine séquentille qui ne peut exécuter qu'un seul flux d'instructions en un cycle d'horloge \emph{CPU}. De plus, un seul flux de données est utilisé comme entrée en un cycle d'horloge. L'exécution du programme y est déterministe et il constitue le type de machines à la fois le plus ancien est le plus répandu de nos jours.
%\subsubsection*{Single instruction, multiple data (SIMD)}
%C'est un type de machines parallèles dont les processeurs exécutent la même instruction en un cycle d'horloge donné. Cependant, chaque unité de traitement peut opérer sur un élément de données différent. Ce type de machines est bien taillé pour des problèmes réguliers tels que le traitement d'images et le rendu graphique. L'exécution des programmes y est synchrone et déterministe. Deux variantes de ces machines existent : 
%\begin{itemize}
%\item Processor Arrays: Connection Machine CM-2, MasPar MP-1 \& MP-2, ILLIAC IV
%\item Vector Pipelines: IBM 9000, Cray X-MP, Y-MP \& C90, Fujitsu VP, NEC SX-2, Hitachi S820, ETA10 
%\end{itemize}
%De plus, la majorité des processeurs des stations de travail actuelles et des unités de traitement graphiques, comportent une unité de traitement spécialisée SIMD, on parle alors de \emph{SWAR} (\emph{SIMD Within A Register}).\nomenclature{SWAR}{SIMD Within A Register}

%\subsubsection*{Multiple instruction, single data (MISD)}
%Un seul flux de données alimente plusieurs unités de traitement et chaque unité de traitement opère sur les données de manière indépendante grâce à un flot d'instructions indépendantes. On ne connaît pas de machines de ce type qui a été conçue.
%\subsubsection*{Multiple instruction, multiple data (MIMD)}
%C'est actuellement le type le plus commun de machines parallèles. Chaque processeur de ces machines peut exécuter un flux d'instructions différent et peut opérer sur un flux de données différent. L'exécution peut être synchrone ou asynchrone, déterministe ou non-déterministe. On peut citer les \emph{Supercomputers} actuels, les clusters de machines parallèles mis en réseau, les grilles de calculs, les multi-processeurs SMP (\emph{Symetric Multi-Processor}) \nomenclature{SMP}{Symetric Multi-Processor}
% et les processeurs multi-core. De plus, plusieurs de ces machines contiennent des unités de traitement SIMD.

%\section*{Architectures mémoire des machines parallèles}
%Dans la suite nous donnons une classification des machines parallèles selon le type de leur hiérarchie mémoire. Cette classification permet d'une part de distinguer les machines parallèles d'un autre point de vue que celui du CPU et permet également de mieux comprendre les motivations des modèles de programmation pour les machines parallèles.
%\subsection*{Les machines parallèles à mémoire partagée}
%Il existe plusieurs variantes de ces machines mais toutes partagent une propriété commune qui est la capacité de tous les processeurs d'accéder à toute la mémoire comme un espace d'adressage global. Ainsi, plusieurs processeurs peuvent opérer d'une manière indépendante mais partagent la même ressource mémoire. Un changement opéré par un processeurs dans un emplacement mémoire est visible à tous les autres processeurs. Cette classe de machines peut être divisée en deux sous-classes basées sur les temps d'accès à la mémoire : UMA et NUMA.
%\subsubsection*{Uniform memory access (UMA)}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=0.7\columnwidth]{Chapter1/figures/SMUMA}
%	\caption{Machine parallèle à mémoire partagée UMA}
%	\label{figSMUMA}
%\end{figure}
%Ce sont principalement les machines de type SMP qui possèdent plusieurs processeurs identiques et qui peuvent accéder de manière égale et en un temps identique à la mémoire. Elles sont parfois appelées CC-UMA - Cache Coherent UMA. La cohérence de cache signifie que si un processeur met à jour un emplacement de la mémoire tous les autres processeurs sont au courant de ce changement. Cette fonctionnalité est assurée au niveau matériel.
%\subsubsection*{Non-uniform memory access (NUMA)}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=0.7\columnwidth]{Chapter1/figures/SMNUMA}
%	\caption{Machine parallèle à mémoire partagée NUMA}
%	\label{figSMNUMA}
%\end{figure}
%Ce type de machines est souvent conçu en connectant deux ou plusieurs SMPs. Un SMP peut avoir un accès direct à la mémoire d'un autre SMP. Le temps d'accès à une mémoire donnée n'est pas égal pour tous les processeurs et lorsque un noeud est traversé, l'accès est plus lent. Si la cohérence de cache est garantie on parle alors de CC-NUMA.

%\subsubsection*{Avantages et inconvénients}
%Parmi les avantages de ce type d'architectures mémoire est une perspective simplifiée de la mémoire du point de vue du programmeur. Le partage des données entre les tâches est à la fois rapide et uniforme. Le premier inconvénient est le manque de mise à l'échelle (\emph{scalability}) entre la mémoire et les CPUs. Le fait d'augmenter le nombre de CPUs augmente le trafic sur le bus mémoire et provoque un goulot d'étranglement et la gestion de la cohérence devient de plus en plus complexe. Le programmeur est responsable de la synchronisation des tâches qui garantit un accès correcte à la mémoire globale. Par conséquent, la conception de machine parallèles à mémoire partagée avec de plus en plus de processeurs devient difficile et coûteuse. 
%\subsection*{Les Machines parallèles à mémoire distribuée}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=0.7\columnwidth]{Chapter1/Figures/DMEM}
%	\caption{Machine parallèle à mémoire distribuée}
%	\label{figDMEM}
%\end{figure}
%Comme les machines à mémoire partagée, les machines à mémoire distribuée varient mais elles partagent tout de même un point commun : elles requièrent un réseau de communication pour connecter les mémoires des processeurs. Les différents processeurs possèdent leur propre mémoire locale. Les adresses mémoire d'un processeur donné ne correspondent pas à celles d'un autre et par conséquent le concept de mémoire globale n'existe pas. Puisque chaque processeur possède sa propre mémoire privée il opère de manière indépendante. En effet, chaque changement opéré sur sa mémoire locale n'a aucun effet sur la mémoire des autres processeurs ce qui exclue le concept de cohérence de cache. Lorsqu'un processeur à besoin des données contenues dans la mémoire d'un autre processeur, le programmeur est en charge de définir quand et comment les données sont transférées. Ce dernier est aussi responsable de la synchronisation.  
%\subsubsection*{Avantages et inconvénients}
%L'avantage majeur de ce type d'architectures est le fait que la mémoire soit \emph{scalable} avec le nombre de processeurs. En effet, la taille de la mémoire croit proportionnellement avec le nombre de processeurs. Chaque processeur peut aussi accéder rapidement à sa mémoire locale sans interférence et sans engendrer de surcout du au maintien de la cohérence de cache. Le principal inconvénient de ce type d'architectures mémoire et la gestion explicite par le logiciel des communications entre les processeurs. Les accès à la mémoire se font souvent à des temps non-uniformes et la présence de plusieurs espaces d'adressage rend complexe l'adaptation de programmes écrits pour une mémoire partagée.
%\subsection*{Les Machines parallèles à mémoire hybride}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=0.7\columnwidth]{Chapter1/figures/HMEM}
%	\caption{Machine parallèle à mémoire hybride}
%	\label{figHMEM}
%\end{figure}
%Les machines les plus rapides du monde emploient des architectures mémoire dites hybrides qui regroupent les deux types précédents: partagée et distribuée. La composante mémoire partagée est souvent une machine SMP. La composante distribuée quant à elle consiste en la mise en réseau de plusieurs machines SMP. Les différents SMPs ne peuvent adresser que leur propre mémoire et le transfert de données entre deux SMPs requiert des communications au travers du réseau. Selon le niveau dans lequel on se trouve, ce type de machines possède les inconvénients et avantages des deux précédentes architectures mémoire.

%\paragraph*{Récapitulatif sur les architectures parallèles}
%Au vu de la classification décrite dans ce qui précède. Il existe plusieurs types de machines parallèles. Ces machines peuvent être classées selon différents critères. Le premier étant la nature du parallélisme : instructions, données ou tâches. Le second critère de classification étant la nature de la hiérarchie mémoire :  partagée, distribuée ou hybride. Ces critères de distinction ne sont pas uniquement faits pour établir un catalogue d'architectures parallèles mais permettent entre autres d'établir une adéquation entre un domaine d'application ou  un modèle de programmation et ses outils associés et une machine parallèle.

%\section*{Modèles de programmation parallèle}
%Il existe plusieurs modèles de programmation pour les machines parallèles. Ces modèles existent à un niveau d'abstraction au dessus de l'architecture matérielle et de celle de la mémoire. Même si à première vue les modèles de programmation sont intimement liés à l'architecture de la machine ils sont supposés pouvoir être implémentés sur n'importe quelle machine parallèle quelque en soient les caractéristiques. Il n'existe pas de modèle de programmation idéal mais certains modèles de programmation sont bien adaptés pour une application données sur une machine donnée. Dans la suite nous décrivons les principaux modèles de programmation parallèles.
%\subsection*{Le Modèle \emph{shared memory}}
%Dans ce modèle de programmation les tâches partagent un espace d'adressage commun sur lequel ils peuvent lire et écrire des données de manière asynchrone. Plusieurs mécanismes, tels que les \emph{locks} et les sémaphores peuvent être utilisés pour contrôler l'accès à la mémoire partagée. Ce modèle de programmation est simplifié du point de vue de l'utilisateur car il n'y a pas de notion d'appartenance des données à une tâche ce qui évite les communications explicites pour transférer des données d'une tâche à une autre. Toutefois, en terme de performances ce dernier point constitue un inconvénient car il engendre un surcout d'accès à la mémoire,  de rafraichissement de cache et de trafic sur le bus lorsque plusieurs processeurs utilisent les mêmes données.
%Les implémentations de ce modèle sur les machines à mémoire partagée se résument au compilateur natif qui traduit les variables du programme en adresses mémoire globales. Il n'existe cependant pas d'implémentation de ce modèle sur des machines à mémoire distribuée.
%\subsection*{Le Modèle de programmation par \emph{threads}}
%Dans le modèle de programmation par threads, un seul \emph{process} peut avoir des chemins d'exécution multiples et concurrents. On peut assimiler ce concept à un programme principal qui inclue un certain nombre de sous-routines. Le programme principal est ordonnancé pour être exécuté par le système d'exploitation, et il acquière toutes les ressources système nécessaires à son exécution. Il effectue alors un ensemble d'instructions en série et crée un certain nombre de tâches (\emph{threads}) qui peuvent être ordonnancées et exécutées par l'OS de manière concurrente. Chaque \emph{thread} possède ses données locales mais partage également les ressources du programme principal avec les autres \emph{threads}. Chaque \emph{thread} possède un accès à la mémoire globale car il partage l'espace d'adressage du programme principal. La charge du travail d'un \emph{thread} peut être considérée comme une sous-routine du programme principal mais qui peut s'exécuter en parallèle d'un autre \emph{thread}. Les \emph{threads} communiquent entre eux via la mémoire globale ce qui nécessite des opérations de synchronisation afin de garantir l'exclusivité de l'accès à un emplacement donnée à un instant donné pour un seul \emph{thread}. Les \emph{threads} ont une durée de vie variable et peuvent être crées et détruits tout au long du déroulement du programme. Le modèle de programmation par \emph{thread} est souvent associé aux machines à mémoire partagée. Les implémentations des \emph{threads} comportent en général une librairie de fonctions ou alors une série de directives enfouis dans le code parallèle. Dans les deux cas l'utilisateur est responsable de la définition du parallélisme. Il existe plusieurs implémentations des \emph{threads}, et la plupart des  constructeurs ont développé leur propre version ce qui a affecté la portabilité des codes parallèles. Cependant, un effort de standardisation à donné naissance à deux implémentations qui sont devenues le standard de nos jours.
%\paragraph*{Les \emph{threads} POSIX}\nomenclature{POSIX}{Portable Operating System Interface}
%Ils sont basés sur une librairie de programmation parallèle et spécifiés par le standard \emph{IEEE POSIX 1003.1c standard (1995)} \cite{pthreads_std}. Ils sont implémentés uniquement en langage C et plus connus sous le nom de \emph{Pthreads}. Le parallélisme y est explicite et l'interface bas-niveau force le programmeur à donner beaucoup d'attention au détails.
%\subsubsection*{OpenMP}\nomenclature{OpenMP}{Open Multiprocessing}
%C'est un modèle de programmation basé sur des directives de compilation et il peut être directement utilisé sur du code série. Ce standard à été défini par un consortium de vendeurs de processeurs et de logiciel. L'API \nomenclature{API}{Application Programming Interface} Fortran à été délivrée en 1997 alors que l'API C/C++ ne l'a été qu'une année plus tard. C'est une API portable et multi-plateforme et est très simple d'utilisation.
%OpenMP est de nos jours largement utilisé sur les architectures parallèles les plus répandues, à savoir les processeurs multi-coeurs symétriques à mémoire partagée. Celui-ci à remplacé graduellement les threads POSIX, car il est plus simple à utiliser. De plus, les performances en OpenMP sont très bonnes au niveau du passage à l'échelle. Toutefois, le compilateur utilisé est le facteur principal qui détermine la performance d'une implémentation parallèle en OpenMP.
%\subsection*{Le Modèle de programmation par passage de message}
%Dans ce modèle, la programmation parallèle se fait par passage de messages. Un ensemble de tâches utilisent leur propre mémoire locale durant le calcul. Plusieurs tâches peuvent résider sur la même machine physique ou alors sur un nombre arbitraire de machines. Les tâches échangent des données au travers des communications en envoyant et recevant des messages. Les transferts de données requièrent des opérations coopératives pour être effectuées par chaque \emph{process}. Par exemple, une opération \emph{send} doit avoir une opération duale \emph{receive}. Les implémentations du \emph{Message Passing} prennent la forme d'une librairie de sous-routines et le programmeur est responsable de la détection du parallélisme. Comme pour toute librairie, plusieurs versions ont été développées, ce qui a provoqué des problèmes de compatibilité. En 1992 le \emph{MPI Forum} a vu le jour dans le but de standardiser les implémentations du \emph{Message Passing} et a délivré deux standard MPI \cite{mpistand} en 1994 et MPI-2 en 1996. Des nos jours MPI est le modèle de programmation le plus utilisé pour le \emph{Message Passing}. Dans les implémentations MPI \nomenclature{MPI}{Message Passing Interface} sur des architectures à mémoire partagée les communications réseaux sont tout simplement remplacées par des copies mémoire.

%\subsection*{Le Modèle \emph{data parallel}}
%Ce modèle est basé sur le parallélisme de données qui concentre le travail en parallèle sur un ensemble de données contenues dans un tableau ou dans une structure de données à plusieurs dimensions. Un ensemble de tâches travaillent collectivement sur la même structure de données mais chaque tâches opère sur une partition différente de cette structure. Les tâches effectuent toutes la même opération sur leur partition de données. Sur les architectures à mémoire partagée toutes les tâches peuvent avoir accès à la structure de données via la mémoire globale. Par contre lorsque l'architecture mémoire est distribuée les données sont divisées en morceaux qui résident dans la mémoire locale de chaque tâche. La programmation avec ce modèle se fait en général en écrivant du code avec des constructions de parallélisme de données. Ces dernières peuvent avoir la forme d'appel à des fonction d'une librairie ou à des directives reconnues par un compilateur \emph{data parallel}. Les implémentation de ce modèle sont souvent des extensions ou de nouveaux compilateurs on peut citer les compilateur \texttt{Fortran} (\texttt{F90 et F95}) et leur extension High Performance Fortran (\emph{HPF}) qui supportent la programmation \emph{data parallel}. \emph{HPF} inclue des directives qui contrôlent la distribution des données, des assertions qui peuvent améliorer l'optimisation du code généré ainsi que des construction \emph{data parallel}. Les implémentations sur les architectures mémoire distribuées de ce modèle sont sous forme d'un compilateur qui convertit le code standard en code \emph{Message Passing} (MPI) qui distribue ainsi les données sur les différents processeurs et tout cela de manière transparente du point de vue de l'utilisateur.

%\subsection*{Modèle de programmation de calcul par flux}
%Ce modèle appelé communément \emph{stream computing} est un modèle basé sur le parallélisme de données. Un même noyau de calcul \emph{kernel} est appliqué à un ensemble de données. Ce modèle est le modèle dominant sur les unités de calcul graphique. 
%\subsection*{Autres modèles}
%D'autres modèles existent et existeront dans le futur proche en plus de ceux mentionnés auparavant. On peut en mentionner trois :
%\subsubsection*{Modèle hybride}
%Dans ce modèle deux ou plusieurs modèles sont combinés. On peut citer par exemple la combinaison de \emph{MPI} avec les \emph{Pthreads}\nomenclature{Pthreads}{POSIX threads} ou avec \emph{OpenMP}. Ainsi, différents niveaux de parallélisme sont gérés, par exemple un réseau de SMPs. On peut citer également la combinaison de \emph{HPF} avec \emph{MPI} pour le même type de configuration. Celui-ci combine le parallélisme de données et de tâches.

%\subsubsection{Modèle single program multiple data}
%Le modèle \emph{SPMD}\nomenclature{SPMD}{Single Program Multiple Data} est un modèle haut niveau qui peut être construit sur la base d'une combinaison des modèles cités précédemment. Un seul programme est exécuté par toutes les tâches simultanément. A n'importe quel instant les tâches peuvent exécuter des instructions différentes ou similaires du même programme.Un programme \emph{SPMD} peut toutefois contenir des branchement qui permettent à une tâche de n'exécuter qu'une portion du code et toutes les tâches peuvent utiliser différentes données.

%\subsubsection{Modèle multiple program multiple data}
%Tout comme le modèle \emph{SPMD}, le modèle \emph{MPMD} est haut-niveau et peut englober l'ensemble des modèles citées précédemment. Les programmes \emph{MPMD} ont typiquement plusieurs objets exécutables. Lors de l'exécution parallèle du programme une tâche peut exécuter le même programme ou un programme différent et toutes les tâches peuvent utiliser des données différentes.

%\paragraph*{Récapitulatif sur les modèles de programmation}
%Dans ce qui précède, nous avons cités les principaux modèles de programmation parallèles. Ces derniers diffèrent par la nature du parallélisme qui est exploité :  données ou tâches. Les modèles sont aussi fortement couplés à l'architecture mémoire de la machine parallèle. Enfin, l'adéquation du modèle avec l'architecture sont déterminants pour la performance.  

%\section*{Parallélisation}
%Les architectures parallèles et les modèles de programmation associés étant définis. La question qui se pose alors est celle du choix à la fois de l'architecture et du modèle de programmation adéquats pour la mise en oeuvre d'une application parallèle donnée. L'efficacité des outils automatiques de parallélisation dépend souvent de plusieurs facteurs, parmi lesquelles : les caractéristiques de l'architecture matérielle et de la hiérarchie mémoire ainsi que la nature des algorithmes qui forment l'application à paralléliser.  L'utilisation d'outils automatiques n'est pas toujours efficace, il est alors parfois nécessaire de gérer manuellement le parallélisme et les optimisations qui lui sont associées. Deux choix se présentent alors : la parallélisation manuelle ou la parallélisation automatique.

%\subsection*{Parallélisation manuelle}
%Elle permet un contrôle précis de la performance, et une grande flexibilité en termes de schéma de parallélisation possible (différents modèles de calcul). Par contre, les temps de développement sont importants, que cela soit pour la mise en oeuvre, le débogage ou la maintenance de l'application. Les erreurs sont parfois très difficiles à trouver, et le processus d'optimisation est souvent itératif.

%\subsection*{Parallélisation automatique}
%L'automatisation du processus de parallélisation de code est un problème ouvert, et les efforts effectués en la matière sont de plus en plus nombreux, en particulier avec l'avènement des nouvelles architectures parallèles et leur démocratisation. On peut trouver deux formes d'outils automatiques de parallélisation. Certains outils sont entièrement automatiques : ils prennent en entrée un code source série et détectent automatiquement le parallélisme potentiel, ils génèrent en suite le code parallèle correspondant. D'autres outils sont semi-automatiques car l'utilisateur indique les portions de codes parallélisables, c'est le cas par exemple d'OpenMP via les directives de compilation.
%L'avantage des approches automatiques est avant tout la rapidité de mise en oeuvre d'une solution à base de calcul parallèle, d'autant plus que dans la majorité des cas, le code original est directement utilisable. Par contre, le contrôle est beaucoup moins précis qu'avec une version entièrement manuelle. Il peut y avoir par exemple des écarts entre les résultats numériques notamment sur les arrondis pour les calculs en virgule flottante. De plus, les modèles de programmation dans ce cas ne permettent pas une grande flexibilité dans le choix des schémas de parallélisation. Dans certains cas le gain de performance peut être médiocre, et on peut même observer une baisse de performances par rapport à la version originale. Enfin, ce genre d'outils n'est généralement efficace que sur des portions de code facilement exploitables comme les boucles. 
%Dans la suite nous allons présenter les différentes étapes de mise en ½uvre d'un code parallèle manuellement, les étapes en question vont de la détermination de l'opportunité de parallélisation jusqu'à la mise en oeuvre et l'évaluation du gain ainsi obtenu.

%\subsection*{Méthodologie de parallélisation manuelle}

%\subsubsection*{Comprendre le problème}
%Avant même de commencer à développer la version parallèle d'une application, la première question qui se pose et celle de la faisabilité d'une telle solution. En effet, il existe certains problèmes dans lesquels il n'existe aucune forme de parallélisme exploitable. Une fois la faisabilité validée, on doit identifier les portions de code qui prennent le plus de temps dans l'application ( les points-chaud de l'application : \emph{hotspots}). Les outils de profilage et d'analyse des performances sont très utiles pour déterminer ses portions de code critiques. Il est nécessaire ensuite, de détecter les goulots d'étranglement (\emph{bottlenecks}) qui limitent la performance de l'application : les entrés/sorties sont un bon exemple de \emph{bottlenecks}. La bande passante limite la performance d'une application consommant beaucoup d'entrées/sorties. Enfin dans certains cas, il peut s'avérer nécessaire de changer l'algorithme de calcul pour qu'il puisse bénéficier du parallélisme d'une architecture.

%\subsubsection*{Partitionner le problème}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=\columnwidth]{Chapter1/Figures/prob_part}
%	\caption{Partitionnement du problème : (a) partitionnement de domaine (b) partitionnement fonctionnel}
%	\label{figPartitionning}
%\end{figure}
%La deuxième étape de la parallélisation concerne le partitionnement du problème. Selon la nature du parallélisme contenu dans l'application : parallélisme de données ou de tâches. Il existe deux manières de décomposer le problème. La première décomposition qui exploite le parallélisme de données consiste à diviser la structure de données en partie égales ou non et d'assigner à chacune des tâches une partition de données sur laquelle elle effectue des calculs. Dans ce cas précis, les tâches effectuent les mêmes opérations sur les données. La décomposition fonctionnelle est la deuxième manière de partitionner  le problème, le parallélisme de tâches est alors exploité. Les tâches exécutent des portions de code différentes sur les mêmes données.

%\subsubsection*{Gestion des communications}
%Les communications sont souvent problématiques en programmation parallèle. En effet, le fait de décomposer le problème engendre parfois un besoin de communication entre les tâches. La première tâche consiste alors à déterminer si les tâches ont besoin de communiquer ou pas, ceci étant généralement déterminé par l'algorithme. Si une communication est nécessaire, il faut alors évaluer les facteurs qui influencent la vitesse des transferts qui sont, la latence et le débit. Ce dernier peut être altéré lors de situations dans lesquelles plusieurs transferts concurrents se partagent le bus de données. Lors d'une communication, il est nécessaire d'effectuer des points de synchronisation pour garantir la validité des traitements. Il faut alors évaluer le coût des opérations de synchronisation. On est souvent confrontés à des choix de conception lorsqu'il s'agit de communications entre les tâches. La multiplicité des transferts engendre autant de temps de latence que de transfert. Il est alors souvent utile de regrouper les transferts en un seul bloc, ce qui n'est pas tout le temps possible, car la largeur des bus et la capacité des mémoires sont limitées.

%\subsubsection*{Visibilité des communications}
%Les communications sont explicites ou implicites selon le modèle de programmation. En MPI par exemple, l'utilisateur contrôle finement les communications et détermine lui-même leur déroulement, les tailles des transferts et les points de synchronisation. Par contre dans le modèle \emph{data parallel}, les communications sont transparentes du point de vue de l'utilisateur et ne sont donc pas directement sous le contrôle du programmeur et tous ces aspects sont gérés automatiquement.

%\subsubsection*{Communications synchrones vs asynchrones}
%Les communications synchrones sont bloquantes i.e : l'exécution du programme est suspendue jusqu'à la fin de la transaction. Elles peuvent limiter la performance car elles augmentent les durées d'inactivité des processeurs. A l'opposé,  les communications asynchrones permettent l'entrelacement de tâches de calcul et de transfert, et ainsi un gain de performances potentiel lorsque l'architecture permet d'effectuer en parallèle des transferts et des calculs.

%\subsubsection*{Gestion de la synchronisation}
%La gestion de plusieurs ressources en parallèle, engendre un besoin de synchronisation. Les tâches ont souvent besoin de se synchroniser soit pendant un échange de données, soit à la suite d'une opération collective. Parmi les opérations de synchronisation les plus utilisées on trouve les barrières, les sémaphores et les verrous.

%\subsubsection*{Les Barrières}
%Ce type de  synchronisation est utilisée pour les opérations collectives comme les réductions. Toutes les tâches effectuent leur travail et sont suspendues lorsqu'elles atteignent la barrière. Lorsque la dernière tâche atteint la barrière toutes les tâches sont synchronisées.

%\subsubsection*{Les verrous et les sémaphores}
%Les \emph{locks} et les sémaphores servent généralement à protéger l'accès à une variable globale ou à rendre une section de code critique i.e : une seule tâche peut alors exécuter ses instructions se trouvant dans cette portion de code. Une tâches possède alors l'accès exclusif à la ressource en effectuant un \emph{lock()} et libère la ressource en effectuant un \emph{unlock()}.

%\subsection*{Dépendances de données}
%Les dépendances de données sont un des principaux inhibiteurs de la parallélisation. Une dépendance de données existe lorsqu'une modification de l'ordre d'exécution des instructions change le résultat du programme. Le concepteur de l'algorithme parallèle, doit gérer proprement les dépendances de données avec les opérations de synchronisations adéquates. Une modification de l'algorithme peut éliminer ces dépendances et permettre ainsi une parallélisation plus efficace. L'exemple le plus parlant étant celui de l'utilisation d'une variable locale dupliquée au lieu d'une variable globale qui nécessite des synchronisations pour assurer un accès cohérent pour les tâches concurrentes. 

%\subsection*{Equilibrage de charge}
%L'équilibrage de charge ou \emph{load-balancing} est une des problématiques qui se posent lors du développement d'un code parallèle. En effet, une distribution équitable de la charge de travail est nécessaire afin de minimiser les durées d'inactivité des processeurs. Lorsque les tâches effectuent le même travail l'équilibrage de charge est facile : il suffit d'attribuer aux tâches les mêmes quantités de données. Si par  contre les tâches exécutent un code différent, un ajustement de la charge de travail est parfois nécessaire.  Il subsiste certains cas où la charge n'est pas prédictible, par exemple lors du calcul de trajectoires de particules. Il faut alors effectuer du \emph{load balancing} dynamique.
%\subsubsection*{Granularité}
%La granularité du parallélisme est définie comme étant le ratio calcul/communication. Il existe alors deux formes de granularités.
%\paragraph{Parallélisme à grain fin \emph{(Fine-Grain)}}
%Dans ce cas là, le ratio calcul/communication est faible et les opportunités d'optimisation. L'équilibrage de charge est alors simplifié puisque les  tâches passent la majorité du temps en communications et pas en calcul.\paragraph*{Parallélisme à grain moyen \emph{(coarse-grain)}}
%Contrairement au parallélisme à grain fin le ratio calcul/communication est important. Les opportunités de gain de performance sont alors importantes car le calcul est prépondérant dans l'application. Ce type de granularité est idéal pour les architectures possédant plusieurs unités de traitement, limitées par la bande-passante mémoire. Par contre, l'équilibrage de charge n'est pas facile.
%\paragraph*{Choix de la granularité}
%Le choix de la granularité dépend de l'architecture et de l'algorithme à la fois. Le parallélisme grain-fin contribue à l'ajustage de l'équilibrage de charge.

%\subsection*{Limites et coût de la parallélisation}
%\subsubsection*{Loi d'Amdhal}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=\columnwidth]{Chapter1/Figures/amdhals_01}
%	\caption{l'accélération maximale en fonction de la portion de code parallélisable $P$}
%	\label{figAmdhals01}
%\end{figure}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=\columnwidth]{Chapter1/Figures/amdhals_02}
%	\caption{L'accélération en fonction du nombre de processeurs $N$ à $P$ constant}
%	\label{figAmdhals02}
%\end{figure}
%La loi d'Amdhal stipule que l'accélération d'un code donnée, est limitée par la proportion parallélisable de celui-ci. Cela se traduit par l'équation suivante : 
%\begin{equation}
%Speedup = \frac{1}{1-P}\end{equation}
%Où le \emph{Speedup} est l'accélération, P la proportion de code parallélisable.Si l'on intègre le nombre de processeurs N dans l'équation on aboutit à la formule :
%\begin{equation}
%Speedup = \frac{1}{\frac{P}{N}+S}
%\end{equation}
%L'influence de P est N sur l'accélération est illustrée sur les deux courbes des figures \ref{figAmdhals01} et \ref{figAmdhals02} : On peut observer à partir de ces courbes que l'accélération est limitée par la proportion de code parallélisable ce qui se traduit par une augmentation presque linéaire de celle-ci. D'autre part la courbe de droite indique que pour une proportion de code donnée, l'accélération est vite saturée ce qui se traduit sur le terrain par le fait que certains algorithmes ne sont pas parallélisables, et l'ajout d'unité de traitement ne se traduit pas forcément par une augmentation linéaire du gain.
%\subsubsection*{Loi de \emph{Gustafson-Barsis}}
%La loi de \emph{Gustafson-Barsis}\cite{Gustafson_1988} vient corriger la loi d'\emph{Amdahl} qui donne une limite à l'accélération atteignable, en considérant un problème de taille fixe. 
%La loi est enoncée comme suit :
%\begin{equation}
%S = N+(1-N)\times s
%\end{equation}
%Où $N$ est le nombre de processeurs, $S$ l'accélération (nommée \emph{Scaled Speedup} par \emph{Gustafson}) et $s$ est la portion non-parallélisable du programme. La loi de \emph{Gustafson-Barsis} aborde un point que la loi d'\emph{Amdahl} ne considère pas, à savoir la puissance de calcul disponible lorsque le nombre de processeurs augmente. L'idée générale de la loi consiste en l'augmentation de la taille du problème afin d'utiliser toute la puissance disponible pour résoudre celui-ci en un temps constant. Ainsi, si une machine plus puissante (avec plus de processeurs) est disponible, des problèmes plus larges peuvent être résolus en un temps identique à celui nécessaire pour la résolution de problèmes de plus petite taille. La loi d'\emph{Amdahl} au contraire, part du principe que la charge de travail du programme ne change pas en fonction du nombre de processeurs, ce qui correspond à un problème de taille fixe. Dans les deux lois, la portion parallélisable est supposée équitablement distribuée sur les processeurs.\\
%La loi de \emph{Gustafson-Barsis} a surtout permis aux chercheurs de réorienter les algorithmes afin qu'il résolvent des problèmes de plus grande taille plutôt que de se focaliser sur l'accélération d'un petit problème sur plusieurs processeurs.
%%------------
%\subsubsection*{Métrique de \emph{Karp-Flatt}}
%La métrique de \emph{Karp-Flatt}\cite{karp_flatt} est une mesure de la parallélisation de code sur les systèmes a processeurs parallèles. Cette métrique vient s'ajouter aux lois d'\emph{Amdahl} et \emph{Gustafson-Barsis} afin de donner une indication sur l'efficacité de la parallélisation d'un code sur une machine parallèle.\\
%Selon \emph{Karp-Flatt}, étant donné un programme parallèle ayant une accélération $S$ sur $p$ processeurs ($p > 1$), la portion séquentielle $s$ déterminée de manière experimentale est donnée par :
%\begin{equation}
%s = \frac{\frac{1}{S}-\frac{1}{p}}{1-\frac{1}{p}} \nonumber
%\end{equation} 
%Ainsi, plus $s$ est petit et plus la parallélisation du code est bonne.\\
%Cette métrique est venue apporter une correction aux deux lois d'\emph{Amdahl} et \emph{Gustafson-Barsis} en déterminant de manière expérimentale la portion séquentielle du code. En effet, la loi d'\emph{Amdahl} ne prend en compte ni les problèmes d'équilibrage de charge ni le surcoût induit par la parallélisation.\\
%Pour un problème de taille fixe, l'efficacité d'un programme parallèle décroit lorsque le nombre de processeurs augmente. En utilisant la portion séquentielle obtenue expérimentalement, on peut déterminer si l'efficacité décroit à cause de la diminution des opportunités de parallélisation ou à cause de l'augmentation du surcoût algorithmique ou architectural.

La grande majorité des classes d'applications ont connu une amélioration régulière et gratuite de leurs performances durant les dernières décennies. Les fabricants de processeurs, de mémoires et de systèmes de stockage ont amélioré leurs systèmes d'une manière sensible, à tel point que les logiciels montraient des améliorations de performances sans même que leur code source ne soit réadapté. Nous avons vu la fréquence des processeurs passer de 500 $MHz$ à 1 $GHz$ puis à 2 $GHz$ pour atteindre 3 $GHz$ de nos jours. \\
La question qui se pose alors est celle de la limite de l'amélioration des processeurs par l'augmentation de leur fréquence d'horloge. Alors que la loi de Moore \cite{Moore_1965} prédit une augmentation exponentielle, les dernières années ont montré que nous approchions la saturation. Par conséquent, cette augmentation de fréquence doit éventuellement ralentir voire même s'arrêter.\\
Pour les développeurs, l'augmentation de la fréquence d'horloge des processeurs fut souvent synonyme d'amélioration des programmes sans en modifier la moindre ligne de code source. Hélas, cette affirmation n'est plus valable à l'heure où l'on parle, et elle ne le sera pas non plus dans le futur proche.\\
Les architectures vont évidemment continuer à évoluer et à améliorer leurs performances, mais l'amélioration des logiciels ne se fera plus de manière aussi systématique que par le passé. En effet, les évolutions architecturales ont changé de direction et s'orientent désormais vers une multiplication des unités de calcul et une coordination plus efficace de leur exécution concurrente pour améliorer les performances des programmes.
Durant les 30 dernières années, les concepteurs de processeurs ont pu améliorer les performances des architectures dans trois domaines :
\begin{itemize}
\item \textbf{La fréquence d'horloge :} cela consiste à réduire la durée du cycle d'horloge CPU et se traduit directement par l'exécution plus rapide d'un même programme.
\item \textbf{L'optimisation de l'exécution :} cela se traduit par l'accomplissement de plus de travail en un cycle d'horloge. Parmi les techniques les plus connues : le \emph{pipelining}, la prédiction de branchements, l'exécution de plusieurs instructions dans un même cycle (processeurs superscalaires) et le réordonnancement des instructions pour une exécution dans le désordre. Ces techniques ont pour but d'améliorer le flot d'exécution des instructions par la réduction de la latence et la maximisation du travail accompli en un cycle d'horloge
\item \textbf{La mémoire cache :} L'augmentation de la mémoire cache sur puce a pour but de réduire les accès à la mémoire vive (RAM). En effet, les accès à la mémoire principale se font toujours avec des latences très grandes et rapprocher les données des unités de calcul demeure une excellente technique pour améliorer les performances. Les tailles des caches ont augmenté sensiblement durant les dernières années pour atteindre des valeurs autour de 10 Mo sur les dernières générations de processeurs.
\end{itemize} 
Les techniques citées dans ce qui précède ont toutes pour point commun d'être efficaces quelque soit la nature de l'application. Une amélioration induite par une de ces techniques induit automatiquement une accélération aussi bien des application séquentielles que des programmes parallèles. Il est important de le noter car la majorité des applications existantes sont séquentielles pour des raisons historiques. La seule technique qui exploite le parallélisme de données SIMD et qui se trouve sous forme d'extensions du jeu d'instructions (MMX, SSE, Altivec) est souvent gérée par les compilateurs, on parle alors de vectorisation automatique (\emph{auto-vectorization}).\\

La pente de la courbe d'augmentation de la fréquence d'horloge CPU a commencé à s'aplatir en 2003. Il est devenu très difficile d'augmenter les fréquences d'horloges à cause de limitations physiques :
\begin{itemize}
\item La chaleur dégagée est devenue importante et très difficile à dissiper.
\item La consommation d'énergie n'a cessé d'augmenter.
\item Des problèmes de fuites de courants ont fait leur apparition.
\end{itemize}
Cela ne signifie pas que la loi de Moore arrive à son terme. Le nombre de transistors sur une puce continuera d'augmenter suivant cette loi, mais les fréquences d'horloges ne suivront pas la même tendance. La différence majeure réside dans le fait que l'amélioration des performances des processeurs se fera d'une manière radicalement différente dans les prochaines années. Ainsi, la plupart des applications actuelles ne pourront plus bénéficier de l'amélioration gratuite des performances, qui était jusque là possible. Les directions futures comprennent une seule des anciennes approches qui est l'augmentation de la taille de la mémoire cache. Pour le reste, deux nouvelles techniques seront utilisées :
\begin{itemize}
\item \textbf{\emph{L'hyperthreading}} : cette technique consiste à exécuter deux ou plusieurs \emph{threads} sur un même CPU. Cette technique permet à certaines instructions de s'exécuter en parallèle. Malgré la présence de registres supplémentaires, ce type d'architectures ne possède q'un seul cache et qu'un seul couple  ALU/FPU\nomenclature{FPU}{Floating-Point Unit}. On attribue à l'\emph{hyperthreading} un potentiel d'amélioration des performances situé entre 5 et 15\% pour une application contenant un proportion raisonnable de \emph{multithreading} et jusqu'à 40\% pour une application exploitant pleinement ce type de parallélisme.
\item \textbf{\emph{Le multicore}} : C'est l'intégration de deux ou plusieurs CPUs sur la même puce. Cette technique permet dans un cas idéal (mais pas réaliste) de diviser le temps d'exécution de l'application par un facteur égal au nombres de CPUs, à condition que l'application utilise plusieurs \emph{threads}.
\end{itemize}
Il est certain que les architectures continueront à évoluer, mais l'amélioration de la performance des applications ne se fera pas de manière exponentielle sans exploiter le parallélisme. Le pas à franchir n'est pas facile, car toutes les applications ne sont pas naturellement parallélisables et la programmation parallèle est difficile.\\
La programmation parallèle existe depuis longtemps, mais elle était jusque là réservée à une poignée de développeurs pour qui les architectures classiques n'apportaient pas de résultats satisfaisants à des problèmes complexes et nécessitant une grande puissance de calcul.\\
Le parallélisme est la nouvelle révolution dans la manière avec laquelle nous écrivons les logiciels. Ce changement est aussi important que l'avènement de la programmation orientée objet dans les années 1990 notamment en termes de courbe d'apprentissage.

%CPU performance growth as we have known it hit a wall two years ago. Most people have only recently started to notice.
%You can get similar graphs for other chips, but I?m going to use Intel data here. Figure 1 graphs the history of Intel chip introductions by clock speed and number of transistors. The number of transistors continues to climb, at least for now. Clock speed, however, is a different story.

	
%\section*{Contexte de l'étude}
La démocratisation des architectures parallèles et massivement parallèles que l'on connait aujourd'hui a ouvert de nouvelles perspectives pour le développement et le déploiement d'applications de plus en plus complexes. Nous avons connu durant les cinq dernières années une émergence des architectures à plusieurs unités de calcul. Des processeurs jusque là réservés à des machines de calculs dédiées aux calculateurs des grands laboratoires scientifiques se retrouvent de nos jours dans des stations de travail de particuliers et même dans des ordinateurs portables voire embarqués. Plusieurs types d'architectures parallèles ont alors vu le jours : les plus répandues sont les architectures multi-coeurs à mémoire partagée du type \emph{SMP} (\emph{Symmetric Multi-Processor}), il équipent la grande majorité des ordinateur de bureau du marché. L'autre catégorie de processeurs parallèles ayant vu le jour à la même période sont les architectures hétérogènes : le processeur Cell étant l'exemple le plus connu de ce type de machines. Ces deux dernières catégories sont plutôt classiques dans leur genre car elles existaient auparavant mais étaient jusque là réservées aux grands calculateurs pour les \emph{SMP} et aux systèmes embarquées pour les architectures parallèles hétérogènes. L'autre type d'architectures ayant émergé peu après sont les GPU (\emph{Graphics Processing Unit})\nomenclature{GPU}{Graphics Processing Unit}. Là encore, ce sont des architectures qui existaient auparavant mais leur usage était uniquement réservé au rendu graphique. Les constructeurs on apporté des modifications architecturales et ils ont fourni de nouvelles interfaces de programmation : Nvidia CUDA\cite{cuda} et plus récemment OpenCL\cite{opencl}, afin de permettre une utilisation en tant que machines de calcul pour les architectures graphiques.\\
Les architectures parallèles sont devenues accessibles aux plus grand nombre et n'ont cessé de s'améliorer en performances. De nouvelles perspectives se sont ouvertes alors pour les applications et leurs concepteurs. Le traitement d'images est l'un des domaines qui requiert une grande puissance de calcul et qui se prête bien à la parallélisation.
Cependant, plusieurs obstacles peuvent constituer un frein à la mise en oeuvre des programmes sur ces nouvelles architectures parallèles.

\subsection*{Les spécificités architecturales}
Même si des points communs existent entre elles, les architectures matérielles diffèrent entres elles selon plusieurs aspects. Par exemple, une architecture du type CPU multi-coeurs contient en général un nombre limité de processeurs capables d'exécuter des tâches avec un flot de contrôle complexe et peuvent également exécuter des applications où le flot de données est important. D'un autre côté , les architectures du type GPU, contiennent plusieurs centaines de coeurs (on parle alors de \emph{many core}) dédiés aux problèmes massivement parallèles (\emph{embarassingly parallel problem}) et sont donc plus appropriées pour le parallélisme de données. De plus, entre deux architectures d'une même famille, les GPU par exemple, subsistent des différences dans les jeux d'instructions, l'organisation de la mémoire et la conception des unités de calcul, ce qui empêche la portabilité de la performance.  

\subsection*{Les spécificités liés aux outils de développement}
Les efforts de normalisation contribuent à la portabilité du code source entre architectures parallèles et OpenCL\cite{opencl} (à l'initiative d'Apple) en est l'exemple le plus récent. Toutefois, chaque architecture possède en général un outil de programmation propriétaire différent de celui des ces concurrentes. De plus, chaque modèle de programmation est adapté à un type d'architecture donné. Par conséquent, la performance n'est pas toujours au rendez-vous et dépend d'une adéquation entre l'architecture matérielle, le modèle de programmation et l'implémentation de celui-ci.

\section*{Objectifs des travaux}
Au vu des contraintes citées auparavant, l'objectif de nos travaux est à la fois d'établir des méthodes génériques d'optimisation de code pour le processeur Cell, et de proposer un modèle de programmation implanté sous forme d'une bibliothèque logicielle pour l'aide à la parallélisation de code. Nos travaux visent le traitement d'images bas-niveaux mais peuvent aisément s'étendre à d'autres domaines ayant des structures de données et des algorithmes similaires.


 Le manuscrit relatant ces travaux est organisé comme suit:
\begin{itemize}
\item \textbf{Chapitre 1, Concepts généraux et architecture du Cell}. Les concepts généraux du calcul parallèle sont donnés dans ce chapitre. Les architectures parallèles y sont classées selon le type de parallélisme qu'elle exploitent et selon leur hiérarchie mémoire. Les principaux modèles de programmation y sont exposés et la méthodologie de parallélisation y est détaillée. Les problèmes et les limitations liés à la parallélisation sont également traités. Nous présentons également dans ce chapitre, une vue d'ensemble de l'architecture principale étudiée lors de nos travaux : le Cell Broadband Engine\nomenclature{Cell BE}{Cell Broadband Engine}. Les principaux composants y sont décrits ainsi que les différents niveaux de parallélisme de l'architecture. L'environnement de développement de base fourni par IBM est également décrit avec une vue globale de l'API et des contraintes de programmation spécifiques à l'architecture du Cell. 
\item \textbf{Chapitre 2, Outils de programmation tierces pour le Cell}. Certains outils de programmation développés à la fois dans le milieu académique et industriel sont présentés. Les modèles de programmation associés aux outils sont étudiés. Le but de cette analyse est également de comparer les approches pour en extraire les avantages et les incovénients en ce qui concerne l'aide à la parallélisation de code sur l'architecture du processeur Cell.
\item \textbf{Chapitre 3, Squelettes algorithmiques pour le Cell}. Nous présentons dans ce chapitre, la bibliothèque \texttt{SKELL\_BE} à base de squelettes algorithmiques. Cet outil de développement issu d'un travail collaboratif entre Joel Falcou, Lionel Lacassagne et moi même, propose un générateur de code parallèle ayant pour entrée une description haut-niveau de l'application. Cette bibliothèque reprend des schémas de parallélisation communs qui servent de composants de base pour la construction d'une application plus complexe. Une analyse du surcoût de mise en oeuvre ainsi que des performances obtenues à l'aide du modèle sont également présentées.  
\item \textbf{Chapitre 4, Parallélisation de la détection de coins d'Harris}. Nous présentons une étude approfondie de l'algorithme de détection de points d'intérêt de Harris. Cet algorithme représentatif des applications de traitement d'images bas-nous a servi de cas d'étude pour l'analyse de schémas de parallélisation et des techniques d'optimisation sur notre architecture cible. Plusieurs paramètres algorithmiques et de l'architecture sont ainsi étudiés afin de déterminer la méthodologie de portage d'application sur le processeur Cell. Le but de ces travaux est à la fois de permettre au développeur de tirer partie des dispositifs de l'architecture et de permettre aux concepteurs de compilateurs et d'outils de parallélisation d'avoir un socle sur lequel ils peuvent se reposer pour développer des processus automatiques d'optimisation.
\item \textbf{Chapitre 5, Comparaison avec les autres architectures parallèles}. Nous proposons une étude comparative des architectures parallèles émergentes concurrentes du processeur Cell. L'algorithme de référence est celui étudié dans le chapitre précedent . Le architectures de type SMP multi-coeurs et les architectures de type GPU sont abordées. Plusieurs critères sont mis en avant pour la comparaison, la programmabilité, les performances pures, ainsi que l'efficacité energétique.
\item \textbf{Conclusion générale et perspectives}  Nous résumons les apports de nos travaux dans ce chapitre et donnons  les perspectives des outils développés ainsi que les évolutions dans le domaine de la parallélisation de code connues à ce jour.
\end{itemize}
%Le traitement d'images est une discipline du traitement du signal dont l'entrée est une image. Les techniques de traitement du signal sont appliquées à un signal de 2 dimensions pour donner en sortie soit une image, soit une certaine caractéristique de l'image. Dans notre cas on s'intéresse au traitement d'images de bas niveau, ou les opérateurs sont souvent des opérateurs point-à-point ou des noyaux de convolution. Ce type de calcul est bien adapté aux machines parallèles car il n'y a généralement pas de dépendances de données et les algorithmes de bas-niveau sont majoritairement des opérations de calcul pur. Toutefois, pour des applications de moyen-niveau, par exemple la segmentation, il existe une forte dépendance de données et les algorithmes sont souvent des imbrications de structures conditions. Ceci rend la tâche de parallélisation pour ce types d'algorithmes fastidieuse, et les accélérations sont souvent médiocres en comparaison avec l'effort fourni pour l'adaptation de l'algorithme série. La conception d'une version parallèle d'un algorithme devient une tâche fastidieuse et qui dépend de plusieurs facteurs. Il est alors important d'accorder de l'importance à plusieurs aspects déterminants pour la performance.
%\subsection{Le Partitionnement}
%La première tâche lors de la parallélisation d'une application de traitement d'image est le choix du type partitionnement à adopter. Ce principe prend ça source du fait qu'il y a plusieurs ressources et que le but est de repartir la charge de travail sur ces ressources sous formes de morceaux qui peuvent être distribués sur plusieurs tâches. Il existe deux méthodes basiques pour le partitionnement la décomposition de domaine (\emph{domain decomposition}) et la décomposition fonctionnelle (\emph{functionnal decomposition})
%\subsubsection{Décomposition de Domaine}
%\begin{figure}[htb]
%	\centering
%	\includegraphics[width=0.8\columnwidth]{Chapter1/figures/domaindecomp}
%	\caption{Partitonnement par décomposition du domaine}
%	\label{figdomain}
%\end{figure}

%Dans ce type de partitionnement, les données associées au problème sont décomposées. Chaque tâche parallèle travaille donc sur une portion des données. Il existe alors plusieurs manières de décomposer les données. En traitement d'images, les données sont en 2 dimensions les décompositions possibles sont illustrées en figure \ref{decomp2D}.

%\begin{figure}[htb]
%	\centering
%	\includegraphics[width=0.8\columnwidth]{Chapter1/figures/decomp2D}
%	\caption{Partitonnement par décomposition du domaine en traitement d'images}
%	\label{decomp2D}
%\end{figure}





