Les logiciels ont été conçus historiquement pour une exécution séquentielle. Les programmes devaient s'exécuter sur une seule machine, contenant une seule unité de traitement centrale (\emph{CPU}) \nomenclature{CPU}{Central Processing Unit} et le problème est décomposé en une suite d'instructions qui sont exécutées les unes après les autres. Ainsi, une seule instruction peut être exécutée à la fois. Le calcul parallèle est par opposition à la précédente approche, l'utilisation simultanée de plusieurs ressources de calcul pour résoudre un problème. Un logiciel peut ainsi s'exécuter sur plusieurs \emph{CPU}s. Le problème est décomposé en plusieurs parties qui peuvent être résolues de manière concurrente. Ces parties sont à leur tour décomposées en plusieurs instructions et chaque paquet d'instructions s'exécute de manière indépendante l'un de l'autre. Les ressources de calcul incluent une seule machine avec plusieurs processeurs, un nombre arbitraire de machines connectées via un réseau ou alors une combinaison des deux. Une bonne partie des problèmes de calcul intensif possèdent certaines caractéristiques qui en font de bons candidats à la parallélisation. Parmi ces caractéristiques : la possibilités de les décomposer en plusieurs sous-problèmes qui peuvent être résolus simultanément et la possibilités d'être résolus en moins de temps avec plusieurs ressources qu'avec une seule. Le calcul parallèle était auparavant, réservé exclusivement à la modélisation de problèmes et de phénomènes scientifiques provenant de la réalité tels que l'environnement, la physique nucléaire, les biotechnologies, la géologie et les mathématiques. A ce jour, le calcul parallèle s'est démocratisé grâce notamment à l'évolution fulgurante de la technologie des semi-conducteurs qui a rendu les plate-formes haute performance plus accessibles. On peut citer des applications comme les bases de données, la prospection pétrolière, les moteurs de recherche, la modélisation financière, les technologies de diffusion multimédia et les applications graphiques et de réalité augmentée.
\section{Concepts généraux}
\subsection{Architecture de \emph{Von Neumann}}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\columnwidth]{Chapter1/figures/vonneumann}
	\caption{Architecture de \emph{Von Neumann}}
	\label{figvonneumann}
\end{figure}
Ce modèle fut inventé par le mathématicien hongrois \emph{John von Neumann} qui a posé les premières bases de la conception d'un ordinateur dans son papier de 1945 \cite{vonneumann}. A partir de ce moment, la majorité des ordinateurs ont été conçus sur ces bases. L'architecture \emph{von Neumann} (Fig. \ref{figvonneumann}) est constituée de 4 composants principaux: une mémoire, une unité de contrôle, une unité arithmétique et logique (\emph{ALU}) des entrées/sorties (\emph{I/O})\nomenclature{I/O}{Input/Output}. La mémoire à accès aléatoire (\emph{RAM}) en lecture/écriture est utilisée pour stocker les instructions ainsi que les données. L'unité de contrôle va chercher les instructions ou les données de la mémoire, décode les instructions et coordonne séquentiellement les opérations afin d'accomplir la tâche programmée. L'ALU effectue les opérations arithmétiques de base. Les I/O font l'interface avec l'utilisateur humain.
\subsection{Classification de Flynn des machines parallèles}
Il existe plusieurs manières de classer les machines parallèles. Toutefois, il existe une classification qui est largement utilisée depuis 1966 et qui est celle de \emph{Flynn} \cite{flynn} (\emph{Flynn's Taxonomy}). Cette classification distingue les architectures parallèles selon deux paramètres indépendants qui sont les instructions et les données : chacun de ces deux paramètres peut avoir deux états possibles \emph{Single} ou \emph{Multiple}. Ainsi le tableau \ref{flynn} illustre la classification de Flynn.
\begin{table}
\centering
\begin{tabular}{|c||c|}
\hline
\rowcolor{medium-gray}\textbf{SISD} &  \textbf{SIMD}\nomenclature{SIMD}{Single Instruction Multiple Data} \\
\hline
Single Instruction Single Data & Single Instruction Multiple Data\\
\hline
\hline
\rowcolor{medium-gray}\textbf{MISD} &  \textbf{MIMD} \\
\hline
Multiple Instruction Single Data& Multiple Instruction Multiple Data\\
\hline
\end{tabular}
\caption{Classification de \emph{Flynn} des machines parallèles}
\label{flynn}
\end{table}

\subsubsection{Single instruction, single data (SISD)}
Une machine séquentille qui ne peut exécuter qu'un seul flux d'instructions en un cycle d'horloge \emph{CPU}. De plus, un seul flux de données est utilisé comme entrée en un cycle d'horloge. L'exécution du programme y est déterministe et il constitue le type de machines à la fois le plus ancien est le plus répandu de nos jours.
\subsubsection{Single instruction, multiple data (SIMD)}
C'est un type de machines parallèles dont les processeurs exécutent la même instruction en un cycle d'horloge donné. Cependant, chaque unité de traitement peut opérer sur un élément de données différent. Ce type de machines est bien taillé pour des problèmes réguliers tels que le traitement d'images et le rendu graphique. L'exécution des programmes y est synchrone et déterministe. Deux variantes de ces machines existent : 
\begin{itemize}
\item Processor Arrays: Connection Machine CM-2, MasPar MP-1 \& MP-2, ILLIAC IV
\item Vector Pipelines: IBM 9000, Cray X-MP, Y-MP \& C90, Fujitsu VP, NEC SX-2, Hitachi S820, ETA10 
\end{itemize}
De plus, la majorité des processeurs des stations de travail actuelles et des unités de traitement graphiques, comportent une unité de traitement spécialisée SIMD, on parle alors de \emph{SWAR} (\emph{SIMD Within A Register}).\nomenclature{SWAR}{SIMD Within A Register}

\subsubsection{Multiple instruction, single data (MISD)}
Un seul flux de données alimente plusieurs unités de traitement et chaque unité de traitement opère sur les données de manière indépendante grâce à un flot d'instructions indépendantes. On ne connaît pas de machines de ce type qui a été conçue.
\subsubsection{Multiple instruction, multiple data (MIMD)}
C'est actuellement le type le plus commun de machines parallèles. Chaque processeur de ces machines peut exécuter un flux d'instructions différent et peut opérer sur un flux de données différent. L'exécution peut être synchrone ou asynchrone, déterministe ou non-déterministe. On peut citer les \emph{Supercomputers} actuels, les clusters de machines parallèles mis en réseau, les grilles de calculs, les multi-processeurs SMP (\emph{Symetric Multi-Processor}) \nomenclature{SMP}{Symetric Multi-Processor}
 et les processeurs multi-core. De plus, plusieurs de ces machines contiennent des unités de traitement SIMD.

\section{Architectures mémoire des machines parallèles}
Dans la suite nous donnons une classification des machines parallèles selon le type de leur hiérarchie mémoire. Cette classification permet d'une part de distinguer les machines parallèles d'un autre point de vue que celui du CPU et permet également de mieux comprendre les motivations des modèles de programmation pour les machines parallèles.
\subsection{Les machines parallèles à mémoire partagée}
Il existe plusieurs variantes de ces machines mais toutes partagent une propriété commune qui est la possibilité à tous les processeurs d'accéder à la mémoire comme un espace d'adressage global. Ainsi, plusieurs processeurs peuvent opérer d'une manière indépendante mais partagent la même ressource mémoire. Un changement opéré par un processeurs dans un emplacement mémoire est visible à tous les autres processeurs. Cette classe de machines peut être divisée en deux sous-classes basées sur les temps d'accès à la mémoire : UMA et NUMA.
\subsubsection{Uniform memory access (UMA)}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\columnwidth]{Chapter1/figures/SMUMA}
	\caption{Machine parallèle à mémoire partagée UMA}
	\label{figSMUMA}
\end{figure}
Ce sont principalement les machines de type SMP qui possèdent plusieurs processeurs identiques et qui peuvent accéder de manière égale et en un temps identique à la mémoire. Elles sont parfois appelées CC-UMA - Cache Coherent UMA. La cohérence de cache signifie que si un processeur met à jour un emplacement de la mémoire tous les autres processeurs sont au courant de ce changement. Cette fonctionnalité est assurée au niveau matériel.
\subsubsection{Non-uniform memory access (NUMA)}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\columnwidth]{Chapter1/figures/SMNUMA}
	\caption{Machine parallèle à mémoire partagée NUMA}
	\label{figSMNUMA}
\end{figure}
Ce type de machines est souvent conçu en connectant deux ou plusieurs SMPs. Un SMP peut avoir un accès direct à la mémoire d'un autre SMP. Le temps d'accès à une mémoire donnée n'est pas égal pour tous les processeurs et lorsque un noeud est traversé, l'accès est plus lent. Si la cohérence de cache est garantie on parle alors de CC-NUMA.

%\subsubsection{Avantages et inconvénients}
%Parmi les avantages de ce type d'architectures mémoire est une perspective simplifiée de la mémoire du point de vue du programmeur. Le partage des données entre les tâches est à la fois rapide et uniforme. Le premier inconvénient est le manque de mise à l'échelle (\emph{scalability}) entre la mémoire et les CPUs. Le fait d'augmenter le nombre de CPUs augmente le trafic sur le bus mémoire et provoque un goulot d'étranglement et la gestion de la cohérence devient de plus en plus complexe. Le programmeur est responsable de la synchronisation des tâches qui garantit un accès correcte à la mémoire globale. Par conséquent, la conception de machine parallèles à mémoire partagée avec de plus en plus de processeurs devient difficile et coûteuse. 
\subsection{Les Machines parallèles à mémoire distribuée}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\columnwidth]{Chapter1/Figures/DMEM}
	\caption{Machine parallèle à mémoire distribuée}
	\label{figDMEM}
\end{figure}
Comme les machines à mémoire partagée, les machines à mémoire distribuée varient mais elles partagent tout de même un point commun : elles requièrent un réseau de communication pour connecter les mémoires des processeurs. Les différents processeurs possèdent leur propre mémoire locale. Les adresses mémoire d'un processeur donné ne correspondent pas à celles d'un autre et par conséquent le concept de mémoire globale n'existe pas. Puisque chaque processeur possède sa propre mémoire privée il opère de manière indépendante. En effet, chaque changement opéré sur sa mémoire locale n'a aucun effet sur la mémoire des autres processeurs ce qui exclue le concept de cohérence de cache. Lorsqu'un processeur à besoin des données contenues dans la mémoire d'un autre processeur, le programmeur est en charge de définir quand et comment les données sont transférées. Ce dernier est aussi responsable de la synchronisation.  
%\subsubsection{Avantages et inconvénients}
%L'avantage majeur de ce type d'architectures est le fait que la mémoire soit \emph{scalable} avec le nombre de processeurs. En effet, la taille de la mémoire croit proportionnellement avec le nombre de processeurs. Chaque processeur peut aussi accéder rapidement à sa mémoire locale sans interférence et sans engendrer de surcout du au maintien de la cohérence de cache. Le principal inconvénient de ce type d'architectures mémoire et la gestion explicite par le logiciel des communications entre les processeurs. Les accès à la mémoire se font souvent à des temps non-uniformes et la présence de plusieurs espaces d'adressage rend complexe l'adaptation de programmes écrits pour une mémoire partagée.
\subsection{Les Machines parallèles à mémoire hybride}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\columnwidth]{Chapter1/figures/HMEM}
	\caption{Machine parallèle à mémoire hybride}
	\label{figHMEM}
\end{figure}
Les machines les plus rapides du monde emploient des architectures mémoire dites hybrides qui regroupent les deux types précédents: partagée et distribuée. La composante mémoire partagée est souvent une machine SMP. La composante distribuée quant à elle consiste en la mise en réseau de plusieurs machines SMP. Les différents SMPs ne peuvent adresser que leur propre mémoire et le transfert de données entre deux SMPs requiert des communications au travers du réseau.
%Selon le niveau dans lequel on se trouve, ce type de machines possède les inconvénients et avantages des deux précédentes architectures mémoire.

%\subsection{Récapitulatif sur les architectures parallèles}
%Au vu de la classification décrite dans ce qui précède. Il existe plusieurs types de machines parallèles. Ces machines peuvent être classées selon différents critères. Le premier étant la nature du parallélisme : instructions, données ou tâches. Le second critère de classification étant la nature de la hiérarchie mémoire :  partagée, distribuée ou hybride. Ces critères de distinction ne sont pas uniquement faits pour établir un catalogue d'architectures parallèles mais permettent entre autres d'établir une adéquation entre un domaine d'application ou  un modèle de programmation et ses outils associés et une machine parallèle.

\section{Modèles de programmation parallèle}
Il existe plusieurs modèles de programmation pour les machines parallèles. Ces modèles existent à un niveau d'abstraction au dessus de l'architecture matérielle et de celle de la mémoire. Même si à première vue les modèles de programmation sont intimement liés à l'architecture de la machine, ils sont supposés pouvoir être implémentés sur n'importe quelle machine parallèle quelqu'en soient les caractéristiques. Il n'existe pas de modèle de programmation idéal mais certains modèles de programmation sont bien adaptés pour une application donnée sur une machine donnée. Dans la suite nous décrivons les principaux modèles de programmation parallèles.
\subsection{Le Modèle \emph{shared memory}}
Dans ce modèle de programmation les tâches partagent un espace d'adressage commun sur lequel ils peuvent lire et écrire des données de manière asynchrone. Plusieurs mécanismes, tels que les \emph{locks} et les sémaphores peuvent être utilisés pour contrôler l'accès à la mémoire partagée. Ce modèle de programmation est simplifié du point de vue de l'utilisateur car il n'y a pas de notion d'appartenance des données à une tâche ce qui évite les communications explicites pour transférer des données d'une tâche à une autre. Toutefois, en terme de performances ce dernier point constitue un inconvénient car il engendre un surcout d'accès à la mémoire,  de rafraichissement de cache et de trafic sur le bus lorsque plusieurs processeurs utilisent les mêmes données.
Les implémentations de ce modèle sur les machines à mémoire partagée se résument au compilateur natif qui traduit les variables du programme en adresses mémoire globales. Il n'existe cependant pas d'implémentation de ce modèle sur des machines à mémoire distribuée.

\subsection{Le Modèle de programmation par \emph{threads}}
Dans le modèle de programmation par threads, un seul \emph{process} peut avoir des chemins d'exécution multiples et concurrents. On peut assimiler ce concept à un programme principal qui inclue un certain nombre de sous-routines. Le programme principal est ordonnancé pour être exécuté par le système d'exploitation, et il acquière toutes les ressources système nécessaires à son exécution. Il effectue alors un ensemble d'instructions en série et crée un certain nombre de tâches (\emph{threads}) qui peuvent être ordonnancées et exécutées par l'OS de manière concurrente. Chaque \emph{thread} possède ses données locales mais partage également les ressources du programme principal avec les autres \emph{threads}. Chaque \emph{thread} possède un accès à la mémoire globale car il partage l'espace d'adressage du programme principal. La charge du travail d'un \emph{thread} peut être considérée comme une sous-routine du programme principal mais qui peut s'exécuter en parallèle d'un autre \emph{thread}. Les \emph{threads} communiquent entre eux via la mémoire globale ce qui nécessite des opérations de synchronisation afin de garantir l'exclusivité de l'accès à un emplacement donnée à un instant donné pour un seul \emph{thread}. Les \emph{threads} ont une durée de vie variable et peuvent être crées et détruits tout au long du déroulement du programme. Le modèle de programmation par \emph{thread} est souvent associé aux machines à mémoire partagée. Les implémentations des \emph{threads} comportent en général une bibliothèque de fonctions ou alors une série de directives enfouis dans le code parallèle. Dans les deux cas l'utilisateur est responsable de la définition du parallélisme. Il existe plusieurs implémentations des \emph{threads}, et la plupart des  constructeurs ont développé leur propre version ce qui a affecté la portabilité des codes parallèles. Cependant, un effort de standardisation à donné naissance à deux implémentations qui sont devenues le standard de nos jours : Les \emph{threads} POSIX\nomenclature{POSIX}{Portable Operating System Interface} \cite{pthreads_std} et OpenMP\nomenclature{OpenMP}{Open Multiprocessing} \cite{OpenMP_1998}.

%\subsubsection{Les \emph{threads} POSIX}\nomenclature{POSIX}{Portable Operating System Interface}
%Ils sont basés sur une librairie de programmation parallèle et spécifiés par le standard \emph{IEEE POSIX 1003.1c standard (1995)} \cite{pthreads_std}. Ils sont implémentés uniquement en langage C et plus connus sous le nom de \emph{Pthreads}. Le parallélisme y est explicite et l'interface bas-niveau force le programmeur à donner beaucoup d'attention au détails.

%\subsubsection{OpenMP}\nomenclature{OpenMP}{Open Multiprocessing}
%C'est un modèle de programmation basé sur des directives de compilation et il peut être directement utilisé sur du code série. Ce standard à été défini par un consortium de vendeurs de processeurs et de logiciel. L'API \nomenclature{API}{Application Programming Interface} Fortran à été délivrée en 1997 alors que l'API C/C++ ne l'a été qu'une année plus tard. C'est une API portable et multi-plateforme et est très simple d'utilisation.
%OpenMP est de nos jours largement utilisé sur les architectures parallèles les plus répandues, à savoir les processeurs multi-coeurs symétriques à mémoire partagée. Celui-ci à remplacé graduellement les threads POSIX, car il est plus simple à utiliser. De plus, les performances en OpenMP sont très bonnes au niveau du passage à l'échelle. Toutefois, le compilateur utilisé est le facteur principal qui détermine la performance d'une implémentation parallèle en OpenMP.

\subsection{Le Modèle de programmation par passage de message}
Dans ce modèle, la programmation parallèle se fait par passage de messages. Un ensemble de tâches utilisent leur propre mémoire locale durant le calcul. Plusieurs tâches peuvent résider sur la même machine physique ou alors sur un nombre arbitraire de machines. Les tâches échangent des données au travers des communications en envoyant et recevant des messages. Les transferts de données requièrent des opérations coopératives pour être effectuées par chaque \emph{process}. Par exemple, une opération \emph{send} doit avoir une opération duale \emph{receive}. Les implémentations du \emph{Message Passing} prennent la forme d'une bibliothèque de sous-routines et le programmeur est responsable de la détection du parallélisme. Comme pour toute bibliothèque, plusieurs versions ont été développées, ce qui a provoqué des problèmes de compatibilité. En 1992 le \emph{MPI Forum} a vu le jour dans le but de standardiser les implémentations du \emph{Message Passing} parmi lesquelles PVM \cite{PVM_1990} et Parmacs \cite{Parmacs_1994}. Deux normes ont alors vu le jour : MPI \cite{mpistand} en 1994 et MPI-2 en 1996. Des nos jours, MPI est le modèle de programmation le plus utilisé pour le \emph{Message Passing}. Dans les implémentations MPI \nomenclature{MPI}{Message Passing Interface} sur des architectures à mémoire partagée, les communications réseau sont tout simplement remplacées par des copies mémoire.

\subsection{Le Modèle \emph{data parallel}}
Ce modèle est basé sur le parallélisme de données qui concentre le travail en parallèle sur un ensemble de données contenues dans un tableau ou dans une structure de données à plusieurs dimensions. Un ensemble de tâches travaillent collectivement sur la même structure de données mais chaque tâches opère sur une partition différente de cette structure. Les tâches effectuent toutes la même opération sur leur partition de données. Sur les architectures à mémoire partagée toutes les tâches peuvent avoir accès à la structure de données via la mémoire globale. Par contre lorsque l'architecture mémoire est distribuée les données sont divisées en morceaux qui résident dans la mémoire locale de chaque tâche. La programmation avec ce modèle se fait en général en écrivant du code avec des constructions de parallélisme de données. Ces dernières peuvent avoir la forme d'appel à des fonction d'une bibliothèque ou à des directives reconnues par un compilateur \emph{data parallel}. Les implémentation de ce modèle sont souvent sous forme de compilateurs ou d'extensions de ces derniers. On peut citer les compilateur Fortran (F90 et F95) et leur extension HPF (\emph{High Performance Fortran}) \cite{HPF_STD_1993} qui supportent la programmation \emph{data parallel}. \emph{HPF} inclut des directives qui contrôlent la distribution des données, des assertions qui peuvent améliorer l'optimisation du code généré ainsi que des construction \emph{data parallel}. Les implémentations sur les architectures mémoire distribuées de ce modèle sont sous forme d'un compilateur qui convertit le code standard en code \emph{Message Passing} (MPI) qui distribue ainsi les données sur les différents processeurs et tout cela de manière transparente du point de vue de l'utilisateur.\\
Malgré un forte popularité à ses débuts, HPF n'a pas connu le succès escompté comme le prouve l'analyse de son principal auteur dans \cite{Kennedy_HPF_2007}. 

\subsection{Modèle de programmation de calcul par flux}
Ce modèle appelé communément \emph{stream computing} est un modèle basé sur le parallélisme de données. Un même noyau de calcul \emph{kernel} est appliqué à un ensemble de données. Ce modèle est le modèle dominant sur les unités de calcul graphique. C'est un modèle ou le parallélisme est du type SIMD, ou plusieurs unités de calcul (typiquement des centaines) exécutent la même instruction sur un ensemble de données en parallèle. Les machines supportant ce type de modèle sont les GPU, les FPGA et certains processeurs spécialisés tels que le Imagine \cite{Imagine_2002} et Merrimac \cite{Merrimac_2003} de Stanford. Plusieurs langages ont été également développés afin de supporter ce type de matériel parmi lesquelles : StreamIt \cite{Streamit_2002} et Brook \cite{Buck_Brook_2004}. CUDA \cite{cuda} et OpenCL \cite{opencl} sont également des implémentations de ce modèle de programmation parallèle et sont de loin les plus utilisés de nos jours. Le modèle consiste à simplifier à la fois le matériel et à restreindre le type de parallélisme exploité.

%\subsection{Autres modèles}
%D'autres modèles existent et existeront dans le futur proche en plus de ceux mentionnés auparavant. On peut en mentionner trois :
%\subsubsection{Modèle hybride}
%Dans ce modèle deux ou plusieurs modèles sont combinés. On peut citer par exemple la combinaison de \emph{MPI} avec les \emph{Pthreads}\nomenclature{Pthreads}{POSIX threads} ou avec \emph{OpenMP}. Ainsi, différents niveaux de parallélisme sont gérés, par exemple un réseau de SMPs. On peut citer également la combinaison de \emph{HPF} avec \emph{MPI} pour le même type de configuration. Celui-ci combine le parallélisme de données et de tâches.

%\subsubsection{Modèle single program multiple data}
%Le modèle \emph{SPMD}\nomenclature{SPMD}{Single Program Multiple Data} est un modèle haut niveau qui peut être construit sur la base d'une combinaison des modèles cités précédemment. Un seul programme est exécuté par toutes les tâches simultanément. A n'importe quel instant les tâches peuvent exécuter des instructions différentes ou similaires du même programme.Un programme \emph{SPMD} peut toutefois contenir des branchement qui permettent à une tâche de n'exécuter qu'une portion du code et toutes les tâches peuvent utiliser différentes données.

%\subsubsection{Modèle multiple program multiple data}
%Tout comme le modèle \emph{SPMD}, le modèle \emph{MPMD} est haut-niveau et peut englober l'ensemble des modèles citées précédemment. Les programmes \emph{MPMD} ont typiquement plusieurs objets exécutables. Lors de l'exécution parallèle du programme une tâche peut exécuter le même programme ou un programme différent et toutes les tâches peuvent utiliser des données différentes.

%\subsection{Récapitulatif sur les modèles de programmation}
%Dans ce qui précède, nous avons cités les principaux modèles de programmation parallèles. Ces derniers diffèrent par la nature du parallélisme qui est exploité :  données ou tâches. Les modèles sont aussi fortement couplés à l'architecture mémoire de la machine parallèle. Enfin, l'adéquation du modèle avec l'architecture sont déterminants pour la performance.  

\section{Parallélisation}
Les architectures parallèles et les modèles de programmation associés étant définis. La question qui se pose alors est celle du choix à la fois de l'architecture et du modèle de programmation adéquats pour la mise en oeuvre d'une application parallèle donnée. L'efficacité des outils automatiques de parallélisation dépend souvent de plusieurs facteurs, parmi lesquelles : les caractéristiques de l'architecture matérielle et de la hiérarchie mémoire ainsi que la nature des algorithmes qui forment l'application à paralléliser.  L'utilisation d'outils automatiques n'est pas toujours efficace, il est alors parfois nécessaire de gérer manuellement le parallélisme et les optimisations qui lui sont associées. Deux choix se présentent alors : la parallélisation manuelle ou la parallélisation automatique.

\subsection{Parallélisation manuelle}
Elle permet un contrôle précis de la performance, et une grande flexibilité en termes de schéma de parallélisation possible (différents modèles de calcul). Par contre, les temps de développement sont importants, que cela soit pour la mise en oeuvre, le débogage ou la maintenance de l'application. Les erreurs sont parfois très difficiles à trouver, et le processus d'optimisation est souvent itératif.

\subsection{Parallélisation automatique}
L'automatisation du processus de parallélisation de code est un problème ouvert, et les efforts effectués en la matière sont de plus en plus nombreux, en particulier avec l'avènement des nouvelles architectures parallèles et leur démocratisation. On peut trouver deux formes d'outils automatiques de parallélisation. Certains outils sont entièrement automatiques : ils prennent en entrée un code source série et détectent automatiquement le parallélisme potentiel, ils génèrent en suite le code parallèle correspondant. D'autres outils sont semi-automatiques car l'utilisateur indique les portions de codes parallélisables, c'est le cas par exemple d'OpenMP via les directives de compilation.
L'avantage des approches automatiques est avant tout la rapidité de mise en oeuvre d'une solution à base de calcul parallèle, d'autant plus que dans la majorité des cas, le code original est directement utilisable. Par contre, le contrôle est beaucoup moins précis qu'avec une version entièrement manuelle. Il peut y avoir par exemple des écarts entre les résultats numériques notamment sur les arrondis pour les calculs en virgule flottante. De plus, les modèles de programmation dans ce cas ne permettent pas une grande flexibilité dans le choix des schémas de parallélisation. Dans certains cas le gain de performance peut être médiocre, et on peut même observer une baisse de performances par rapport à la version originale. Enfin, ce genre d'outils n'est généralement efficace que sur des portions de code facilement exploitables comme les boucles. 
Dans la suite nous allons présenter les différentes étapes de mise en oeuvre d'un code parallèle manuellement, les étapes en question vont de la détermination de l'opportunité de parallélisation jusqu'à la mise en oeuvre et l'évaluation du gain ainsi obtenu.

\subsection{Méthodologie de parallélisation manuelle}

\subsubsection{Comprendre le problème}
Avant même de commencer à développer la version parallèle d'une application, la première question qui se pose et celle de la faisabilité d'une telle solution. En effet, il existe certains problèmes dans lesquels il n'existe aucune forme de parallélisme exploitable. Une fois la faisabilité validée, on doit identifier les portions de code qui prennent le plus de temps dans l'application ( les points-chaud de l'application : \emph{hotspots}). Les outils de profilage et d'analyse des performances sont très utiles pour déterminer ses portions de code critiques. Il est nécessaire ensuite, de détecter les goulots d'étranglement (\emph{bottlenecks}) qui limitent la performance de l'application : les entrés/sorties sont un bon exemple de \emph{bottlenecks}. La bande passante limite la performance d'une application consommant beaucoup d'entrées/sorties. Enfin dans certains cas, il peut s'avérer nécessaire de changer l'algorithme de calcul pour qu'il puisse bénéficier du parallélisme d'une architecture.

\subsubsection{Partitionner le problème}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=\columnwidth]{Chapter1/Figures/prob_part}
	\caption{Partitionnement du problème : (a) partitionnement de domaine (b) partitionnement fonctionnel}
	\label{figPartitionning}
\end{figure}
La deuxième étape de la parallélisation concerne le partitionnement du problème. Selon la nature du parallélisme contenu dans l'application : parallélisme de données ou de tâches. Il existe deux manières de décomposer le problème. La première décomposition qui exploite le parallélisme de données consiste à diviser la structure de données en partie égales ou non et d'assigner à chacune des tâches une partition de données sur laquelle elle effectue des calculs. Dans ce cas précis, les tâches effectuent les mêmes opérations sur les données. La décomposition fonctionnelle est la deuxième manière de partitionner  le problème, le parallélisme de tâches est alors exploité. Les tâches exécutent des portions de code différentes sur les mêmes données.

\subsubsection{Gestion des communications}
Les communications sont souvent problématiques en programmation parallèle. En effet, le fait de décomposer le problème engendre parfois un besoin de communication entre les tâches. La première tâche consiste alors à déterminer si les tâches ont besoin de communiquer ou pas, ceci étant généralement déterminé par l'algorithme. Si une communication est nécessaire, il faut alors évaluer les facteurs qui influencent la vitesse des transferts qui sont, la latence et le débit. Ce dernier peut être altéré lors de situations dans lesquelles plusieurs transferts concurrents se partagent le bus de données. Lors d'une communication, il est nécessaire d'effectuer des points de synchronisation pour garantir la validité des traitements. Il faut alors évaluer le coût des opérations de synchronisation. On est souvent confrontés à des choix de conception lorsqu'il s'agit de communications entre les tâches. La multiplicité des transferts engendre autant de temps de latence que de transfert. Il est alors souvent utile de regrouper les transferts en un seul bloc, ce qui n'est pas tout le temps possible, car la largeur des bus et la capacité des mémoires sont limitées.

\subsubsection{Visibilité des communications}
Les communications sont explicites ou implicites selon le modèle de programmation. En MPI par exemple, l'utilisateur contrôle finement les communications et détermine lui-même leur déroulement, les tailles des transferts et les points de synchronisation. Par contre dans le modèle \emph{data parallel}, les communications sont transparentes du point de vue de l'utilisateur et ne sont donc pas directement sous le contrôle du programmeur et tous ces aspects sont gérés automatiquement.

\subsubsection{Communications synchrones vs asynchrones}
Les communications synchrones sont bloquantes i.e : l'exécution du programme est suspendue jusqu'à la fin de la transaction. Elles peuvent limiter la performance car elles augmentent les durées d'inactivité des processeurs. A l'opposé,  les communications asynchrones permettent l'entrelacement de tâches de calcul et de transfert, et ainsi un gain de performances potentiel lorsque l'architecture permet d'effectuer en parallèle des transferts et des calculs.

\subsubsection{Gestion de la synchronisation}
La gestion de plusieurs ressources en parallèle, engendre un besoin de synchronisation. Les tâches ont souvent besoin de se synchroniser soit pendant un échange de données, soit à la suite d'une opération collective. Parmi les opérations de synchronisation les plus utilisées on trouve les barrières, les sémaphores et les verrous.

\subsubsection{Les Barrières}
Ce type de  synchronisation est utilisée pour les opérations collectives comme les réductions. Toutes les tâches effectuent leur travail et sont suspendues lorsqu'elles atteignent la barrière. Lorsque la dernière tâche atteint la barrière toutes les tâches sont synchronisées.

\subsubsection{Les verrous et les sémaphores}
Les \emph{locks} et les sémaphores servent généralement à protéger l'accès à une variable globale ou à rendre une section de code critique i.e : une seule tâche peut alors exécuter ses instructions se trouvant dans cette portion de code. Une tâches possède alors l'accès exclusif à la ressource en effectuant un \emph{lock()} et libère la ressource en effectuant un \emph{unlock()}.

\subsection{Dépendances de données}
Les dépendances de données sont un des principaux inhibiteurs de la parallélisation. Une dépendance de données existe lorsqu'une modification de l'ordre d'exécution des instructions change le résultat du programme. Le concepteur de l'algorithme parallèle, doit gérer proprement les dépendances de données avec les opérations de synchronisations adéquates. Une modification de l'algorithme peut éliminer ces dépendances et permettre ainsi une parallélisation plus efficace. L'exemple le plus parlant étant celui de l'utilisation d'une variable locale dupliquée au lieu d'une variable globale qui nécessite des synchronisations pour assurer un accès cohérent pour les tâches concurrentes. 

\subsection{Equilibrage de charge}
L'équilibrage de charge ou \emph{load balancing} est une des problématiques qui se posent lors du développement d'un code parallèle. En effet, une distribution équitable de la charge de travail est nécessaire afin de minimiser les durées d'inactivité des processeurs. Lorsque les tâches effectuent le même travail, l'équilibrage de charge est trivial : il suffit d'attribuer aux tâches les mêmes quantités de données. Si par  contre les tâches exécutent un code différent, un ajustement de la charge de travail est parfois nécessaire.  Il subsiste certains cas où la charge n'est pas prédictible, par exemple lors du calcul de trajectoires de particules. Il faut alors effectuer du \emph{load balancing} dynamique.
\subsection{Granularité}
La granularité du parallélisme est définie comme étant le ratio calcul/communication. Il existe alors deux formes de granularités.
\subsubsection{Parallélisme à grain fin \emph{(Fine-Grain)}}
Dans ce cas là, le ratio calcul/communication est faible et les opportunités d'optimisation. L'équilibrage de charge est alors simplifié puisque les  tâches passent la majorité du temps en communications et pas en calcul.
\subsubsection{Parallélisme à gros grain \emph{(coarse-grain)}}
Contrairement au parallélisme à grain fin le ratio calcul/communication est important. Les opportunités de gain de performance sont alors importantes car le calcul est prépondérant dans l'application. Ce type de granularité est idéal pour les architectures possédant plusieurs unités de traitement, limitées par la bande-passante mémoire. Par contre, l'équilibrage de charge n'est pas facile.
\subsubsection{Choix de la granularité}
Le choix de la granularité dépend de l'architecture et de l'algorithme à la fois. Le parallélisme grain-fin contribue à l'ajustement de l'équilibrage de charge.

\subsection{Limites et coût de la parallélisation}

\subsubsection{Loi d'Amdhal}

%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=\columnwidth]{Chapter1/Figures/amdhals_01}
%	\caption{l'accélération maximale en fonction de la portion de code parallélisable $P$}
%	\label{figAmdhals01}
%\end{figure}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=\columnwidth]{Chapter1/Figures/amdhals_02}
%	\caption{L'accélération en fonction du nombre de processeurs $N$ à $P$ constant}
%	\label{figAmdhals02}
%\end{figure}
La loi d'Amdahl stipule que l'accélération d'un programme est limitée par la proportion parallélisable de celui-ci. Cela se traduit par l'équation suivante : 
%\begin{equation}
%Speedup = \frac{1}{1-P}\end{equation}
%Où le \emph{Speedup} est l'accélération, P la proportion de code parallélisable.Si l'on intègre le nombre de processeurs N dans l'équation on aboutit à la formule :
\begin{equation}
Accélération = \frac{\mbox{Temps d'exécution séquentielle}}{\mbox{Temps d'exécution parallèle}} = \frac{1}{(1-FP)+\frac{FP}{N}}
\label{eq_Amdahl}
\end{equation}
%L'influence de P est N sur l'accélération est illustrée sur les deux courbes des figures \ref{figAmdhals01} et \ref{figAmdhals02} : On peut observer à partir de ces courbes que l'accélération est limitée par la proportion de code parallélisable ce qui se traduit par une augmentation presque linéaire de celle-ci. D'autre part la courbe de droite indique que pour une proportion de code donnée, l'accélération est vite saturée ce qui se traduit sur le terrain par le fait que certains algorithmes ne sont pas parallélisables, et l'ajout d'unité de traitement ne se traduit pas forcément par une augmentation linéaire du gain.
Où $FP$ est la fraction parallélisable du programme et $N$ le nombre de processeurs.\\
On peut constater à partir de l'équation \ref{eq_Amdahl} que l'accélération est limitée par la proportion de code parallélisable. L'augmentation de l'accélération est quasi-linéaire avec l'augmentation de $N$ pour une $FP$ donnée. D'autre part, l'accélération est vite saturée et atteint $\frac{1}{1-FP}$ lorsque $N \to\infty$. Cela signifie qu'une augmentation linéaire de l'accélération n'est plus possible à partir d'une certaine valeur de $N$ et qu'il est inutile d'ajouter des unités de traitement pour améliorer les performances. De plus, le terme $\frac{FP}{N}$ implique que la fraction parallélisable peut être réduite d'un facteur $N$ ce qui est faux dans la réalité, car l'exécution parallèle induit un surcoût dû à plusieurs facteurs : la gestion des \emph{thread}, la synchronisation et les communications.

\subsubsection{Loi de \emph{Gustafson-Barsis}}
La loi de \emph{Gustafson-Barsis}\cite{Gustafson_1988} vient corriger la loi d'\emph{Amdahl} qui donne une limite à l'accélération atteignable, en considérant un problème de taille fixe. 
La loi est enoncée comme suit :
\begin{equation}
S = N+(1-N)\times s
\end{equation}
Où $N$ est le nombre de processeurs, $S$ l'accélération (nommée \emph{Scaled Speedup} par \emph{Gustafson}) et $s$ est la portion non-parallélisable du programme. La loi de \emph{Gustafson-Barsis} aborde un point que la loi d'\emph{Amdahl} ne considère pas, à savoir la puissance de calcul disponible lorsque le nombre de processeurs augmente. L'idée générale de la loi consiste en l'augmentation de la taille du problème afin d'utiliser toute la puissance disponible pour résoudre celui-ci en un temps constant. Ainsi, si une machine plus puissante (avec plus de processeurs) est disponible, des problèmes plus larges peuvent être résolus en un temps identique à celui nécessaire pour la résolution de problèmes de plus petite taille. La loi d'\emph{Amdahl} au contraire, part du principe que la charge de travail du programme ne change pas en fonction du nombre de processeurs, ce qui correspond à un problème de taille fixe. Dans les deux lois, la portion parallélisable est supposée équitablement distribuée sur les processeurs.\\
La loi de \emph{Gustafson-Barsis} a surtout permis aux chercheurs de réorienter les algorithmes afin qu'il résolvent des problèmes de plus grande taille plutôt que de se focaliser sur l'accélération d'un petit problème sur plusieurs processeurs.
%------------

\subsubsection{Métrique de \emph{Karp-Flatt}}
La métrique de \emph{Karp-Flatt}\cite{karp_flatt} est une mesure de la parallélisation de code sur les systèmes a processeurs parallèles. Cette métrique vient s'ajouter aux lois d'\emph{Amdahl} et \emph{Gustafson-Barsis} afin de donner une indication sur l'efficacité de la parallélisation d'un code sur une machine parallèle.\\
Selon \emph{Karp-Flatt}, étant donné un programme parallèle ayant une accélération $S$ sur $p$ processeurs ($p > 1$), la portion séquentielle $s$ déterminée de manière experimentale est donnée par :
\begin{equation}
s = \frac{\frac{1}{S}-\frac{1}{p}}{1-\frac{1}{p}} \nonumber
\end{equation} 
Ainsi, plus $s$ est petit et plus la parallélisation du code est bonne.\\
Cette métrique est venue apporter une correction aux deux lois d'\emph{Amdahl} et \emph{Gustafson-Barsis} en déterminant de manière expérimentale la portion séquentielle du code. En effet, la loi d'\emph{Amdahl} ne prend en compte ni les problèmes d'équilibrage de charge ni le surcoût induit par la parallélisation.\\
Pour un problème de taille fixe, l'efficacité d'un programme parallèle décroit lorsque le nombre de processeurs augmente. En utilisant la portion séquentielle obtenue expérimentalement, on peut déterminer si l'efficacité décroit à cause de la diminution des opportunités de parallélisation ou à cause de l'augmentation du surcoût algorithmique ou architectural.

\section{Architecture du processeur Cell}
%\begin{quote}
%\emph{"I think games are an interesting application area, but quite clearly, Cell is not just for games.  There are many other areas it can be used.  Games are the thing that inspired us to do it.}\\
%Peter Hofstee, chief architect of the Cell processor
%\end{quote}
\indent Le processeur Cell \cite{Cell_Johns_2007} est une architecture unique en son genre car elle renferme une multitude de dispositifs dédiés au calcul haute-performance. Son architecture parallèle à plusieurs niveaux permet aux utilisateurs aguerris d'atteindre des performances jusque là réservées aux seuls clusters de machines et utilisant des paradigmes de haut niveau tels que le \emph{message-passing}. En ce sens l'architecture du Cell, destinée initialement au domaine des jeux vidéos, a trouvé d'autres débouchés notamment dans le calcul scientifique au sens large.\\
\indent Le Cell est composé d'un processeur PowerPC classique nommé PPE (Power Processor Element)\nomenclature{PPE}{Power Processor Element} et de huit unités de calcul accélératrices appelées SPE (\emph{Synergestic Processor Element})\nomenclature{SPE}{Synergestic Processor Element}. Ces unités de calcul sont reliées par un bus interne qui permet également l'accès à la mémoire principale (\emph{Main Storage}), ainsi qu'à  d'autres périphériques externes. Le processeur Cell est considéré comme un processeur hétérogène car il comporte deux types d'architectures différentes : celle du PPE qui n'est autre qu'une déclinaison du PowerPC 970, et celle des SPEs qui sont des unités SIMD accélératrices spécialisées dans des traitements contenant un flot de données important comme le multimédia par exemple. Le jeu d'instructions vectorielles des SPE est très proche d'Altivec \cite{altivec_2000}, présent sur les architectures de type PowerPC 

\subsection{Vue générale}
\begin{figure}[!htbf]
	\centering
	\includegraphics[scale =0.65]{Chapter2/figures/cell_fig01_bw}
	\caption{Vue d'ensemble de l'architecture hétérogène du processeur Cell}
  \label{cell_fig1}
\end{figure}
Le processeur Cell est la première implémentation de l'architecture \emph{Cell Broadband Engine} (CBEA), qui est entièrement compatible avec l'architecture \emph{PowerPC} 64-bit. Ce processeur à été initialement conçu pour la console de jeux \emph{PlayStation 3} mais ses performances hors normes ont très vite fait de lui un bon candidat pour d'autres domaines d'applications qui requièrent une grande puissance de calcul, comme le traitement du signal et des images.\\
Le processeur Cell est une machine multi-coeurs hétérogène, capable d'effectuer une quantité de calcul en virgule flottante considérable, sur des données occupant une large bande-passante. Il est composé d'un processeur 64-bit appelé \emph{Power Processor Element} (PPE), huit co-processeurs spécialisés appelés \emph{Synergistic Processor Element} (SPE), un contrôleur mémoire haute-vitesse et une interface de bus à large bande-passante. Le tout intégré sur une seule et même puce.\\
Le PPE et les SPEs communiquent par le biais d'un bus interne de communication très rapide appelé \emph{Element Interconnect Bus} (EIB) (Fig. \ref{cell_fig1}). Avec une fréquence d'horloge de 3.2 GHz, le processeur Cell peut atteindre théoriquement une performance crête de 204.8 GFlop/s en flottants simple-précision (32 bits) et 14.6 GFlop/s en flottants double-précision (64 bits). Il est important de noter que sur la première version du Cell, le débit des instructions arithmétiques en double-précision était d'une instruction tous les 7 cycles ce qui explique les 14.6 GFlop/s. Ce débit a été amélioré pour atteindre une instruction par cycle sur la dernière génération \emph{PowerXCell 8i}, ce qui correspond  à une puissance de calcul de 102.4 GFlop/s. \\
%Le bus interne supporte une bande passante qui peut aller jusqu'à 204.8 Go/s pour les transferts internes à la puce (impliquant le PPE, les SPEs, la mémoire et les contrôleurs I/O). le contrôleur mémoire: \emph{Memory Interface Controller} (MIC) fournit une bande-passante de 25.6 Gbytes/s vers la mémoire principale. Le contrôleur I/O quand à lui fournit 25 Gbytes/s en entrée et 35 Gbytes/s en sortie.\\
%Le rôle du PPE est celui d'un chef d'orchestre. Il prend en charge l'OS (\emph{Operating System}) et coordonne les SPEs. Au niveau de l'architecture c'est un \emph{PowerPC} 64-bit classique avec une extension SIMD, un cache L1 de 32 Ko (données et instructions) et un cache L2 de 512 KB. C'est un processeur à exécution dans l'ordre (\emph{in-order execution processor}), il supporte le \emph{dual-issue} (parallélisme d'instructions) ainsi que le multi-threading d'ordre 2 (parallélisme de tâches).\\
%Chaque SPE est composé d'une SPU (\emph{Synergistic Processor Unit}) et d'un MFC (\emph{Memory Flow Controller}). Le MFC contient à son tour un contrôleur DMA (\emph{Direct Access Memory}), une unité de gestion de la mémoire (MMU), une unité interface de bus, et une unité atomique pour la synchronisation avec les autres SPEs et le PPE. Le SPU est un processeur de type RISC avec un jeu d'instructions et une micro-architecture conçus pour les applications flot de données haute-performance ou de calcul intensif. Le SPU inclut une mémoire locale de 256 Ko qui contient les données et les instructions. Le SPU ne peut pas accéder directement à la mémoire principale mais par le biais de commandes DMA via le MFC qui permettent de lire et d'écrire dans la mémoire principale. Les deux unités MFC et SPU sont indépendantes ce qui permet l'exécution des tâches de calcul et de transferts en parallèle sur le SPE.\\
%l n'existe pas de mécanisme hardware tel que la mémoire cache pour la gestion automatique des mémoires locales, et celles-ci doivent être gérées par le software. Le MFC effectue des commandes DMA pour transférer entre la mémoire centrale et les mémoires locales. Les instructions DMA pointent des emplacements de la mémoire centrale en utilisant des adresses virtuelles compatibles \emph{PowerPC}. Les commandes DMA peuvent transférer des données à partir de n'importe quel emplacement lié au bus d'interconnexion (mémoire principale, la mémoire locale d'un autre SPE, ou un périphérique I/O). Des transferts SPE vers SPE en parallèle sont faisables à raison de 16 \emph{bytes} par cycle d'horloge SPE, tandis que la bande-passante de la mémoire centrale est de 25.6 Gbytes/s pour le processeur entier.\\
%Chaque SPU contient 128 registres SIMD de taille 128-bits. Cette quantité importante de registres facilite l'ordonnancement efficace des instructions ainsi que d'autres optimisations comme le déroulage de boucle (loop-unrolling).\\
%Toutes les instructions SIMD sont des instructions que le pipeline peut exécuter à 4 granularités: 16 entiers 8-bit, 8 entiers 16-bit, 4 entiers 32-bits ou flottants simple-précision. Le processeur SPU est un processeur à exécution dans l'ordre (\emph{in-order-execution processor}), il possède deux pipelines d'instructions connus sous les dénominations pair (\emph{even}) et impair (\emph{odd}).\\
%Les instructions flottantes et entières sont dans le pipeline \emph{even} alors que le reste est dans le pipeline \emph{odd}. Chaque SPU peut lancer et compléter jusqu'à deux instructions par cycle, une par pipeline. Toutes les instructions flottantes en simple-précision peuvent être lancées en un cycle d'horloge du processeur. Par contre les instructions flottantes en double-précision ne sont pipelinées que partiellement, il en résulte un débit d'exécution moindre (deux instructions double-précision tous les 7 cycles d'horloge SPU).\\
%Si l'on prend une instruction de multiplication accumulation en flottant simple-précision (qui compte pour deux opérations) les 8 SPEs peuvent exécuter un total de 64 opérations par cycle \cite{kahle_2005}.\\

\subsection{Le PPE:  Power Processor Element}
Le PPE est un processeur 64 bits compatible avec l'architecture POWER \cite{ibm_power}, optimisé au niveau de l'efficacité énergétique\cite{Cell_Hofstee_2005}. La profondeur de pipeline du PPE est de 23 étages \cite{Cell_Kahle_2005}, chiffre qui peut paraitre faible par rapport au précédentes architectures PowerPC surtout quand on sait que la durée de l'étage est réduite d'un facteur 2. Le PPE est une architecture \emph{dual-issue} (deux instructions peuvent être lancées par cycle) qui ne réordonne pas dynamiquement les instructions  (exécution dans l'ordre). Les instructions arithmétiques simples, s'exécutent et fournissent leur résultat en deux cycles. Les instructions de chargements (\texttt{\textbf{load}}) s'exécutent également en deux cycles. Une instruction flottante en double précision s'exécute en dix cycles. Le PPE supporte une hiérarchie conventionnelle de caches, avec un cache L1 (de niveau 1) données et instructions de 32 Ko, et un cache L2 de 512 Ko.\\
Le PPE peut lancer deux \emph{threads} simultanément et peut être vu comme un processeur double-coeur avec un flot de données partagé, ceci donne l'impression au logiciel d'avoir deux unités de traitement distinctes. Les registres sont dupliqués mais pas les caches qui sont partagés par les deux \emph{threads}. Les instructions provenant de deux \emph{threads} de calcul différents sont entrelacées afin d'optimiser l'utilisation de la fenêtre d'exécution.\\
Le processeur est composé de trois unités : l'unité d'instructions (UI) responsable du chargement, décodage, branchements, exécution et complétion des instructions. Une unité d'exécution des opérations en arithmétique point-fixe (XU) qui est également responsable des instructions \texttt{\textbf{load/store}}. Et enfin l'unité VSU qui exécute les instructions en virgule flottante ainsi que les instructions vectorielles. Les instructions SIMD dans le PPE sont celles des générations précédentes de PowerPC 970 et effectuent des opérations sur des registres 128 bits de données qui permettent un parallélisme de 2, 4, 8 ou 16, selon le type de données traité.

\subsection{Les SPE (Synergistic Processing Element)}
Le SPE contient un jeu d'instructions nouveau mais qui n'est autre qu'une version réduite du jeu d'instructions SIMD VMX, strictement équivalent au jeu d'instructions Altivec cité auparavant, mais optimisé au niveau de la consommation d'énergie et des performances pour les applications de calcul intensif et de multimedia. Le SPE contient une mémoire locale de 256 Ko (\emph{scratchpad}) qui est une mémoire de données et d'instructions. Les données et les instructions sont transférées de la mémoire centrale vers cette mémoire privée au travers de commandes DMA\nomenclature{DMA}{Direct Memory Access} qui sont exécutées par le MFC (Memory Flow Controller)\nomenclature{MFC}{Memory Flow Controller} qui est présent dans chaque SPE. Chaque SPE peut supporter jusqu'à 16 commandes DMA en suspens. L'unité DMA peut être programmée de trois manières différentes : 1) avec des instructions sur le SPE qui insèrent des commandes DMA dans la file d'attente; 2) par la programmation de transferts permettant de faire des accès sur des zones non contiguës de la  mémoire en utilisant une liste de DMA, ce qui permet de définir un ensemble de transferts contigus de tailles différentes; 3) par l'insertion d'une commande DMA dans la file d'attente d'un autre processeur par les commandes de DMA-write.\\
Afin de faciliter la programmation et de permettre des transferts entre SPEs, les mémoires locales sont dupliquées en mémoire centrale. La présence des mémoires locales introduit un autre niveau dans la hiérarchie mémoire au-dessus des registres. Le temps d'accès à ces mémoires est de l'ordre du cycle ce qui en fait de bons candidats pour réduire la latence d'accès à la mémoire centrale qui est de l'ordre du millier de cycles. De plus, le contrôleur DMA est indépendant de l'unité de calcul ce qui donne un niveau de parallélisme supplémentaire. La présence de ces mémoires privées permet d'appliquer différents modèles de programmation sur le processeur Cell.\\
La mémoire locale est le composant le plus important en taille du SPE, et il était important de l'implémenter de manière efficace. Une mémoire SRAM\nomenclature{SRAM}{Static Random-Access Memory} à un seul port est utilisée pour réduire la surface. En dépit du fait que la mémoire locale doit arbitrer entre lectures/écritures DMA, chargements d'instructions, et  lecture/écriture mémoire, celle-ci a été conçue avec de ports de lecture moyens (128-bits) et larges (128 octets) dans le but de toujours fournir la meilleure performance possible. Le port le plus large est utilisé pour les commandes DMA et le chargement d'instructions. Cela est dû au fait qu'une commande DMA de 128 octets requière 16 cycles d'horloge pour placer les données sur le bus cohérent du Cell, même lorsque les commandes DMA s'exécutent avec une bande passante maximale. Ainsi, 7 sur 8 cycles d'horloge restent disponibles pour les \texttt{\textbf{load}}, \texttt{\textbf{store}} et les chargements d'instructions. De la même manière, les instructions sont chargées par morceaux de 128 octets et la pression sur la mémoire locale est par conséquent réduite. La plus haute priorité est donnée aux commandes DMA, la seconde plus haute priorité aux commandes \texttt{\textbf{load}} et \texttt{\textbf{store}}, le chargement ou pré-chargement des instructions n'est fait que lorsqu'il y a des cycles disponibles. Toutefois, une instruction qui force la disponibilité d'une fenêtre d'exécution pour le chargement d'instructions existe.\\
Les unités d'exécution du SPE sont organisées autour d'un flot de données 128 bits. Un banc de 128 registres fournit assez d'entrées pour permettre à un compilateur de réorganiser des groupes d'instructions afin de masquer la latence d'exécution des instructions. Il n'y a qu'un seul banc de registres et toutes les instructions sont SIMD 128 bits avec une largeur d'élément différente selon le type (2x64 bits, 4x32 bits, 8x16 bits, 16x8 bits et 128x 1 bit). Deux instructions sont lancées à chaque cycle :  une fenêtre d'exécution supporte les instructions en virgule fixe et flottante et l'autre exécute les instructions \texttt{\textbf{load}} et \texttt{\textbf{store}}, les permutations, ainsi que les instructions de branchement. Les opérations simples sur des entiers prennent deux cycles, les opérations sur des flottants simple-précision et les instructions \texttt{\textbf{load}} prennent 6 cycles. Les instructions vectorielles en flottants double-précision sont également supportées et leur débit maximal est d'une instruction tous les 7 cycles pour la première génération du Cell, et 1 cycle pour la dernière génération. Toutes les autres instructions sont entièrement pipelinées quelque soit la génération du processeur.\\
Afin de limiter la complexité du matériel dédié à la prédiction de branchement, une prédiction de branchement peut être fournie par le programmeur ou le compilateur. L'instruction de prédiction de branchement informe le matériel de l'adresse cible du branchement à venir, et le matériel répond en pré-chargeant au moins 17 instructions à partir de l'adresse cible de branchement. Une instruction de masque de sélection par bits peut également être utilisée afin d'éviter les branchements conditionnels dans le code. La surface dédiée aux unités de contrôle ne représente alors que 10 à 15 \% de la surface totale de 10 $mm^{2}$ du SPE. Le SPE ne dissipe que très peu de \emph{Watts} tout en opérant à une fréquence de 3.2 GHz.
%L'ensemble des spécificités des SPEs citées auparavant rendent la programmation de ces derniers fastidieuse. Les instructions arithmétiques des SPEs sont exclusivement vectorielles :  le calcul se fait forcément sur des registres de 128 bits et l'accès à la mémoire doit se faire avec un alignement multiple de 16 octets. Le jeu d'instructions est assez restreint comparé à ses prédécesseurs. En effet le calcul flottant est privilégié au détriment des instructions en nombres entiers souvent suffisantes pour notre domaine d'application. Cela réduit considérablement le degré de parallélisme et  par conséquent le potentiel d'accélération sur ces processeurs.
%La nature distribuée des mémoires locales des SPEs est aussi une particularité dont on à l'habitude sur les systèmes embarqués. En effet, l'espace mémoire sur le Cell n'est pas partagé entre les processeurs mais distribué sur les SPEs qui contiennent chacun une mémoire privée dont l'espace d'adressage est séparé de celui du PPE qui adresse la mémoire externe. La gestion de cet espace mémoire limité est faite de manière explicite par le programmeur sans qu'il ne soit assisté par un cache matériel comme c'est le cas sur les architectures \emph{SMP} à mémoire partagée. Ce dernier aspect, qui complique le travail du programmeur sur le Cell est d'autant plus crucial pour le traitement d'images car le flux de données y est souvent très important et la gestion de la mémoire est un des facteurs clés pour l'obtention de performances optimales sur ce type d'algorithmes.

\subsection{Architecture de communication}
Afin de tirer partie de toute la puissance de calcul enfouie dans le Cell, la charge de travail doit être distribuée et coordonnée entre le PPE et les SPEs. Les mécanismes spécifiques de communication du processeur permettent la collection et la distribution des données ainsi que la coordination d'activités concurrentes de manière efficace entre les unités de calcul. Puisque le SPE ne peut directement agir que sur des données et des instruction présentes dans sa mémoire locale, chaque SPE possède un contrôleur DMA qui effectue des transferts à haut débit entre la mémoire locale et la mémoire principale du Cell. Ces contrôleurs DMA permettent également des transferts directs entre les mémoires locales de deux SPEs dans le cas d'un schéma de calcul du type pipeline ou producteur-consommateur.\\
Par ailleurs, le SPE peut utiliser les signaux out les \emph{mailboxes} pour effectuer des opérations de signalisation avec le PPE ou d'autres SPEs. Il existe également d'autre mécanismes de synchronisation disponibles sur le SPE qui opèrent de la même manière que les instructions atomiques des processeurs PowerPC. En réalité les opérations atomiques sur le SPE interagissent avec celles du PPE pour construire des mécanismes de synchronisation tels que les sémaphores entre PPE et SPE. Enfin, le Cell permet d'accéder à toutes les ressources du SPE ainsi qu'à l'intégralité de la mémoire locale de ce dernier par le biais d'une zone de la mémoire centrale réservée à cet effet. Cette variété de mécanismes de communication permet aux programmeurs d'implémenter différents modèles de programmation pour des applications parallèles \cite{Cell_Kahle_2005}.

\subsubsection{Bus d'interconnection d'éléments \emph{EIB}}
\begin{figure}[!htb]
	\centering
  \includegraphics[width= \columnwidth]{Chapter2/figures/cellnoc}
  \caption{Réseau d'interconnexion du Cell}
  \label{fignoc}
\end{figure}
La figure \ref{fignoc} illustre le \emph{EIB} : le coeur de l'architecture de communication du processeur Cell et qui permet la communication entre le PPE, les SPEs, la mémoire centrale, et les entrées/sorties externes\cite{cellnoc}. 
Le \emph{EIB} possède des chemins de données séparés pour les commandes (requêtes à partir ou vers un élément sur le bus) et les données. Chaque élément du bus est connecté avec une liaison point à point au concentrateur d'adresses, qui reçoit et coordonne les commandes des éléments du bus, diffuse les commandes dans l'ordre à tous les éléments du bus (pour la scrutation), il agrège et diffuse ensuite la réponse des commandes. La réponse à la commande est le signal à l'élément du bus approprié pour démarrer le transfert de données.\\
Le réseau de données du \emph{EIB} est constitué d'anneaux de données d'une largeur de 16 octets : deux dans le sens horaire, et deux dans le sens anti-horaire. Chaque anneau permet jusqu'à trois transferts concurrents, tant que leur chemins ne se croisent pas. Afin d'initier un transfert de données, les éléments du bus doivent demander l'accès au bus de données. L'arbitre de bus du \emph{EIB} traite les requêtes et décide du noeud qui gère une requête donnée. L'arbitre choisit toujours l'anneau qui permet d'accomplir le chemin le plus court, et garantit ainsi que les données ne peuvent pas parcourir plus de la moitié de l'anneau avant d'atteindre leur destination. L'arbitre ordonnance également le transfert afin de s'assurer qu'il n'interfère pas avec une transaction en cours. Afin de minimiser les temps morts lors des lectures, l'arbitre donne la priorité aux requêtes venant du contrôleur mémoire. Il traite les autres requêtes équitablement suivant un ordonnancement \emph{round-robin}. Ainsi, certains schémas de communication seront plus efficaces que d'autres.
Le \emph{EIB} opère à une fréquence deux fois moindres que celle du processeur. Chaque unité \emph{EIB} peut simultanément envoyer et recevoir 16 octets de données durant chaque cycle de bus. La bande passante maximale du \emph{EIB} est limitée par la fréquence à laquelle les adresses sont scrutées à travers les unités du système, dont la valeur est de une adresse par cycle de bus. Chaque scrutation d'adresse peut potentiellement transférer jusqu'à 128 octets. Ainsi, dans un processeur Cell cadencé à 3.2 GHz, la bande passante crête théorique est 128 octets $\times$1.6 GHz = 204.8 Go/s.\\
Les interfaces d'entrées/sorties permettent à deux processeurs Cell d'être connectés en utilisant un protocole cohérent appelé \emph{the broadband interface (BIF)}, qui permet d'étendre le réseau de multiprocesseurs en connectant les PPEs ainsi que 16 SPEs dans un seul réseau cohérent. Le protocole \emph{BIF} opère à travers l'unité \emph{IOIF0} (Fig. \ref{fignoc}), une des deux interfaces d'entrées/sorties. La seconde interface \emph{IOIF1} opère en mode non-cohérent seulement. La bande passante du \emph{IOIF0} est configurée pour 30 Go/s en sortie et 25 Go/s en entrée.\\
La bande passante effective du \emph{EIB} dépend de plusieurs facteurs : les positions relatives de la destination et la source, l'éventualité qu'un transfert puisse croiser d'autres transferts en cours, le nombre de processeurs Cell dans le système, le fait que les transferts soient entre mémoires locales de SPEs ou avec la mémoire principale et enfin de l'efficacité de l'arbitre de bus.\\
Une bande passante réduite peut résulter des situations suivantes :
\begin{itemize}
\item Tous les requérants accèdent à la même destination, par exemple à la mémoire locale en même temps.
\item Tous les transferts sont dans la même direction et causent ainsi des temps morts sur deux ou sur la totalité des anneaux.
\item Un nombre important de transferts de lignes de cache partielles réduit la bande passante.
\item Tous les transferts de données doivent traverser la moitié de l'anneau pour atteindre leur destination et empêchent ainsi les unités sur le chemin d'utiliser l'anneau.
\end{itemize}

\subsection{Contrôleur de flot mémoire (\emph{Memory Flow Controler})}
Chaque SPE contient un \emph{MFC} qui permet de connecter le SPE avec le \emph{EIB} et gère les différents chemins de données entre le SPE et les autres éléments du Cell. Le \emph{MFC} est cadencé à la même fréquence que celle du \emph{EIB} donc à la moitié de celle du processeur. Le SPE interagit avec le \emph{MFC} à travers l'interface de canaux du SPE. Les canaux sont des chemins de communication sans direction qui agissent comme des FIFO à capacité fixe. Cela signifie que chaque canal est défini soit en lecture seule, soit en écriture seule du point de vue du SPE. De plus, certains canaux sont définis avec une sémantique de blocage : ce qui signifie qu'une lecture sur un canal en lecture seule ou une écriture sur un canal plein en écriture seule provoque le blocage du SPE jusqu'à la complétion de l'opération. Chaque canal possède un compteur associé qui indique le nombre d'éléments disponibles dans le canal. Le SPE utilise les instructions assembleur de lecture de canal (\emph{rdch}), d'écriture de canal (\emph{wrch}), et lecture de compteur de canal (\emph{rchcnt}) pour accéder aux canaux du SPE.

\subsubsection{Commandes DMA}
Le \emph{MFC} accepte et traite les commandes DMA que le PPE ou le SPE en utilisant l'interface du canal ou les registres d'entrées/sorties dupliqués en mémoire (\emph{memory mapped I/O} MMIO). Les commandes DMA sont mises en file d'attente dans le \emph{MFC}, et le SPE ou le PPE (celui qui a initié la commande) peut continuer l'exécution en parallèle du transfert de données, en utilisant les canaux de scrutation ou les interfaces bloquantes pour connaitre le statut d'un transfert. Cette exécution autonome des commandes DMA permet de couvrir les transferts par les calculs et d'améliorer ainsi les performances dans les cas où le calcul est dominant sur les transferts.\\
Le \emph{MFC} supporte les transferts alignés de 1, 2, 4, 8 octets et multiples de 16 octets avec un maximum de 16 Ko par transfert. Les DMA \emph{list} peuvent lancer jusqu'à 2048 transferts DMA en utilisant une seul commande DMA du \emph{MFC}. Cependant, il n'y a que le \emph{MFC} associé au SPE qui peut initier de telles commandes. Une DMA \emph{list} est un tableau d'adresses source/destination et de taille de transfert, sauvegardées dans la mémoire locale du SPE. Lorsque un SPE initie une commande DMA \emph{list}, il spécifie l'adresse et la longueur de la liste dans la mémoire locale. Une performance crête est atteignable pour les transferts quand l'adresse effective en mémoire principale et l'adresse en mémoire locale sont alignées sur 128 octets (taille de la ligne de cache) et la taille du transfert est un multiple pair de cette taille.
%Afin d'accéder aux données globales partagées par les threads s'exécutant sur le PPE et ceux exécutés sur le SPE, chaque SPE contient un contrôleur de flot mémoire, qui effectue des transferts entre la mémoire système et les mémoire privées des SPEs. Le contrôleur mémoire fournit au SPEs l'accès à la mémoire système par le biais des transferts DMA entre celle-ci et les mémoire locales des SPE. Les tailles des blocs de transfert varient entre un octet et 16 Ko. Une requête de transfert spécifie l'adresse locale du SPE à partir de l'adresse physique dans le SPE. L'adresse mémoire système est elle spécifiée par une adresse virtuelle qui est traduite en adresse physique par le contrôleur mémoire en se basant sur les tables de pages.    
\subsubsection{Signaux et \emph{mailboxes}}
Le mécanisme de signalisation supporte deux canaux : \emph{Sig\_Notify\_1} et \emph{Sig\_Notify\_2}. Le SPE peut lire ses propres canaux de signalisation en utilisant les lectures bloquantes \emph{SPU\_RdSigNotify1} et \emph{SPU\_RdSigNotify2}. Le PPE ou le SPE peuvent écrire dans ces canaux en utilisant les adresses dupliquées en mémoire centrale. Il existe une fonctionnalité spécifique des canaux de signalisation dans laquelle ils traitent les lectures comme des opérations OU logiques, permettant ainsi une fonctionnalité de communication collective à travers les processeurs.\\
Chaque SPE possède également un ensemble de \emph{mailboxes} qui peuvent fonctionner comme des  canaux de communication étroits (32 bits) entre un SPE et d'autres SPEs ou le PPE. Le SPE possède une \emph{mailbox} de réception à quatre entrées à lecture bloquante, et deux \emph{mailboxes} d'émission à entrée unique en écriture bloquante également. Une de ces dernières peut générer une interruption pour le PPE lorsque le SPE y écrit. Le PPE utilise les adresses dupliquées en mémoire pour écrire dans les boites de réception des SPEs et pour lire dans une des boites d'envoi du SPE. Contrairement aux canaux de signalisation, les \emph{mailboxes} se prêtent plus à des schémas de communication point à point tels que des modèles maître-esclave ou producteur-consommateur. Une communication en émission-réception typique, en utilisant les \emph{mailboxes} dure approximativement 300 nano secondes.\\

\subsubsection{Opérations atomiques}
Afin de supporter des mécanismes de communication plus complexes, le SPE peut utiliser des commandes DMA spécifiques afin de mettre à jour de manière atomique une ligne bloquée en mémoire principale. Ces opérations appelées \emph{get-lock-line-and-reserve} (\emph{getllar}) et \emph{put-lock-line-conditional} (\emph{putllc}), sont conceptuellement équivalentes aux instructions \emph{load-and-reserve} (\emph{lwarx}) et \emph{store-conditional} (\emph{stcwx}) du \emph{PowerPC}.
L'instruction \emph{getllar} lit la valeur d'une variable de synchronisation dans la mémoire principale et réserve son adresse. Si le PPE ou le SPE modifie la variable de synchronisation par la suite, le SPE perd sa réservation. L'instruction \emph{putllc} ne met à jour une variable de synchronisation que si le SPE possède une réservation sur son adresse. Si \emph{putllc} échoue, le SPE doit relancer une instruction \emph{getllar} pour obtenir la nouvelle valeur de la variable de synchronisation et réessayer par la suite de la mettre à jour. L'unité atomique du \emph{MFC} effectue les opérations DMA atomiques et gère les réservations prises par le SPE.\\
En utilisant les mises à jour atomiques, le SPE peut participer avec le PPE et d'autres SPEs à des protocoles de sémaphores, des barrières ou d'autres mécanismes de synchronisation.

\subsubsection{Entrées/sorties dupliquées en mémoire : \emph{Memory-Mapped I/O}}
Les ressources dupliquées en mémoire jouent un rôle majeur dans la plupart des mécanismes de communication discutés auparavant. Elles se divisent en quatre catégories :
\begin{itemize}
\item \textbf{Mémoire locale} : la mémoire locale du SPE peut être entièrement dupliquée dans l'espace d'adressage effectif. Cela permet au PPE d'accéder à l'espace des SPE par l'intermédiaire d'opérations \texttt{\textbf{load/store}} simples, même si cela est beaucoup moins efficace qu'une opération DMA. l'accès à la mémoire locale dupliquée n'est pas synchronisé avec l'exécution du SPE. Ainsi le programmeur doit s'assurer que le programme SPE est conçu de telle sorte à autoriser des accès non synchronisés à ces données (par exemple, en utilisant des variables "volatiles").
\item \textbf{Zone de \emph{problem state}} : Les ressources de cet espace sont destinées à être utilisées directement par les applications, elles incluent l'accès au contrôleur DMA, au \emph{mailboxes} et aux canaux de notification des signaux. 
\item \textbf{Zone \emph{Privilege 1}} : Ces ressources sont disponibles à des programmes privilégiés tel que le système d'exploitation.
\item \textbf{Zone \emph{Privilege 2}} : Le système d'exploitation utilise ces ressources pour contrôler les ressources disponibles sur le SPE
\end{itemize}

\subsection{Flot d'exécution DMA}
Le contrôleur DMA du SPE gère la plupart des communications entre le SPE et les autres éléments du Cell, il exécute également les commandes DMA initiées par le PPE ou par d'autres SPEs. Une direction de transferts de données est toujours vue du côté SPE. Ainsi, les commandes qui transfèrent les données dans un SPE (à partir du PPE ou d'un autre SPE) sont considérées comme des opérations \emph{get}, alors que les transferts du SPE vers la mémoire principale ou celle d'un autre SPE sont considérées comme des \emph{puts}.\\
Les transferts DMA sont cohérents par rapport à la mémoire principale. Le \emph{MFC} peut traiter les commandes DMA dans la file d'attente dans un ordre différent de celui de leur entrée dans la file d'attente. Lorsque l'ordre est important, le programmeur doit utiliser les commandes adéquates de \emph{put} et \emph{get} pour forcer des mécanismes de barrières par rapport à d'autres transferts dans la file d'attente.
La \emph{MMU} (\emph{Memory Management Unit}) gère la traduction d'adresses des accès DMA à la mémoire principale, en utilisant des informations provenant des segments et des tables définies dans l'architecture \emph{PowerPC}. Le \emph{MMU} possède un \emph{TLB} (\emph{Transfer Lookaside-Buffer}) pour mettre en cache les résultats des traductions faites récemment.\\
Le contrôleur DMA traite les commandes DMA présentes dans le \emph{MFC}. Ce dernier possède deux files d'attente distinctes :
\begin{itemize}
\item \emph{File d'attente SPE} : pour les commandes initiées par le SPE en utilisant l'interface de canaux.
\item \emph{File d'attente proxy} : pour les commandes initiées par le PPE ou d'autre éléments en utilisant les registres \emph{MMIO}.
\end{itemize}
La file d'attente SPU contient 16 entrées et la file d'attente proxy en contient 8.
Le flot basique d'un transfert DMA vers la mémoire principale initié par le SPE est le suivant :
\begin{enumerate}
\item Le SPE utilise l'interface de canal pour placer la commande DMA dans la file d'attente du \emph{MFC}.
\item Le contrôleur DMA sélectionne une commande pour le traitement. 
\item Si la commande est une DMA \emph{list} et requière le chargement d'une liste. Le contrôleur DMA charge la liste dans la mémoire locale. Une fois que l'élément de la liste est chargé, les champs de l'élément sont mis à jour et le transfert suivant peut commencer
\item Si la commande requière une traduction d'adresse, le contrôleur la met dans le \emph{MMU} pour le traitement. Lorsque la traduction est disponible dans le TLB, le traitement atteint l'étape suivante (déroulage). dans le cas d'un échec de lecture dans le TLB, le \emph{MMU} effectue la traduction en utilisant les pages de tables en mémoire principale et met à jour le TLB.
\item Le DMA déroule la commande : il crée une requête sur le bus pour transférer le bloc de données suivant. Cette requête de bus peut transférer jusqu'à 128 octets de données mais peut également transférer une quantité moindre, en fonction des problèmes d'alignement ou de la quantité de données à transférer. Le contrôleur soumet  la requête à l'interface de bus (\emph{BIU}).
\item le \emph{BIU} effectue les lectures dans la mémoire locale nécessaires au transfert, il envoie ensuite les données vers le \emph{MIC} (\emph{Memory Interface Controler}). Ce dernier transfert les donnés vers ou à partir de la mémoire principale.
\item Le processus de déroulage produit une séquence de requêtes de bus pour la commande DMA, qui traverse le réseau de communication. La commande DMA demeure dans la file d'attente jusqu'à ce que toutes les requêtes soient terminées. Toutefois, le contrôleur peut très bien continuer à dérouler d'autres commandes. Lorsque toutes les requêtes vers le bus d'une commande sont terminées le contrôleur signale la complétion de la commande et la retire de la file d'attente.
\end{enumerate}
En cas d'absence de congestion, un \emph{thread} qui s'exécute sur le SPE peut lancer une requête DMA en 10 cycles d'horloge qui correspondent au temps d'écriture sur les canaux qui décrivent la source, la destination, la taille du transfert et l'étiquette de la commande. A partir de ce moment là, le contrôleur DMA peut exécuter la commande sans l'aide du SPE.\\
La latence globale, due à la génération de la commande DMA est de 30 cycles d'horloge lorsque toutes les ressources sont disponibles. Si le chargement d'un élément de liste est requis, cela rajoute 20 cycles supplémentaires. Si la file d'attente dans le \emph{BIU} est pleine, le contrôleur est bloqué jusqu'à ce que les ressources soient disponibles à nouveau.\\
Une commande de transfert implique la scrutation de tous les éléments du bus pour assurer la cohérence et requière ainsi 50 cycles supplémentaire (100 cycles au total). Pour les commandes de type \emph{get}, la latence restante est attribuée au transfert des données de la mémoire externe au contrôleur mémoire qui transitent seulement après par le bus interne vers la mémoire locale des SPEs. Pour les opérations \emph{put} la latence n'inclue pas le temps de parcours du bus vers la mémoire externe car le SPE considère que le \emph{put} est terminé une fois que les données ont été transmises au contrôleur mémoire.
%\subsection{Gestion de la mémoire}
%Dans une application pour le processeur Cell, plusieurs SPEs peuvent partager un espace mémoire commun avec les threads PPE. En même temps, d'autres SPEs peuvent référencer des espaces mémoire virtuels associés à des applications qui s'exécutent de manière concurrente sur le système. Le support de cette fonctionnalité est assuré par l'unité de gestion mémoire (\emph{Memory Management Unit}) qui permet de traduire les adresses système lors de la requête. Le contrôleur participe aux protocoles de cohérence de la mémoire afin d'assurer la cohérence des tables de pages.\\
%Chaque contrôleur peut être programmé pour effectuer des transferts mémoire soit à partir du SPE en insérant une commande dans la file d'attente locale soit à partir d'un noeud distant. Le contrôleur mémoire peut également participer à des opérations de synchronisation de la mémoire et des threads. Un mécanisme de liste de transferts est également supporté par le contrôleur qui permet d'englober une séquence de transferts au sein d'une même commande.

%\subsection{Contrôleur de flot mémoire (\emph{Memory Flow Controler})}
%Afin d'accéder aux données globales partagées par les threads s'exécutant sur le PPE et ceux exécutés sur le SPE, chaque SPE contient un contrôleur de flot mémoire, qui effectue des transferts entre la mémoire système et les mémoire privées des SPEs. Le contrôleur mémoire fournit au SPEs l'accès à la mémoire système par le biais des transferts DMA entre celle-ci et les mémoire locales des SPE. Les tailles des blocs de transfert varient entre un octet et 16 Ko. Une requête de transfert spécifie l'adresse locale du SPE à partir de l'adresse physique dans le SPE. L'adresse mémoire système est elle spécifiée par une adresse virtuelle qui est traduite en adresse physique par le contrôleur mémoire en se basant sur les tables de pages.    



%Le bus interne du processeur permet de relier les unités de traitement PPE, SPE  la fois entre elles,  la mémoire centrale ainsi qu'aux sorties externes. Le bus contient des chemins de données différents de ceux des requêtes. Les éléments autour du bus sont connectés par des liaisons point-à-point et un arbitre de bus est responsable de la réception des commandes et de leur diffusion vers les unités. Le bus est constitué de quatre anneaux d'une largeur de 16-octets, deux fonctionnent dans le sens d'une aiguille d'une montre et les deux autres dans le sens inverse. Chaque anneau peux potentiellement gérer 3 transferts en parallèle si toutefois leurs chemins ne se croisent pas. Le EIB opère à une fréquence qui est la moitié de celle du processeur, chaque unité du bus peut simultanément envoyer et recevoir 16 octets par cycle d'horloge du bus.

\subsection{Programmabilité et modèles de programmation}
\label{prog_models}
Si l'innovation dans l'architecture matérielle peut permettre d'atteindre de nouveaux niveaux de performances et/ou d'efficacité énergétique, il va de soi que l'effort fourni pour améliorer les performances doit être raisonnable. La programmabilité du Cell a été un souci pour ses concepteurs dès ces débuts, ils ont essayé de rendre le système le plus programmable possible et accessible au plus grand nombre. Mais il est clair que les aspects architecturaux qui sont les plus difficiles à appréhender par les programmeurs sont ceux qui renferment le plus grand potentiel d'amélioration des performances. L'existence de la mémoire locale et le fait que celle-ci doit être gérée par le logiciel est un bon exemple de verrou technologique. Cette gestion peut éventuellement être confiée à un compilateur mais la tâche peut être complexe suivant le cas d'utilisation.
Le second aspect de la conception qui affecte la programmabilité est la nature SIMD du flot de données. Le programmeur peut ignorer cet aspect là de l'architecture, mais en faisant abstraction de celui-ci, une grande partie de la performance est laissée de côté. Il faut noter, que comme pour les applications qui s'exécutent sur des processeurs classiques sans utiliser leur unité SIMD, le Cell peut être programmé comme un processeur scalaire. La nature SIMD des SPEs est gérée par les programmeurs et supportée par les compilateurs de la même manière que pour les processeurs possédant des unités SIMD.\\
Le SPE diffère des processeurs généralistes par plusieurs aspects : la taille du banc de registres (128 entrées), la manière dont les branchements et les   instructions qui permettent de synchroniser la mémoire locale avec le lancement des instructions sont gérés . Ces particularités peuvent être utilisées par un compilateur pour l'optimisation, et le programmeur a intérêt  d'en avoir connaissance afin de tirer profit au maximum des dispositifs de l'architecture, mais en tout état de cause il n'est pas nécessaire de programmer les SPEs en assembleur. Une autre différence majeure qui distingue les SPEs des processeurs conventionnels est le fait que ceux-ci ne puissent supporter qu'un seul contexte de programme à la fois. Ce contexte peut être un \emph{thread} dans une application ou un \emph{thread} dans un mode privilégié, qui étend le système d'exploitation. Le processeur Cell supporte la virtualisation et permet à plusieurs systèmes d'exploitation de s'exécuter de manière concurrente au dessus d'un logiciel de virtualisation qui s'exécute en mode hyperviseur.\\
L'intégration d'un processeur de contrôle compatible avec l'architecture PowerPC  permet au Cell d'exécuter les applications Power et PowerPC 32 et 64 bits sans aucune modification. Toutefois, l'utilisation des SPEs est nécessaire pour atteindre les performances et profiter pleinement de l'efficacité énergétique du Cell. L'ensemble des dispositifs de communication et de transfert de données permet d'imaginer plusieurs modèles de programmation possibles pour le Cell. Par exemple il est possible d'utiliser le SPE comme coprocesseur sur lequel on décharge une partie du calcul afin de l'accélérer. Plusieurs changements ont été effectué sur l'architecture pour des raisons de programmabilité. Des programmes de test, des bibliothèque de calcul, des extensions de systèmes d'exploitation ont été écrits, analysés vérifiés sur un simulateur fonctionnel avant la finalisation de l'architecture et l'implémentation du Cell. Certains des modèles de programmation les plus communs sont décrits dans ce qui suit.
\subsubsection{Modèle de décharge de fonction}
Ce modèle peut être le plus rapide à mettre en oeuvre tout en bénéficiant du fort potentiel de performances du Cell. Dans ce modèle de programmation, les SPEs sont utilisés comme des accélérateurs de certaines fonctions critiques. L'application principale peut être une application existante ou une nouvelle application qui s'exécute sur le PPE. Dans ce modèle les fonctions critiques invoquées par le programme principal sont remplacées par des fonctions qui s'exécutent sur un ou plusieurs SPEs. La logique du programme principal reste inchangée. La fonction originale est optimisée et recompilée pour le SPE, et l'exécutable SPE généré est intégré au programme PPE dans une section en lecture seule avec la possibilité de l'invoquer à distance. Le programmeur détermine statiquement les portions de code à exécuter sur le PPE et celles à décharger sur les SPEs. Un compilateur \emph{single-source} (à un seul fichier source en opposition à la compilation séparée du code PPE d'un côté et celui du SPE de l'autre) a été conçu par IBM. Ce dernier se base sur des directives de compilation qui permettent de décharger le calcul sur les SPEs pour une portion délimitée du code. Une des évolutions les plus significatives que pourrait connaitre les compilateurs est de pouvoir déterminer automatiquement les portions de code à déporter sur les SPEs.
\subsubsection{Modèle d'accélération du calcul}
Ce modèle est centré sur le SPE : il fournit une utilisation plus importante du SPE par l'application que le modèle de décharge de fonction. Le modèle est mis en oeuvre en effectuant les tâches de calcul intensif sur les SPEs. le code source PPE agit essentiellement comme serveur et unité de contrôle pour les SPEs. Les techniques de parallélisation peuvent être utilisées pour répartir le calcul sur les SPEs. La charge de travail peut être partitionnée manuellement par le programmeur ou parallélisée automatiquement par un compilateur. Ce partitionnement doit inclure un ordonnancement efficace des opération DMA pour le mouvement des données et du code entre les SPEs. Ce modèle de programmation peut s'appuyer sur une mémoire partagée ou sur un système par passage de message. Dans plusieurs situations, le modèle d'accélération de calcul peut être utilisé pour fournir une accélération à des fonctions mathématiques de calcul intensif sans que le code ne soit modifié de manière significative.
\subsubsection{Modèle de calcul par flux}
Comme mentionné auparavant, les SPEs peuvent se baser sur un système par passage de message pour faire un calcul en parallèle. Il est donc très simple de mettre en oeuvre des pipelines séquentiels ou parallèles, dans lesquels chaque SPE effectue un calcul distinct sur les données qui transitent par lui. Le PPE peut alors agir comme contrôleur de flux et les SPEs comme des processeurs de flux de données. Lorsque les SPEs ont une charge de travail équivalente, ce modèle peut s'avérer très efficace sur le Cell, à partir du moment où les données résident le plus longtemps possible dans les mémoires internes du SPE. Dans certains cas, il peut être plus efficace de faire migrer le code d'un SPE vers un autre SPE au lieu du mouvement de données plus conventionnel.
\subsubsection{Modèle à mémoire partagée}
Le processeur Cell peut être programmé comme un multi-processeur à mémoire partagée ayant des unités avec des jeux d'instructions différents qui permettent de couvrir un spectre de tâches qu'un seul jeu d'instructions ne peut pas couvrir. Le PPE et les SPEs peuvent interagir dans un modèle à mémoire partagée complètement cohérent. Les \texttt{\textbf{load}}s conventionnels en mémoire partagée sont remplacés par une combinaison d'opérations DMA de la mémoire partagée vers la mémoire locale du SPE, en utilisant une adresse effective commune au PPE et aux SPEs, et un \texttt{\textbf{load}} de la mémoire locale vers le banc de registres. Les opérations conventionnelles du type \texttt{\textbf{store}} sont remplacées par une combinaison d'un \texttt{\textbf{store}} du banc de registres vers la mémoire locale et d'une opération DMA de la mémoire locale vers la mémoire partagée en utilisant le même mécanisme d'adressage que pour le \texttt{\textbf{load}}. On peut alors imaginer un compilateur qui s'appuie sur les mêmes mécanismes pour utiliser la mémoire locale des SPEs comme un cache de données et d'instructions lues à partir de la mémoire partagée. 

%====================================================== THREADS POSIX ======================================================================================================================================
%\section{Les threads POSIX}
%Les \emph{threads} POSIX\footnote{Portable Operating System Interface for Unix} ou \emph {Pthreads} sont une standardisation\cite{pthreads_std} du modèle de programmation par \emph{threads} pour les systèmes UNIX. Ce modèle est basé sur une API de programmation parallèle qui permet la gestion des \emph{threads} ainsi que la synchronisation par \emph{mutex} ou variables conditionnelles. Historiquement, les concepteur de \emph{hardware} ont développé leurs implémentations propriétaires des \emph{threads}, ceci a rendu la portabilité du code des programmeurs quasi impossible. La nécessité d'une API standard est donc devenue vitale, c'est pour cette raison que la majorité des vendeurs de \emph{hardware} possèdent actuellement leur implémentation standard des \emph{threads} POSIX. Les \emph{Pthreads} sont définis autour d'un ensemble de procédures et de types en langage C contenus dans le fichier d'entête \texttt{"pthread.h"}.\\
%D'une manière générale un \emph{thread} est un flux d'instructions pouvant s'exécuter de manière indépendante sur un OS donné. Du point de vue du programmeur ceci s'apparente à une procédure qui peut s'exécuter indépendamment du programme principal, un programme contenant des procédures de ce type est dit \emph{multi-threaded}. Afin de détailler le principe de fonctionnement des \emph{threads} il est nécessaire de faire un rappel sur les \emph{process} UNIX. Un \emph{process} est créé par l'OS est contient un certain \emph{overhead} qui consiste en certaines ressources nécessaires à son exécution. Les \emph{threads} résident à l'intérieur de ces ressources et sont capables d'être ordonnancés et exécutés en tant qu'entités indépendantes car ils ne dupliquent qu'une partie des ressources qui leurs permettent d'être des morceaux de code exécutables. Le flot de contrôle est rendu indépendant car le \emph{thread} possède ses propres : pointeur de pile, registres, propriétés d'ordonnancement (priorité et politique), ensemble de signaux et données spécifiques. En somme, un \emph{thread} existe dans un \emph{process} dont il utilise les ressources. Il possède son propre flot de contrôle et ne duplique que les ressources nécessaires à son exécution indépendante. Il peut partager les ressources du \emph{process} avec d'autres \emph{threads} et s'exécuter en coordination avec ces derniers. La complexité due à sa création et à sa gestion est légère comparativement à celle du \emph{process} et sa durée de vie est celle de son \emph{process} parent.
%\subsection{l'API pthread}
%L'API des \emph{threads} POSIX peut être décomposée en trois types de routines:
%\begin{itemize}
%\item \textbf{Les routines de gestion des \emph{threads}} : comprends les tâches de création, propriétés d'exécution et terminaison des \emph{threads}.
%\item \textbf{Les \emph{mutex}} (abréviation de \emph{mutual exclusion}): ils permettent de synchroniser les \emph{threads}, les routines gèrent la création, destruction, réservation et libération des \emph{mutex}.
%\item \textbf{Les variables conditionnelles} : ces dernières gèrent la communication entre des \emph{threads} qui partagent les \emph{mutex}. Elles sont basées sur des conditions fixées par le programmeur, elles incluent des fonctions de création, destruction, attente et signalisation basées sur certaines valeurs de ces variables. 
%\end{itemize}
%\rule{\textwidth}{0.2mm}\\
%\subsubsection{Gestion des threads}
%\begin{itemize}
%\item \textbf{Création et Terminaison des \emph{threads}}: Initialement le programme contient un seul \emph{thread} qui est le \texttt{main()}. Les autres \emph{threads} doivent être créés explicitement par le programmeur. \texttt{pthread\_create} créé un nouveau \emph{thread} et le rend exécutable, un exemple de code à base de \emph{Pthreads} est donné dans le listing \ref{pthreadcode}. Cette routine peut être appelée autant de fois que l'on veut et à n'importe quel endroit dans le code. Le nombre de \emph{threads} maximal créé par un \emph{process} dépend de l'implémentation. Une fois créés les \emph{threads} peuvent créer à leur tour d'autres \emph{threads} et il n'existe aucune dépendance ni hiérarchie entre les \emph{threads}. Le \emph{thread} est crée avec certains attributs par défaut, ceux-ci pouvant être changés ultérieurement. Il existe plusieurs manières de terminer un \emph{thread} : soit par le \emph{thread} lui même qui le fait par un \texttt{return} de sa routine principale ou par la fonction \texttt{pthread\_exit()}, ou alors par un autre \emph{thread} en utilisant \texttt{pthread\_cancel()} et enfin en cas de terminaison du \emph{process} parent.
%\item \textbf{Passage d'Arguments au \emph{threads}} : On peut passer un argument au \emph{thread} via la routine \texttt{pthread\_create()}. On peut également passer plusieurs arguments en les rassemblant dans une structure et en passant l'adresse de celle-ci.
%\item \textbf{Jonction et Détachement des \emph{threads}} : La jonction est une manière de faire une synchronisation entre les \emph{threads}, la fonction \texttt{pthread\_join()} bloque le \emph{thread} appelant jusqu'a ce que le \emph{thread} appelé termine son exécution. Le caractère joignable ou pas d'un \emph{thread} est spécifié à sa création : s'il est créé en tant que \emph{thread} détaché il ne pourra pas être joignable. La routine \texttt{pthread\_detach()} sert à détacher un \emph{thread} qui était joignable à sa création.
%\item \textbf{Gestion de la Pile} : Le standard ne définit pas la taille par défaut de la pile du \emph{thread}, celle-ci dépend de l'implémentation. Toutefois, le programmeur peut en spécifier la taille ainsi que l'emplacement de la mémoire dans laquelle elle doit être stockée.
%\end{itemize}

%\subsubsection{Les variables \emph{mutex}}
%Les \emph{mutex} sont un des principaux mécanismes de synchronisation de \emph{threads}. Ils permettent par exemple de synchroniser des \emph{threads} ou alors de protéger des données partagées en cas d'écriture simultanée. Un \emph{mutex} peut être réservé (\emph{lock}) par un seul \emph{thread} à un moment donné, le propriétaire du \emph{mutex} est le seul à le posséder : tout autre \emph{thread} qui essaye de réserver ce même \emph{mutex} échoue jusqu'à ce que le propriétaire le libère (\emph{unlock}). Il existe des routines de création et de destruction des \emph{mutex}, la réservation des \emph{mutex} se fait soit par appel à la routine \texttt{pthread\_\emph{mutex}\_lock()} (bloquant), \texttt{pthread\_\emph{mutex}\_trylock()} (non bloquant) et se libère par \texttt{pthread\_\emph{mutex}\_unlock()}. Parmi les exemples d'utilisation des \emph{mutex} on peut citer les cas de \emph{race condition} où plusieurs \emph{threads} essayent de mettre à jour une variable globale, il est nécessaire dans ce genre de situation de protéger cette variable par un \emph{mutex} afin que celle-ci ait la même valeur du point de vue de tous les \emph{threads}. On dit alors que l'on crée une \emph{section critique}.
%===============================Listing Pthreads =======================================
%\lstset{ %
%language=C,                % choose the language of the code
%basicstyle=\footnotesize,       % the size of the fonts that are used for the code
%backgroundcolor=\color{light-gray},  % choose the background color. You must add \usepackage{color}
%showspaces=false,               % show spaces adding particular underscores
%showstringspaces=false,         % underline spaces within strings
%showtabs=false,                 % show tabs within strings adding particular underscores
%frame=single,			% adds a frame around the code
%tabsize=2,			% sets default tabsize to 2 spaces
%captionpos=b,			% sets the caption-position to bottom
%breaklines=true,		% sets automatic line breaking
%breakatwhitespace=false,	% sets if automatic breaks should only happen at whitespace
%escapeinside={\%*}{*)}  ,        % if you want to add a comment within your code
%caption = Exemple de code \emph{Pthread} basique montrant les routines de création de \emph{threads},
%label = pthreadcode
%}
% \lstinputlisting{Chapter2/Code/pthreadexample.c}
%===================================================================================
%\subsubsection{Les variables de condition}
%Les variables de condition fournissent aux \emph{threads} une autre manière de se synchroniser. Contrairement aux \emph{mutex} qui sont basés sur le contrôle de l'accès à une variable, la synchronisation par variable de condition se fait selon une valeur spécifique. Les variables de condition sont utilisées en conjonction avec les \emph{mutex}. L'appel à la fonction \texttt{pthread\_cond\_wait()} bloque le \emph{thread} appelant jusqu'à ce que la condition spécifiée est signalée. Cette routine doit être appelée tant que le \emph{mutex} est réservé et libère automatiquement le \emph{mutex} quand elle est en attente. Une fois que le signal est reçu, le \emph{thread} est réveillé et le \emph{mutex} est réservé automatiquement pour être utilisé par le \emph{thread}. La responsabilité de libérer le \emph{mutex} est à la charge du programmeur qui le fait une fois que son utilisation n'est plus nécessaire. La fonction \texttt{pthread\_cond\_signal()} est utilisée pour signaler (ou réveiller) un autre \emph{thread} qui est en attente de la variable de condition. Elle doit être appelée après que le \emph{mutex} est réservé et doit libérer le \emph{mutex} dans l'ordre pour permettre à \texttt{pthread\_cond\_wait()} de s'achever. Il existe une routine nommée \texttt{pthread\_cond\_broadcast()} qui met en attente plusieurs \emph{threads} en même temps. 

\subsection{Environnement de développement de base}
L'environnement de développement fourni par IBM est le \emph{Cell Software Development Kit (Cell SDK)}. Plusieurs versions ont vu le jour, la première a précédé la livraison des premières puces Cell. L'exécution du code se faisait alors sur un simulateur \emph{cycle-accurate} du processeur \cite{cellsystemsim}. La dernière version du SDK connue à ce jour est la 3.1. La description qui suit est basée sur une version antérieure 2.1 du SDK. L'environnement de développement contient les composants suivants : 
\begin{itemize}
\item \textbf{La chaîne d'outils GNU : } Cette chaîne contient le compilateur \emph{gcc} (\emph{GNU C-language Compiler}) pour le PPE et le SPE. Les deux compilateurs peuvent effectuer de la \emph{cross} compilation sur des plateformes x86. La présence d'une suite d'outils \emph{open source} est le fruit d'une stratégie d'ouverture d'IBM vers le développement communautaire décrite dans \cite{Cell_Gschwind_2007};
\item \textbf{Le compilateur IBM XL C/C++ : } C'est le compilateur conçu par IBM pour le processeur Cell \cite{Cell_Eichenberger_2006}. Il contient également deux compilateurs spécifiques, un pour le PPE; l'autre pour le SPE. La \emph{cross} compilation est également possible avec les compilateurs \emph{XLC} . La présence de deux chaînes de compilateurs distinctes s'explique par le fait que \emph{gcc} est destiné à la communauté \emph{open source} et dépend de sa contribution, alors que le compilateur \emph{XLC} est développé et commercialisé par IBM. Dans nos travaux nous avons pu utiliser les deux compilateurs. \emph{XLC} se distingue de \emph{gcc} par la gestion d'OpenMP et un code généré plus efficace sur le PPE et le SPE.
\item \textbf{IBM full-system simulator : } Le simulateur du Cell est une application logicielle qui émule le comportement d'un système complet contenant un processeur Cell. Un système d'exploitation linux est géré par le simulateur. On peut ainsi, exécuter des applications pré-compilées sur le simulateur. Il existe plusieurs modes de simulation possibles, du mode purement fonctionnel, au mode précis au cycle près (\emph{cycle accurate}).
\item \textbf{Noyau Linux : } C'est le noyau Linux pour le Cell \cite{celllinuxkernel} qui supporte notamment l'exécution parallèle sur les SPEs. Il est développé et maintenu par le \emph{Barcelona Supercomputer Center}. Celui-ci s'exécute sur le PPE qui peut gérer un système d'exploitation alors que les SPEs sont des coprocesseurs dédiés au calcul intensif.
\item \textbf{La bibliothèque de gestion du support d'exécution SPE : } Cette bibliothèque constitue l'API bas-niveau standard pour la programmation d'applications pour le Cell, en particulier pour l'accès au SPEs. La bibliothèque fournit une API neutre par rapport aux systèmes d'exploitation et la manière dont ils gèrent les SPEs, à travers une interface standard très proche de POSIX.
\end{itemize}
Nous avons cité ci-dessus les principaux composants du SDK\nomenclature{SDK}{Software Development Kit} que nous avons utilisé lors du développement des applications sur le processeur Cell. D'autres composants sont fournis avec le SDK : des bibliothèques de calcul mathématique et numérique optimisées pour le Cell, des utilitaires de mesure de performance ainsi qu'un \emph{plugin} pour l'environnement de développement Eclipse spécifique au développement sur le processeur Cell.

\subsubsection{Les threads POSIX sur le Cell}
Sur un système Linux pour le Cell, le \emph{thread} principal s'exécute sur le PPE. Un \emph{thread} PPE peut contenir un ou plusieurs \emph{threads} Linux qui peuvent s'exécuter soit sur le PPE soit sur les SPEs. Un \emph{thread} qui s'exécute sur le SPE possède son propre contexte incluant un banc de registres de 128 $\times$ 128 bits, un compteur de programme et une file d'attente de commandes MFC, et il peut communiquer avec d'autres unités d'exécution au travers de l'interface des canaux MFC. Un \emph{thread} PPE peut interagir directement avec un \emph{thread} SPE via sa mémoire locale ou à travers un espace alloué en mémoire principale nommé \emph{problem state space}, ou encore indirectement via la mémoire centrale ou les routines de la \emph{SPE Runtime Management Library}. L'OS définit le mécanisme et la politique d'ordonnancement pour les SPEs disponibles, il est également responsable de la gestion des priorités entre les tâches, du chargement du programme, de la notification des événements au SPEs ainsi que du support du \emph{debugger}.
Une API de gestion des \emph{threads} SPE similaire à la bibliothèque POSIX a été conçue, dans le but de fournir à la fois un environnement de programmation familier et une flexibilité dans la gestion des SPEs. Cette API supporte à la fois la création et la terminaison des tâches SPE ainsi que l'exclusion mutuelle par des primitives de mise à jour atomiques. L'API peut accéder aux SPEs en utilisant un modèle virtuel dans lequel l'OS affecte dynamiquement les \emph{threads} aux SPEs dans l'ordre de leur disponibilité. Les applications, peuvent spécifier de manière optionnelle un masque d'affinité pour affecter les \emph{threads} à un SPE spécifique. Les dispositifs architecturaux de communication entre les \emph{threads} ainsi que les mécanismes de synchronisation (mailbox, signaux, etc...) peuvent être accèdés via un ensemble d'appels système ou alors via l'application qui mappe un bloc de contrôle du SPE dans l'espace mémoire de l'application. Sur le Cell il existe trois blocs de contrôle du SPE, un accédé par l'application, un autre par l'OS et un troisième par un superviseur. Une interface accessible à l'utilisateur permet  la communication directe entre les processeurs SPEs ou PPE, ceci permet d'éviter des appels système couteux.\\
Lorsque l'application fait une requête de création de \emph{threads}, la bibliothèque de \emph{threads} SPE envoie la requête à l'OS pour allouer un SPE et créer un \emph{thread} SPE à partir d'un fichier objet de format ELF (\emph{Executable and Linkable Format}) intégré dans un exécutable Cell. Le \emph{miniloader} un programme SPE de 256 bits, charge le segment de code à exécuter sur le SPE. L'avantage de cette approche est d'une part d'éviter au PPE d'effectuer cette tâche et d'autre part de profiter du fait que les les transferts PPE-SPE quand il se font du côté SPE, sont nettement plus efficace grâce à une interface qui contient plus de canaux de communication. Le code à exécuter réside alors dans la mémoire locale des SPEs.

\subsubsection{Processus de génération de code}
\begin{figure}[!htbf]
	\centering
	\includegraphics[scale =0.6]{Chapter2/figures/cell_compilation_process}
	\caption{Processus \emph{dual source} de génération de code exécutable pour le Cell}
  \label{fig_compprocess}
\end{figure}
Dans le modèle de programmation décrit ci-dessus, le processus de génération de code binaire exécutable sur le Cell est dit \emph{dual-source}. En effet, il existe deux codes sources distincts, un code pour le SPE (\texttt{spe.c} sur la figure \ref{fig_compprocess}) qui contient le code exécuté sur le SPE. Un deuxième code source qui est celui s'exécutant sur le PPE (\texttt{ppe.c} sur la figure \ref{fig_compprocess})  contient le \emph{\emph{thread}} maître qui gère les \emph{\emph{threads}} SPE. Le processus de génération de code exécutable est décrit dans la figure \ref{fig_compprocess}. Dans la première étape le code SPE est compilé et un binaire exécutable SPE est ainsi généré. Celui-ci est par la suite traité par un outil spécifique \emph{spu-embedd} qui permet de transformer ce binaire code binaire pouvant être enfoui dans l'exécutable du PPE. Cette procédure se fait par l'éditeur de liens qui considère alors le code SPE comme une bibliothèque dont le code objet doit être intégré dans l'exécutable final.

\section{Conclusion}
Les architectures parallèles ont fait leur apparition dans les premières années de l'informatique moderne. Le parallélisme y est exploité à plusieurs niveaux, instructions, données et tâches. Il existe plusieurs types de hiérarchies mémoire pour machines parallèles : essentiellement partagées et distribuées. Afin de pouvoir exploiter le parallélisme et la hiérarchie mémoire de ce type de machines, plusieurs modèles de programmation ont été proposés, leur but étant de faciliter la programmation ainsi que d'atteindre de meilleures performances grâce à une exécution parallèle.\\
La parallélisation de code qui consiste en l'adaptation d'un code source séquentiel pour son exécution sur une machine parallèle pose plusieurs défis. L'algorithme ainsi que son implémentation doivent être adaptés à une exécution parallèle. L'exécution concurrente induit des contraintes de synchronisation ainsi que la nécessité d'une communication inter-processeurs. Ces facteurs peuvent freiner l'exécution efficace du programme en parallèle.\\
Le processeur Cell possède une architecture parallèle hétérogène complexe. Il renferme plusieurs dispositifs qui font de son architecture un concentré de technologies parallèles. Plusieurs formes de parallélisme y sont présentes et à plusieurs niveaux:  le parallélisme de données résultant de la nature vectorielle (SIMD) des processeurs SPE, le parallélisme d'instructions grâce à la possibilité d'exécuter deux instructions par cycle, le parallélisme de tâches car plusieurs \emph{threads} peuvent s'exécuter de manière concurrente sur différents SPEs et enfin le parallélisme transfert/calcul qui résulte de contrôleurs DMA indépendants des unités de calcul sur les SPEs. L'architecture mémoire quand à elle, est similaire à celle d'un DSP embarqué. Cette hiérarchie mémoire distribuée nécessite une gestion explicite pour une utilisation efficace.
La programmation par \emph{threads} sur le Cell est le modèle de base pour la mise en oeuvre de codes parallèles sur cette architecture. Le développement se fait alors à l'aide d'une API bas-niveau qui permet de garder un contrôle précis sur le déroulement de son application tout en ayant une grande flexibilité dans le choix de déploiement d'un algorithme donné. Grâce à des dispositifs architecturaux de signalisation et de synchronisation, on peut concevoir des programmes parallèles très efficaces en termes de performance sur le Cell. Toutefois, du point de vue du programmeur, la mise en oeuvre du code est certainement plus laborieuse que lorsque d'autres modèles de programmation sont utilisés, mais celle-ci peut être justifiée dans le cadre de fortes contraintes sur les temps d'exécution ou dans le cas où des schémas de parallélisation classiques ne sont pas adaptés à l'application déployée.\\
Au vu des difficultés de programmation énoncées ci-dessus, d'autres outils de programmation pour le Cell ont été développés. Ils se basent sur l'API de base et fournissent des outils de plus haut-niveau plus faciles à prendre en main par le développeur. La question qui se pose alors est celle de la garantie de la performance et de la flexibilité d'utilisation de ces outils par rapport à l'API fournie par IBM.