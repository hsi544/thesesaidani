Le chapitre précédent a permis de présenter l'architecture particulière du Cell ainsi que l'environnement de développement de base pour cette architecture. Nous avons mis en avant la complexité de l'architecture accentuée notamment par son hétérogénéité et les espaces d'adresses distincts entre PPE et SPEs. De plus, nous avons présenté l'outil de programmation de base proposé qui est assez difficile à prendre en main par les développeurs et nécessite des connaissances poussées de l'architecture. Au vu des difficultés citées auparavant, l'implémentation d'une application de traitement d'images sur le processeur Cell devient une tâche complexe. L'obtention de performances proches des chiffres théoriques donnés par le constructeur est encore moins évidente.\\
Ce chapitre aborde dans le détail, la démarche de parallélisation manuelle d'un exemple de code de traitement d'images sur le processeur Cell. Tout au long de cette étude, plusieurs aspects de l'optimisation sont traités. D'une part les optimisations spécifiques au traitement d'images et d'autre part ceux spécifiques au processeur Cell et ses différents niveaux de parallélisme.\\
L'algorithme considéré est celui de la détection de points d'intérêts selon l'algorithme de Harris. Le choix de cet algorithme s'est fait selon plusieurs critères qui sont les suivants:  
\begin{itemize}
\item c'est un algorithme de traitement d'images bas niveau qu'on retrouve dans plusieurs applications plus complexes, comme la reconstruction 3D et le suivi d'objets;
\item il est composé de blocs de traitement de base qui sont représentatifs des algorithmes bas niveau comme les opérateurs de convolution et les opérateurs point à point;
%\item c'est un algorithme qui ne peux pas s'exécuter en temps réel sans optimisations spécifiques.
\item c'est un algorithme dont la complexité arithmétique et le rapport calcul/transfert, justifient le portage sur des architectures massivement parallèles
\item le respect d'un débit de traitement de 25  fps (\emph{frame per second}) ne peut pas être obtenu par une implémentation triviale sur une architecture mono-coeur.
.%qui ne peux pas s'exécuter en temps réel sans optimisations spécifiques.

\end{itemize}
\'Etant donné les caractéristiques de l'architecture du Cell ainsi que celles de l'algorithme, le but est de trouver la meilleure implémentation permettant d'exploiter au mieux les dispositifs haute performance de l'architecture. Le processeur Cell est un vrai concentré de dispositifs accélérateurs parmi lesquels les unités SPE purement SIMD, les contrôleurs DMA permettant un parallélisme entre transferts mémoire et tâches de calcul ainsi que la présence de plusieurs coeurs physiques qui permettent de répartir la charge de calcul de plusieurs manières possibles, soit sous forme de parallélisme de données uniquement, ou alors de parallélisme de tâches ou encore un mélange des deux.
\section{Algorithme de Harris}
La détection de points d'intérêt de \emph{Harris} et \emph{Stephen} \cite{harris_corner} est utilisée dans les systèmes de vision par ordinateur pour l'extraction de connaissances comme la détection de mouvement, la mise en correspondance d'images, le suivi d'objets, la reconstruction 3D et la stabilisation d'images. Cet algorithme fut proposé pour palier les manques de l'algorithme de \emph{Moravec} \cite{moravec} qui était sensible au bruit et pas sensible à la rotation. Un coin peut être défini comme étant l'intersection de deux contours alors qu'un point d'intérêt peut être défini comme un point ayant une position bien déterminée et qui peut être détectée de manière robuste. Ainsi, le point d'intérêt peut être un coin mais aussi un point isolé d'intensité maximale ou minimale localement, une terminaison de ligne ou encore un point de courbe où la courbure est localement maximale.\\
Les résultats qui suivent sont présentés dans \cite{medea07}, \cite{ispa07} et \cite{hipeac_2008}.
%\begin{itemize}
%\item \bibentry{medea07}
%\item \bibentry{ispa07} 
%\item \bibentry{hipeac_2008}
%\end{itemize}
\subsection{Description de l'algorithme}
Si l'on considère des zones de l'image de dimensions $u \times v$ (dans notre cas $3 \times 3$) dans une images 2-dimensions en niveaux de gris $I$, l'opérateur de Harris est basé sur l'estimation de l'auto-corrélation locale $S$ dont l'équation est la suivante:
\begin{equation}
\label{eq_00}
S(x,y) =\sum\limits_{u}\sum\limits_{v} w(u,v)\left( I(u+x,v+y) - I(u,v) \right)^{2}
\end{equation}

Où $w(u,v)$ est un noyau de lissage Gaussien\\
$I(u+x, v+y)$ peu être approché par un développement de Taylor. Supposons $I_{x}$ et $I_{y}$, les dérivées partielles de $I$, telles que

\begin{equation}
\label{eq_010}
I(u+x, v+y)=I(u, v)+I_{x}(u, v)x+I_{y}(u, v)y \nonumber
\end{equation}
Ce qui donne l'approximation :
\begin{equation}
\label{eq_011}
S(x,y) =\sum\limits_{u}\sum\limits_{v} w(u,v)\left( I_{x}(u, v)x+I_{y}(u, v)y \right)^{2} \nonumber
\end{equation}
Cette expression correspond à l'expression matricielles suivante : 
\begin{equation}
\label{eq_012}
S(x,y) = \begin{pmatrix}x & y\end{pmatrix}M\begin{pmatrix}x\\y\end{pmatrix}\nonumber
\end{equation}
%Par l'approximation de $S$ avec une série de Taylor du second ordre
La matrice de Harris $M$ est donnée par :
\begin{equation}
\label{eq_02}
M=\sum\limits_{u}\sum\limits_{v}w(u,v)\begin{bmatrix}I_{x}^{2}& I_{x}I_{y}\\I_{x}I_{y}&I_{y}^{2}\end{bmatrix}
\end{equation}
Un point d'intérêt est caractérisé par une large variation de $S$ dans toutes les directions du vecteur $(x,y)$. En analysant les valeurs propres de $M$, cette caractérisation peut être exprimée de la manière suivante. Soit $\lambda_{1}$, $\lambda_{2}$ les valeurs propres de $M$:
\begin{enumerate}
	\item Si $\lambda_{1}$ $\approx 0$ et $\lambda_{2}$ $\approx 0$ alors il n'y a pas de point d'intérêt au pixel $(x,y)$.
	\item Si $\lambda_{1}$ $\approx 0$ et $\lambda_{2} >> \lambda_{1}$, ou $\lambda_{2}$ $\approx 0$ et $\lambda_{1} >> \lambda_{2}$, alors alors un contour est retrouvé.
	\item Si $\lambda_{1}$ and $\lambda_{2}$ sont deux positives distinctes très grandes par rapport à 0 alors un coin est détecté.
\end{enumerate}
\emph{Harris} et \emph{Stephen} ont constaté que le calcul des valeurs propres était couteux car il requiert le calcul d'une racine carrée, et ont proposé à la place l'algorithme suivant : 

\begin{enumerate}
\item Pour chaque pixel $(x, y)$ de l'image, calculer la matrice de corrélation $M$:\\
\begin{equation}
\label{eq_03}
M=\begin{bmatrix} S_{xx} & S_{xy} \\ S_{xy} & S_{yy} \end{bmatrix}; \mbox{où:} S_{xx}=\left(\frac{\partial I}{\partial x}\right)^{2}\otimes w, S_{yy}=\left(\frac{\partial I}{\partial y}\right)^{2}\otimes w, S_{xy} = \left(\frac{\partial I}{\partial x}\frac{\partial I}{\partial y}\right)\otimes w
\end{equation}

Où	$\otimes$ est l'opérateur de convolution\\ %$w$ un noyau Gaussien.\\
\item Construire la carte des coins en calculant la mesure de la fonction de réponse des coins $C(x, y)$ pour chaque pixel $(x, y)$:\\
\begin{equation}
\label{eq_04}
C(x,y)=det(M)-k(trace(M))^{2}
\end{equation}
\begin{eqnarray*}
det(M)=S_{xx}.S_{yy}-S_{xy}^{2}\\
trace(M)=S_{xx}+S_{yy}
\end{eqnarray*}
et $k$ une constante empirique.\\
\end{enumerate}
Une illustration d'une détection de points d'intérêt sur une image 512$\times$512 en niveaux de gris est donnée en figure \ref{fig_house}. Afin d'obtenir ce résultat, deux étapes supplémentaires sont nécessaires qui permettent d'extraire une information visuelle à partir de la matrice $C(x,y)$\footnote{Ces étapes ont pour but de visualiser le résultat et ne sont donc pas incluses dans le diagramme en blocs de l'algorithme}. Ces étapes sont les suivantes :
\begin{enumerate}
\item Seuillage de la carte d'intérêt en mettant toutes les valeurs de $C(x,y)$ inférieures à un seuil donné à zéro.
\item Extraction des maxima locaux en gardant les points qui sont plus grands que tous leurs voisins dans un voisinage 3$\times$3.
\item Seuillage des maximas locaux à 10 \% de la valeur du plus grand maximum
\end{enumerate}
\begin{figure}[!htb]
	\centering
\begin{tabular}{cc}
	\includegraphics[width= 0.5\columnwidth]{Chapter5/figures/house1} & \includegraphics[width= 0.5\columnwidth]{Chapter5/figures/house_harris_map}
	\end{tabular}
	\caption{Illustration de la détection de points d'intérêts sur une image niveaux de gris 512$\times$512 avec $k= 0$}
	\label{fig_house}
\end{figure}

\subsection{Détails de l'implémentation}
\begin{figure}[!htb]
	\centering
	\includegraphics[width= \columnwidth]{Chapter5/figures/harris_NB}
	\caption{Implémentation de l'algorithme de Harris sous forme de graphe flot de données}
	\label{fig_HarrisAlgorithm}
\end{figure}
Les images en niveaux de gris sont typiquement des données stockées dans des entiers 8-bit non signés et la sortie de l'algorithme de Harris est dans ce cas là un entier 32 bit afin de satisfaire la dynamique des valeurs calculées qui font l'objet d'opérations de multiplication et d'addition tout au long de l'algorithme. Toutefois, pour des raisons de limitation du jeu d'instructions du SPU, et afin de garantir une comparaison directe avec les extension Altivec et SSE nous avons choisi le format flottant simple précision pour les données d'entrée et de sortie de l'algorithme. Dans notre implémentation nous avons divisé l'algorithme en 4 noyaux de traitement : un opérateur de \emph{Sobel} qui calcule la dérivée dans les directions horizontale et verticale, un opérateur de multiplication, un noyau de lissage de \emph{Gauss} ($w$ dans l'équation \ref{eq_03}),  suivi d'un opérateur de calcul de réponse des coins. Nous avons fixé la constante $k$ à zéro ($k=0$) (typiquement elle est fixée à 0.04) car ceci n'avait pas d'influence sur le résultat qualitatif et simplifie grandement l'équation \ref{eq_04} qui devient :
\begin{equation}
C(x,y) = S_{xx}.S_{yy}-S_{xy}^{2} \nonumber
\end{equation}

On obtient ainsi le graphe flot de données illustré par la figure \ref{fig_HarrisAlgorithm} qui est représentatif d'un algorithme de traitement d'images bas-niveau car il englobe des noyaux de convolution et des opérateurs point à point. Les noyaux de convolution de Sobel ($GradX$ et $GradY$)et le noyau de $Gauss$ sont définis comme suit: 

\begin{equation}
Grad_{X} = \frac{1}{16}\begin{bmatrix}-1 & \;\;\;0 & \;\;\;1 \\ -2 & \;\;\;0 & 2 \\ -1 & \;\;\;0 & \;\;\;1 \end{bmatrix} \nonumber
\end{equation}

\begin{equation}
Grad_{Y} = \frac{1}{16}\begin{bmatrix}-1 & -2 & -1 \\ \;\;\;0 & \;\;\;0 & \;\;\;0 \\ \;\;\;1 & \;\;\;2 & \;\;\;1 \end{bmatrix} \nonumber
\end{equation}

\begin{equation}
 Gauss = \frac{1}{16}\begin{bmatrix}\;\;\;1 & \;\;\;2 & \;\;\;1 \\ \;\;\;2 & \;\;\;4 & \;\;\;2 \\ \;\;\;1 & \;\;\;2 & \;\;\;1 \end{bmatrix} \nonumber
 \end{equation}

\'Etant donné qu'ils consomment plus d'entrées qu'ils ne produisent de sorties, les noyaux de convolution sont le goulot d'étranglement de l'algorithme car ils augmentent considérablement le trafic mémoire. Au vu de la nature des calculs effectués dans les différents noyaux, et qui sont très simples généralement (une suite de multiplications/accumulation) on peut considérer que les instructions mémoire sont prépondérantes dans l'application et que de ce fait, on peut qualifier l'algorithme de problème limité par la mémoire (\emph{memory-bounded problem}). C'est pour cette raison que les efforts d'optimisation sur l'algorithme de Harris sont faites par l'optimisation des accès à différents niveaux de la hiérarchie mémoire du processeur Cell. 
\section{Exploitation du parallélisme et optimisations multi-niveau}
Les techniques d'optimisation démontrées ici sont multiples et variées. Certaines sont de nature algorithmique et relèvent plutôt du domaine du traitement du signal et des images. D'autres techniques génériques relèvent plutôt du domaine de l'optimisation logicielle, on les retrouve parfois dans certains compilateurs optimisants. Les techniques précédentes sont générales et peuvent être appliquées à la majorité des processeurs généralistes car elle ne tiennent pas compte des aspects spécifiques d'une architecture donnée. Par contre, des optimisations spécifiques à l'architecture particulière du Cell ont été employées également. Celles-ci ne sont généralement pas reproductibles sur d'autres architectures parallèles car elles relèvent plus d'une adéquation entre l'algorithme et l'architecture qui contient certains dispositifs qui n'existent que sur le Cell et qui parfois résultent de contraintes de programmation spécifiques au Cell comme la taille limitée des mémoires locales des SPEs qui nécessitent une gestion logicielle explicite.

\subsection{Techniques spécifiques au traitement d'images}
Ces optimisations sont spécifiques au domaine du traitement du signal et des images. Elles peuvent donc être appliquées à plusieurs algorithmes et sur n'importe quelle architecture. Celles que nous avons utilisé concernent les noyaux de convolution et sont : la séparabilité, le chevauchement (\emph{overlapping}) et la factorisation des calculs.

\subsubsection{Séparabilité des noyaux}
\begin{figure}[!htb]
	\centering
	\includegraphics[width= \columnwidth]{Chapter5/figures/convolution_sep}
	\caption{Exemples de convolution par un filtre Gaussien $3\times3$ : (a) version avec noyaux 2D et (b) version avec deux noyaux 1D, résultant de la séparation du noyau 2D.}
	\label{fig_convosep}
\end{figure}
Cette optimisation consiste à exploiter le fait que les noyaux de convolution 2D de \emph{Sobel} et de \emph{Gauss} sont séparables en deux filtres de convolution 1D (Fig. \ref{fig_convosep}). Ainsi, la matrice des coefficients peut être exprimée comme un produit de deux vecteurs comme l'illustre les équations ci-dessous :
\begin{equation}
Grad_{X} =  \frac{1}{16} \begin{bmatrix}    -1 &  \;\;\;0 & \;\;\;1  \\ -2 & \;\;\;0 & \;\;\;2 \\ -1 & \;\;\;0 & \;\;\;1 \end{bmatrix} =    \frac{1}{16} \begin{bmatrix} -1 \\ \;\;\;0 \\ \;\;\;1 \end{bmatrix} \times  \begin{bmatrix} \;\;\;1 & \;\;\;2 & \;\;\;1 \end{bmatrix}\nonumber
\end{equation}
\begin{equation}
Grad_{Y} =  \frac{1}{16} \begin{bmatrix} -1 & -2 & -1   \\  \;\;\;0 & \;\;\;0 & \;\;\;0 \\  \;\;\;1 & \;\;\;2 & \;\;\;1 \end{bmatrix}  =   \frac{1}{16} \begin{bmatrix} \;\;\;1 \\ \;\;\;2 \\ \;\;\;1 \end{bmatrix} \times  \begin{bmatrix} -1 & \;\;\;0 & \;\;\;1 \end{bmatrix}\nonumber
\end{equation}
\begin{equation}
Gauss      =  \frac{1}{16} \begin{bmatrix}  \;\;\;1 &  \;\;\;2 &   \;\;\;1  \\  \;\;\;2 & \;\;\;4 & \;\;\;2 \\  \;\;\;1 & \;\;\;2 & \;\;\;1 \end{bmatrix} = \frac{1}{16}\begin{bmatrix}\;\;\;1 \\ \;\;\;2 \\ \;\;\;1 \end{bmatrix} \times \begin{bmatrix}1  & 2 & 1 \end{bmatrix}\nonumber
\end{equation}
Lorsque l'on sépare les noyaux de convolution, le calcul se fait en deux passes : une pour chaque vecteur. Grâce à la séparabilité des noyaux on arrive à réduire le nombre d'accès mémoire ainsi que la complexité arithmétique. La comparaison est illustrée dans les tableaux \ref{tab_sepa_arith} et \ref{tab_sepa_mem}.
\begin{table}
\centering
\begin{tabular}{|c|c|c||c|c|c|c|}
\multicolumn{7}{c}{\textbf{Complexité arithmétique filtre de \emph{Sobel}}}\\
\hline
\multicolumn{3}{|c||}{\cellcolor{medium-gray} \emph{Sans séparation du noyau}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec séparation du noyau}} & \cellcolor{medium-gray}\emph{Gain}\\
\hline
\textbf{\texttt{MUL}} & \textbf{\texttt{ADD}} & \textbf{Total}& \textbf{\texttt{MUL}} & \textbf{\texttt{ADD}} & \textbf{Total}  & \multirow{2}{*}{\textbf{x1,75}}\\
\cline{1-6}
 2  & 5 & 7  & 1  & 3 & 4  & \\
\hline
\multicolumn{7}{c}{\textbf{Complexité arithmétique filtre de \emph{Gauss}}}\\
\hline
\multicolumn{3}{|c||}{\cellcolor{medium-gray}\emph{Sans séparation du noyau}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec séparation du noyau}}  & \cellcolor{medium-gray}\emph{Gain } \\
\hline
\textbf{\texttt{MUL}} & \textbf{\texttt{ADD}} & \textbf{Total}& \textbf{\texttt{MUL}} & \textbf{\texttt{ADD}} & \textbf{Total} &   \multirow{2}{*}{\textbf{x2,16}} \\
\cline{1-6}
 8  & 5 & 13  & 2  & 4 & 6  &  \\
\hline
\end{tabular}
\caption{Réduction de la complexité arithmétique par séparabilité des noyaux}
\label{tab_sepa_arith}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c|c|c||c|c|c|c|}
\multicolumn{7}{c}{\textbf{Complexité mémoire filtre de \emph{Sobel}}}\\
\hline
\multicolumn{3}{|c||}{\cellcolor{medium-gray}\emph{Sans séparation du noyau}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec séparation du noyau}} & \cellcolor{medium-gray}\emph{Gain}\\
\hline
\textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}& \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}  & \multirow{2}{*}{\textbf{x1,16}}\\
\cline{1-6}
 6  & 1 & 7  & 1  & 5 & 6  & \\
\hline
\multicolumn{7}{c}{\textbf{Complexité mémoire filtre de \emph{Gauss}}}\\
\hline
\multicolumn{3}{|c||}{\emph{\cellcolor{medium-gray}Sans séparation du noyau}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec séparation du noyau}}  & \cellcolor{medium-gray}\emph{Gain} \\
\hline
\textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}& \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total} &   \multirow{2}{*}{\textbf{x1,42}} \\
\cline{1-6}
 9  & 1 & 10  & 6  & 1 & 7  &  \\
\hline
\end{tabular}
\caption{Réduction du nombre d'accès mémoire par décomposition des noyaux}
\label{tab_sepa_mem}
\end{table}

\subsubsection{Chevauchement des noyaux}
\begin{figure}[!htb]
	\centering
	\includegraphics[width= \columnwidth]{Chapter5/figures/convolution_overlap}
	\caption{Recouvrement de la fenêtre du filtre gaussien $3\times3$, certaines données sont conservées en décalant le masque de convolution.}
	\label{fig_convoover}
\end{figure}
La deuxième particularité des noyaux de l'opérateur de convolution est une notion de chevauchement qui permet d'avoir une redondance d'une partie des données (Fig. \ref{fig_convoover}). En effet, à chaque itération du calcul de la convolution il n'y a qu'une seule nouvelle colonne chargée. Les colonnes redondantes sont copiées dans les registres en les décalant d'un pas à droite par rapport à leur position précédente (\emph{rotation de registres}). On notera que le même type d'optimisation peut se faire grâce à un \emph{déroulage de boucle}.
\begin{table}
\centering
\begin{tabular}{|c|c|c||c|c|c|c|}
\multicolumn{7}{c}{\textbf{Complexité mémoire filtre de \emph{Sobel}}}\\
\hline
\multicolumn{3}{|c||}{\cellcolor{medium-gray}\emph{Sans chevauchement du noyau}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec chevauchement du noyau}} & \cellcolor{medium-gray}\emph{Gain}\\
\hline
\textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}& \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}  & \multirow{2}{*}{\textbf{x2,33}}\\
\cline{1-6}
 6  & 1 & 7  & 2  & 1 & 3  & \\
\hline
\multicolumn{7}{c}{\textbf{Complexité mémoire filtre de \emph{Gauss}}}\\
\hline
\multicolumn{3}{|c||}{\cellcolor{medium-gray}\emph{Sans chevauchement du noyau}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec chevauchement du noyau}}  & \cellcolor{medium-gray}\emph{Gain} \\
\hline
\textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}& \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total} &   \multirow{2}{*}{\textbf{x2,5}} \\
\cline{1-6}
 9  & 1 & 10  & 3  & 1 & 4  &  \\
\hline
\end{tabular}
\label{tab_over_mem}
\caption{Réduction de la complexité mémoire par chevauchement des noyaux}
\end{table}
\subsubsection{Séparation des noyaux et chevauchement}
Cette optimisation n'est en fait qu'une combinaison des deux précédentes. En effet on profite d'une part du fait que les noyaux soient séparables pour réduire la complexité arithmétique, et d'autre part du chevauchement des noyaux pour mémoriser le résultat précédent. Ainsi, au lieu de mémoriser les deux dernières colonnes, on mémorise le résultat du filtrage par le premier filtre 1D pour le réutiliser à l'itération suivante de la boucle. Dans ce cas là; la compléxité arithmétique est la même que celle de la version avec séparation des noyaux, alors que la complexité mémoire est réduite d'avantage.
Les tableau \ref{tab_sepaover_arith} et \ref{tab_sepaover_mem} donnent la différence en terme de complexité arithmétique et mémoire entre la version de base de l'algorithme et la version tenant compte des deux optimisations combinées.
\begin{table}
\centering
\begin{tabular}{|c|c|c||c|c|c|c|}
\multicolumn{7}{c}{\textbf{Complexité arithmétique filtre de \emph{Sobel}}}\\
\hline
\multicolumn{3}{|c||}{\cellcolor{medium-gray}\emph{Sans séparation du noyau + chevauchement}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec séparation du noyau + chevauchement}} & \cellcolor{medium-gray}\emph{Gain}\\
\hline
\textbf{\texttt{MUL}} & \textbf{\texttt{ADD}} & \textbf{Total}& \textbf{\texttt{MUL}} & \textbf{\texttt{ADD}} & \textbf{Total}  & \multirow{2}{*}{\textbf{x1,75}}\\
\cline{1-6}
 2  & 5 & 7  & 1  & 3 & 4  & \\
\hline
\multicolumn{7}{c}{\textbf{Complexité arithmétique filtre de \emph{Gauss}}}\\
\hline
\multicolumn{3}{|c||}{\cellcolor{medium-gray}\emph{Sans séparation du noyau + chevauchement}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec séparation du noyau + chevauchement}}  & \cellcolor{medium-gray}\emph{Gain} \\
\hline
\textbf{\texttt{MUL}} & \textbf{\texttt{ADD}} & \textbf{Total}& \textbf{\texttt{MUL}} & \textbf{\texttt{ADD}} & \textbf{Total} &   \multirow{2}{*}{\textbf{x2,16}} \\
\cline{1-6}
 8  & 5 & 13  & 2  & 4 & 6  &  \\
\hline
\end{tabular}
\caption{Réduction de la complexité arithmétique par séparabilité et chevauchement des noyaux}
\label{tab_sepaover_arith}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c|c|c||c|c|c|c|}
\multicolumn{7}{c}{\textbf{Complexité mémoire filtre de \emph{Sobel}}}\\
\hline
\multicolumn{3}{|c||}{\cellcolor{medium-gray}\emph{Sans séparation du noyau + chevauchement}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec séparation du noyau + chevauchement }} & \cellcolor{medium-gray}\emph{Gain}\\
\hline
\textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}& \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}  & \multirow{2}{*}{\textbf{x3,5}}\\
\cline{1-6}
 6  & 1 & 7  & 1  & 1 & 2  & \\
\hline
\multicolumn{7}{c}{\textbf{Complexité mémoire filtre de \emph{Gauss}}}\\
\hline
\multicolumn{3}{|c||}{\cellcolor{medium-gray}\emph{Sans séparation du noyau + chevauchement}} & \multicolumn{3}{|c|}{\cellcolor{medium-gray}\emph{Avec séparation du noyau + chevauchement }}  & \cellcolor{medium-gray}\emph{Gain} \\
\hline
\textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}& \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total} &   \multirow{2}{*}{\textbf{x5}} \\
\cline{1-6}
 9  & 1 & 10  & 1  & 1 & 2  &  \\
\hline
\end{tabular}
\caption{Réduction de la complexité mémoire par séparabilité et chevauchement des noyaux}
\label{tab_sepaover_mem}
\end{table}
\subsubsection{Composition de fonctions}
\begin{figure}[!htb]
	\centering
  \includegraphics[width= \columnwidth]{Chapter5/figures/funccompo}
	\caption{Règle de composition de fonctions.\textbf{(a)} composition de deux opérateurs point à point \textbf{(b)} composition d'un noyaux de convolution suivi d'un opérateur point à point \textbf{(c)} composition d'un opérateur point à point  suivi d'un noyaux de convolution \textbf{(d)} composition de deux noyaux de convolution successifs}
\label{fig_compo}
\end{figure}

Cette technique d'optimisation s'avère très efficace surtout dans les codes où les instructions mémoire sont prépondérantes. En effet, le fait de composer deux fonctions de calcul pour en faire une seule qui effectue les deux calculs, réduit considérablement le nombre d'accès mémoire puisque les opérations de sauvegarde et de chargement intermédiaires entre les deux fonctions initiales sont supprimées, les accès se font alors au niveau des registres. De plus, le coût de l'appel de fonction (sauvegarde dans la pile et branchement) est également réduit, car le résultat de la composition de fonctions est une seule fonction. Dans les cas les plus simples, la mise en oeuvre de cette technique n'est pas difficile. Toutefois, lorsqu'il s'agit d'opérateurs de convolution comme dans notre cas, des nouvelles contraintes apparaissent afin de garantir la validité du résultat du calcul. Ainsi, des règles de composition s'imposent en fonction de l'ordre dans lequel s'enchainent les fonctions. Ces règles sont illustrées sur la figure \ref{fig_compo}. On peut alors constater qu'il existe plusieurs règles de composition des fonctions selon d'une part, la nature des opérateurs mis en jeu et, d'autre part, l'ordre dans lequel ils s'enchainent. En terme d'apport de performances de cette technique, nous observons deux aspects distincts:
\begin{itemize}
\item \textbf{Nombre d'accès mémoire} : on observe en effet que ceux-ci sont systématiquement réduits, car les opérations de \texttt{\textbf{load/store}} intermédiaires sont supprimées, sauf dans le cas de la composition de deux noyaux de convolution (cas \textbf{(d)} sur la figure \ref{fig_compo})où la présence de bords supplémentaires augmente considérablement le nombre des accès mémoire.
\item \textbf{Nombre d'opérations arithmétiques} : Celui-ci ne change pas dans le meilleur des cas notamment lorsque les opérateurs composés sont point à point ou alors lorsque l'opérateur de convolution est placé devant un opérateur point à point (cas \textbf{(b)} sur la figure \ref{fig_compo}). Toutefois, si la convolution est placée à la suite d'un quelconque traitement, elle impose que l'opérateur qui la précède soit effectué sur tous les pixels de voisinage, ce qui augmente considérablement la complexité arithmétique, en particulier lorsque l'opérateur qui précède est une convolution (cas \textbf{(d)} sur la figure \ref{fig_compo}).
\end{itemize}
D'après les observations ci-dessus, la composition de fonction peut être un bon choix pour l'optimisation d'une chaîne de traitement telle que l'algorithme de détection de point d'intérêts de Harris. Toutefois, toutes les combinaisons n'apportent pas forcément une amélioration des performances. Elles peuvent même dans certains cas les dégrader. Le tableau \ref{tab_memall} résume les complexité mémoire et arithmétique des différents cas de composition sur la figure \ref{fig_compo} en incluant également les optimisations décrites auparavant.
\begin{table}
\centering
\begin{tabular}{|c||c|c|c|c|}
\hline
\multicolumn{5}{|c|}{\cellcolor{medium-gray}\textbf{Version de base sans composition}}\\
\hline
\textbf{Opérateur} & \textbf{Occurrences} & \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}\\
\hline
\hline
\rowcolor{light-gray} \emph{Sobel} & 2 & 6 & 1 & 14\\
\hline
\emph{Mul} & 3 & 2 & 1 & 9\\
\hline
\rowcolor{light-gray} \emph{Gauss} & 3 & 9 & 1 & 30\\
\hline
\emph{Coarsity} & 1 & 3 & 1 & 4\\
\hline
\rowcolor{light-gray} \emph{Harris} & 1 & 48 & 9 & 57\\
\hline
\multicolumn{5}{|c|}{\cellcolor{medium-gray}\textbf{Version avec chevauchement et séparation des noyaux sans composition}}\\
\hline
\textbf{Opérateur} & \textbf{Occurrences} & \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}\\
\hline
\hline
\rowcolor{light-gray}\emph{Sobel} & 1 & 3 & 2 & 5\\
\hline
\emph{Mul} & 3 & 2 & 1 & 6\\
\hline
\rowcolor{light-gray} \emph{Gauss} & 3 & 3 & 1 & 12\\
\hline
\emph{Coarsity} & 1 & 3 & 1 & 4\\
\hline
\rowcolor{light-gray} \emph{Harris} & 1 & 21 & 9 & 30\\
\hline
\multicolumn{5}{|c|}{\cellcolor{medium-gray}\textbf{Version avec composition de Mul$\circ$Sobel et Coarsity$\circ$Gauss}}\\
\hline
\textbf{Opérateur} & \textbf{Occurrences} & \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}\\
\hline
\hline
\rowcolor{light-gray} \emph{Sobel$\circ$Mul} & 1 & 9 & 3 & 12\\
\hline
\emph{Gauss$\circ$Coarsity} & 3 & 9 & 1 & 28\\
\hline
\rowcolor{light-gray} \emph{Harris} & 1 & 36 & 4 & 40\\
\hline
\end{tabular}
\caption{Tableau récapitulatif de l'optimisation des accès mémoire}
\label{tab_memall}
\end{table}
\begin{table}
\begin{tabular}{|c||c|c|c|c|}
\hline
\multicolumn{5}{|c|}{\cellcolor{medium-gray}\textbf{Version avec chevauchement et séparation + composition de Mul$\circ$Sobel et Coarsity$\circ$Gauss}}\\
\hline
\textbf{Opérateur} & \textbf{Occurrences} & \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}\\
\hline
\hline
\rowcolor{light-gray}\emph{Sobel$\circ$Mul} & 1 & 3 & 3 & 6\\
\hline
\emph{Gauss$\circ$Coarsity} & 3 & 3 & 1 & 10\\
\hline
\rowcolor{light-gray}\emph{Harris} & 1 & 12 & 4 & 16\\
\hline
\multicolumn{5}{|c|}{\cellcolor{medium-gray}\textbf{Version avec composition de Coarsity$\circ$Gauss$\circ$Mul$\circ$Sobel}}\\
\hline
\textbf{Opérateur} & \textbf{Occurrences} & \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}\\
\hline
\hline
\rowcolor{light-gray}\emph{Sobel$\circ$Mul$\circ$Gauss$\circ$Coarsity} & 1 & 25 & 1 & 26\\
\hline
\emph{Harris} & - & - & - & 26\\
\hline
\multicolumn{5}{|c|}{\cellcolor{medium-gray}\textbf{Version avec chevauchement et séparation + composition de Coarsity$\circ$Gauss$\circ$Mul$\circ$Sobel}}\\
\hline
\textbf{Opérateur} & \textbf{Occurrences} & \textbf{\texttt{LOAD}} & \textbf{\texttt{STORE}} & \textbf{Total}\\
\hline
\hline
\rowcolor{light-gray}\emph{Sobel$\circ$Mul$\circ$Gauss$\circ$Coarsity} & 1 & 5 & 1 & 6\\
\hline
\emph{Harris} & - & - & - & 6\\
\hline
\end{tabular}
\caption{Tableau récapitulatif de l'optimisation des accès mémoire}
\label{tab_memall}
\end{table}

\label{sectioncompo}

\subsection{Techniques spécifiques à l'architecture du processeur Cell}
%\begin{figure}[!htb]
%	\centering
 % \includegraphics[width= \columnwidth]{Chapter5/figures/cellnoc}
  %\caption{Réseau d'interconnexion du Cell d'après \cite{cellnoc}}
  %\label{fignoc}
%\end{figure}
Dans cette partie, nous abordons des techniques d'optimisation qui sont spécifiques à notre architecture cible. On se limite dans l'étude aux seuls transferts de données présents dans l'application. Cette limitation s'explique par le fait que les applications que nous étudions sont souvent dominées par les transferts de données et où le ratio transfert/calcul est grand. Les transferts constituent donc le principal goulot d'étranglement dans nos algorithmes. On entend par transfert de données, toute communication qui met en jeu deux mémoires physiques sur le Cell. Cela comporte les transferts DMA entre la mémoire principale et les mémoires locales des SPEs ainsi que les communications mettant en jeu deux mémoires privées de SPEs. Plusieurs aspects sont mis en avant. D'une part, les caractéristiques internes de l'architecture du réseau de communications \cite{cellnoc} (\emph{Network On Chip (NoC)}) \nomenclature{NoC}{Network on Chip}du Cell et d'autres part la nature des transferts de données imposées par l'algorithme et la partition des données à plusieurs niveaux de la hiérarchie mémoire.
\subsubsection{Optimisation de la bande-passante du \emph{NoC}}
Dans le modèle de programmation utilisé qui est basé sur le \emph{SDK} du Cell et les librairies \emph{libspe}, les données présentes en mémoire centrale sont transférées aux mémoires locales des SPEs avant d'être traitées. Ces transferts se font d'une manière explicite dans le logiciel par des appels à des fonctions de l'API de gestion du \emph{MFC} et ils sont gérés par le \emph{NoC} au travers de contrôleurs mémoire et de l'arbitre de bus. L'architecture du bus et la topologie du réseau imposent quelques contraintes qui jouent un rôle primordial dans la minimisation des latences des communications point à point.
\paragraph{Taille du transfert}
\begin{figure}[htb]
  \centering
  \includegraphics[width= 0.8\columnwidth]{Chapter5/figures/bwsize}
  \caption{Influence de la taille du transfert DMA sur la bande-passante}
    \label{fiGowsize}
\end{figure}
%\begin{figure}[!htb]
%	\centering
%  \includegraphics[width= 0.8\columnwidth]{Chapter3/figures/grapheBW}
%  \label{graphbw}
%	\caption{Bande-passante cumulée }
%\end{figure}

Le premier paramètre influant sur le débit de transfert est la taille du bloc de données transféré. L'API de transfert limite les tailles d'un bloc transféré par une commande DMA à des valeurs de 1, 2, 4, 8 octets ou tout multiple de 16 octets et la taille de ce même bloc de données ne peut excéder 16 Ko \nomenclature{Ko}{Kilos octets}si on veut le faire le transfert en une seule commande. De plus, les adresses doivent obligatoirement être alignées sur 16 octets et un alignement sur 128 octets est préférable à cause de la taille de la ligne de cache sur la mémoire locale du SPE qui est de 128 octets. D'autre part l'espace mémoire disponible sur les SPEs pour stocker données et instructions est limité à 256 Ko. Toutes ces contraintes, imposent d'accorder beaucoup d'attention à la taille de bloc transféré et à l'alignement des données qui ne sont pas du ressort du développeur dans les architectures à mémoire partagée. Plusieurs \emph{benchmarks} ont été effectués dans \cite{cellnoc} et \cite{cell_bw02} et qui démontrent la relation entre d'une part la taille et d'autre part le débit et la latence des transferts sur le Cell. Le graphe \ref{fiGowsize} donne les résultats sur un benchmark de bande-passante que nous avons effectué sur une BladeCenter QS20 et démontrent que celle-ci est d'autant plus grande que la taille des données transférées est importante. Ceci s'explique par la latence du transfert qui représente le temps d'initialisation d'un transfert, cette durée étant constante quelque soit la taille du paquet jusqu'à 16 Ko.
\paragraph{Nombre de transferts concurrents}
\begin{figure}[htb]
  \centering
  \includegraphics[width= 0.8\columnwidth]{Chapter5/figures/bwcontention}
  \caption{Influence du nombre de transferts dans le cas d'une communication entre deux mémoires locales de deux différents SPEs}
    \label{figowncontention}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width= 0.8\columnwidth]{Chapter5/figures/bwnspes}
  \caption{Influence du nombre de transferts dans le cas d'une communication entre la mémoire externe et une mémoire locale de SPE}
    \label{figownspes}
\end{figure}

Les second facteur qui influe sur l'efficacité du réseau lors des transferts, est le nombre de transferts s'exécutant en parallèle. En effet, l'architecture du réseau dont la topologie est du type \emph{token ring} contient quatre anneaux d'une largeur de 128 bits : deux dans le sens d'une aiguille d'une montre et les deux autres dans le sens opposé. En observant la topologie du réseau ainsi que le sens de circulation des données sur les anneaux du bus, on constate qu'il peut y avoir collision entre deux transferts s'exécutant de manière concurrente (Fig. \ref{fignoc}). Sachant qu'un anneau peut gérer 3 transferts concurrents tant que ceux-ci n'entrent pas en collision, ce risque est d'autant plus important lorsque le nombre de transferts concurrents augmente (au-delà de 12). Lorsqu'un tel conflit est détecté,  l'arbitre de bus le résout avec un surcoût qui divise globalement la bande passante par deux. La courbe sur le figure \ref{figowncontention} permet de constater l'influence du nombre de transferts concurrents sur le débit de transfert. On peut ainsi observer que la courbe perd sa linéarité au delà de 4 SPEs, ce qui correspond à un nombre de transferts trop important pour ne pas provoquer de collisions sur le bus.\\
\indent Dans le cas d'une communication du PPE vers le SPE, la bande passante maximale qui peut être atteinte est de 25.6 Go/s\nomenclature{Go/s}{Giga octets/seconde}. Dans le cas où plusieurs SPEs font une requête vers la mémoire principale, les transferts ne peuvent être que sérialisés car il n'existe qu'une seule liaison vers le MS (\emph{Main Storage}). On observe alors sur le graphique de la figure \ref{figownspes} que la bande passante maximale n'est atteinte que lorsqu'au moins 4 SPEs font un transfert de la mémoire centrale vers leurs mémoires privées. 

\subsubsection{Optimisation de la localité temporelle par chainage des opérateurs}
Le but visé par cette technique est de rapprocher le plus possible les données des unités de traitement. En effet, les coûts liés au transfert des données de la mémoire centrale vers les mémoires locales étant important, il est pertinent de garder les données en mémoire locale après chaque traitement au lieu de multiplier les lectures/écritures vers la mémoire centrale. Les règles de chainage des opérateurs au niveau des accès mémoires, sont les mêmes que pour la composition des opérateurs citée dans la section \ref{sectioncompo}. Cette optimisation apporte beaucoup à la performance globale car l'application est caractérisée par un ratio transfert/calcul important et la différence de débit entre l'accès en mémoire locale et l'accès à une mémoire distante par DMA est d'un facteur dix \cite{cell_bw02}.

\subsubsection{Optimisation du tuilage des données}
Le \emph{loop tiling} \cite{wolfetiling} ou \emph{loop blocking} est une technique d'optimisation très utilisée que cela soit par les programmeurs ou par les compilateurs optimisants\cite{Kennedy_2001}. Cette technique consiste en un découpage des données à différents niveaux de la hiérarchie mémoire, de telle sorte que la latence d'accès soit la plus petite possible. Dans une architecture à mémoire partagée contenant des caches, ceci revient à un découpage qui garantit que les données utilisées par un traitement donné tiennent toujours dans le cache. Ainsi, le temps d'accès aux données est de l'ordre du cycle et les défauts de cache (\emph{cache misses}) sont quasi inexistants quelque soit la taille des données.\\
\indent Au vu de la nature de la hiérarchie mémoire du processeur Cell, le \emph{tiling} est une obligation. D'une part, il n'existe pas de mémoire cache pour gérer les mémoires privées des SPEs. D'autre part l'espace de stockage est limité dans les mémoires locales des SPEs à 256 Ko. Ceci rend le découpage des données en tuiles pouvant tenir dans le cache obligatoire. L'unité de données atomique devient alors la tuile qui, représente le morceau de données le plus petit traité par le code du SPE.
\paragraph{Taille de la tuile}
La taille de la tuile est un paramètre primordial lorsqu'il s'agit de découper les données de manière optimale. Dans le cas du processeur Cell, les contraintes imposées par l'architecture font que le choix de la taille optimale est restreint. En effet, la taille limitée de la mémoire locale pour le code source et les données, impose que la taille de la tuile ne dépasse pas la capacité de stockage qui est de 256 Ko. L'autre paramètre dont on doit tenir compte, est le nombre d'entrées et de sorties des fonctions de traitement car celui-ci donne le nombre de tuiles. On peut en déduire globalement, que la taille de la tuile est égale à la capacité de stockage restante pour les données, divisée par le nombre de tuiles en entrée et en sortie de la chaine de traitement mise en jeu. D'autre part, selon ce qui a été vu précedemment, la taille de transfert qui garantit une bande-passante maximale sur le bus est de 16 Ko ou un multiple de cette taille.
\paragraph{Forme de la tuile}
\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{Chapter5/figures/qreloads}
  \caption{Redondances de données pour un opérateur de convolution $3 \times 3$}
    \label{qreloads}
\end{figure}
Les tuiles que l'on traite dans notre cas sont en général d'une forme rectangulaire. Toutefois, à cause de la présence d'opérateurs de convolution, les dimensions hauteur et largeur de la tuile ($h$ et $w$) doivent également être considérées. En effet, dans le cas d'opérateurs de convolution, les bords qui représentent les voisinages des pixels traités augmentent la quantité de données transférées de la mémoire. Cette quantité peut être réduite avec un choix judicieux des dimensions de la tuile. Comme le montre la figure \ref{qreloads} certains des pixels sont rechargés. Nous étudions dans la suite l'influence des dimensions de la tuile sur ce nombre de pixels redondants.\\
\indent On considère une image de hauteur $H$ et de largeur $W$, une tuile de dimensions $h$ et $w$ et un opérateur de convolution nécessitant un voisinage $3 \times 3$ pour chaque pixel. Afin de simplifier le calcul, on suppose que la matrice de convolution est carrée, ce qui fait que les bords des deux côtés sont égaux. La quantité totale de pixels transférés pour le traitement est alors :
\begin{equation}
Q = (h+2b)\times(w+2b)\times nb_{tiles}\nonumber
\end{equation}
où $nb_{tiles}$ est le nombres de tuiles dans l'image et b la taille du bord en pixel.
\begin{equation}
nb_{tiles} = \lfloor {\frac{H} {h}} \rfloor  \times \lfloor \frac{W} {w} \rfloor \nonumber
\end{equation}
Le but étant de trouver à taille de tuile constante $h\times w$ quels sont les dimensions $h$ et $w$ qui minimisent $Q$
\begin{equation}
Q = (h+2b)\times(w+2b)\times\frac{H \times W}{h \times w} \nonumber
\end{equation}

Posons alors $\lambda = h \times w = C^{te}$, la fonction à minimiser devient alors :
\begin{equation}
Q = (h+2b)\times(w+2b)\times\frac{H \times W}{\lambda} \nonumber
\end{equation}

\begin{equation} \mbox{Calculons alors les dérivées : } \frac{\partial Q}{\partial h} \mbox{ et } \frac{\partial Q}{\partial w} \nonumber\end{equation}


%\begin{equation} Q = \frac{H \times W}{\lambda} (h+2b) \times (\frac{\lambda}{h}+2b) \mbox{  et symétriquement  } Q = \frac{H \times W}{\lambda} (w+2b) \times (\frac{\lambda}{w}+2b) \nonumber\end{equation}
\begin{equation} Q = \frac{H \times W}{\lambda} (h+2b) \times (\frac{\lambda}{h}+2b) \quad \quad Q = \frac{H \times W}{\lambda} (w+2b) \times (\frac{\lambda}{w}+2b) \nonumber\end{equation}


\begin{equation}
%$\frac{\partial Q}{\partial h} = \frac{H \times W \times 2b}{\lambda \times h^{2}}(h^{2}-\lambda)$
\frac{\partial Q}{\partial h} = \frac{H \times W \times 2b}{\lambda \times h^{2}}(h^{2}-\lambda) \nonumber \\
\end{equation}
\begin{equation}
\frac{\partial Q}{\partial w} = \frac{H \times W \times 2b}{\lambda \times w^{2}}(w^{2}-\lambda) \nonumber
\end{equation}
\begin{equation}\mbox{Le minimum de la fonction $Q$ est atteint lorsque : }\frac{\partial Q}{\partial h} = 0 \mbox{ et } \frac{\partial Q}{\partial w}  = 0 \nonumber\end{equation}

\begin{equation} \mbox{ce qui donne : } h = w = \sqrt{\lambda} \nonumber \end{equation}
\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{Chapter5/figures/meshtile}
  \caption{Tracé de la fonction $Q(h,w)$ dans l'espace}
  \label{meshtile}\end{figure}
Ce qui permet de déduire que la forme de la tuile qui minimise la quantité de données transférée est une forme carrée. D'autre part, lorsque l'on observe le tracé de la fonction $Q$ sur la figure \ref{meshtile} on peut constater que cette fonction décroit quand $h$ et $w$ augmentent et que la valeur de $Q$ est minimale pour une surface de tuile donnée ($ h \times w = constante $) lorsque $h = w$ ce qui correspond à la première bissectrice du plan $(hw)$. Ceci permet d'affirmer que la tuile carrée est optimale pour une surface donnée et que la valeur de $Q$ diminue d'autant plus lorsque $h$ et $w$ augmentent. Les valeurs de $h$ et $w$ sont alors limitées par l'espace mémoire disponible pour une tuile dans la mémoire locale des SPEs.\\
\indent Ce résultat nous a permis de démontrer que la forme de la tuile avait une influence sur la quantité de données transférées et par conséquent sur la performance globale de l'application. Cependant, ce découpage n'est pas forcément optimal lorsqu'on passe à l'implémentation. En effet, des tuiles de forme carrée signifient des accès à des zones non-contigues de la mémoire. Ce type d'accès est en général coûteux car il provoque des sauts dans la mémoire. De plus, sur le processeur Cell, ceci se traduit en commandes DMA sur des zones non-contigues de la mémoire ce qui nécessite des commandes du type \emph{DMA list}. Ces dernières requièrent la création d'une liste qui contient chaque DMA élémentaire et qui est d'autant plus grande que le nombre de transferts est important. Cette liste doit également être mise à jour lors de chaque nouveau transfert. Toutes ces contraintes nous ont poussé dans un premier temps à adopter un découpage en bandes qui consiste en l'utilisation de tuiles ayant une largeur égale à celle de l'image. Ceci permet des accès sur des zones contigues de la mémoire et des transferts pouvant se faire en une seule commande DMA. De plus, lorsqu'une tuile ne contient pas de bords latéraux comme dans notre cas, le problème d'alignement des transferts est également contourné. Par contre, ce choix induit des limitations en terme de taille d'image pouvant être traitée. En effet, sachant que la taille de la tuile est limitée et que sa largeur est égale à celle de l'image, la hauteur de la tuile elle, diminue au fur et à mesure que la largeur de l'image augmente. De ce fait, les accès non-contigus sont nécessaires pour des taille d'images très grandes.
\paragraph{Résultats expérimentaux}
Nous avons procédé à quelques expérimentations sur l'influence de la forme de la tuile. Cette étude à fait l'objet d'une publication dans \cite{heart2010}. Le but étant de valider notre implémentation en considérant plusieurs formes de tuiles, et en vérifiant la conformité des résultats obtenus avec l'étude théorique. Le programme s'exécute à partir du PPE qui utilise un seul SPE pour le calcul. Pour chaque image, nous avons choisi une taille de tuile fixe i.e : un volume de données transférées constant et avons fait varier la forme de la tuile.
Nous observons d'après les tableaux \ref{tile_perf_01}, \ref{tile_perf_02}, \ref{tile_perf_03} et \ref{tile_perf_04}  que la tuile qui se rapproche le plus de la forme carrée possède les meilleures performances globales. La différence de performances entre deux tuiles de tailles proches est marginale mais dans un cas réaliste où le flux d'images est continu, cette différence est plus importante. Pour les tailles d'images très grandes (Tab. \ref{tile_perf_03} et \ref{tile_perf_04}) on constate que l'amélioration est de l'ordre de 50\% en comparaison avec des tuiles qui ont la largeur de l'image. Il faut noter qu'il existe un surcoût à l'utilisation de tuiles carrées. En effet, une telle forme nécessite l'utilisation des DMA listes. Le compromis se situe dans ce cas, entre la redondance de données et les accès mémoire irréguliers. Les résultats expérimentaux montrent que les tuiles carrées donnent de meilleurs performances malgré ce compromis. 

\begin{table}
\centering
\begin{tabular}{|c||c|c|}
\hline
\rowcolor{medium-gray} \textbf{$h_{tuile}$} & \textbf{$w_{tuile}$} & \textbf{temps total (sec)}  \\
\hline
\hline
8 & 512 & 0.0494\\
\hline
\rowcolor{light-gray} 16 & 256 & 0.0598\\
\hline
32 & 128 & 0.0485\\
\hline
\rowcolor{light-gray} 64 & 64 & 0.0345\\
\hline
128 & 32 & 0.0517\\
\hline
\rowcolor{light-gray} 256 & 16 & 0.0699\\
\hline
512 & 8 & 0.0734\\
\hline
\end{tabular}
\caption{Performances sur une image 512 $\times$ 512}
\label{tile_perf_01}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c||c|c|}
\hline
\rowcolor{medium-gray}\textbf{$h_{tuile}$} & \textbf{$w_{tuile}$} & \textbf{temps total (sec)}  \\
\hline
\hline
8 & 512 & 0.198\\
\hline
\rowcolor{light-gray}16 & 256 & 0.238\\
\hline
32 & 128 & 0.187\\
\hline
\rowcolor{light-gray}64 & 64 & 0.110\\
\hline
128 & 32 & 0.180\\
\hline
\rowcolor{light-gray}256 & 16 & 0.218\\
\hline
512 & 8 & 0.352\\
\hline
\end{tabular}
\caption{Performances sur une image 2048 $\times$ 512}
\label{tile_perf_02}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c||c|c|}
\hline
\rowcolor{medium-gray}\textbf{$h_{tuile}$} & \textbf{$w_{tuile}$} & \textbf{temps total (sec)}  \\
\hline
\hline
5 & 1200 & 0.494\\
\hline
\rowcolor{light-gray}10 & 600 & 0.360\\
\hline
20 & 300 & 0.264\\
\hline
\rowcolor{light-gray}40 & 150 & 0.235\\
\hline
80 & 75 & 0.183\\
\hline
\rowcolor{light-gray}160 & 37 & 0.247\\
\hline
320 & 18 & 0.275\\
\hline
\end{tabular}
\caption{Performances sur une image 1200 $\times$ 1200}
\label{tile_perf_03}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c||c|c|}
\hline
\rowcolor{medium-gray}\textbf{$h_{tuile}$} & \textbf{$w_{tuile}$} & \textbf{temps total (sec)}  \\
\hline
\hline
8 & 512 & 0.985\\
\hline
\rowcolor{light-gray}16 & 256 & 0.726\\
\hline
32 & 128 & 0.643\\
\hline
\rowcolor{light-gray}64 & 64 & 0.438\\
\hline
128 & 32 & 0.692\\
\hline
\rowcolor{light-gray}256 & 16 & 0.866\\
\hline
512 & 8 & 1.422\\
\hline
\end{tabular}
\caption{Performances sur une image 2048 $\times$ 2048}
\label{tile_perf_04}
\end{table}


\subsection{Schémas de parallélisation}\label{parallel_schemes}
Dans ce qui suit, nous abordons l'optimisation de notre algorithme à un niveau d'abstraction plus haut, celui des tâches. L'architecture du Cell permet plusieurs placements possibles du graphe d'opérateurs par la présence de 8 SPEs et la possibilité de mettre en place différents schémas de communication. Dans les figures qui suivent, les opérateurs sont représentés par des cercles, les processeurs par des rectangles à coins arrondis, les tuiles sont de forme rectangulaire et peuvent contenir des bords. Les flèches à trait fin représentent des instruction \texttt{\textbf{load/store}} dans la mémoire privée du SPE alors que les flèches plus épaisses représentent des commandes DMA inter-SPE ou alors entre la mémoire centrale et la mémoire locale d'un SPE. La lettre $S$ représente le gradient de \emph{Sobel} en $x$ et $y$ combinés. La lettre $G$ représente le noyau de lissage de \emph{Gauss}. La lettre $M$ représente une multiplication point à point et la lettre $H$ le calcul de la coarsité. Si des chiffres précèdent les lettres citées auparavant, ils correspondent au nombre d'occurrences de l'opérateur. Par exemple, $3M$ représente trois instances de l'opérateur de multiplication. Les différents placements se basent sur le graphe de référence de la figure \ref{fig_HarrisAlgorithm}.
\subsubsection{Data parallel :  Fig. \ref{harris-spmd}}
\begin{figure}[!htb]
	\centering
  \includegraphics[width= \columnwidth]{Chapter5/figures/harris-spmd}
	\caption{Schéma de parallélisation SPMD conventionnel}
	  \label{harris-spmd}
\end{figure}
Dans ce schéma de déploiement, l'image est divisée en 8 régions de même taille, afin que chacun des 8 SPEs ait une charge de calcul équivalente. Tous les SPEs exécutent le même code. Les opérateurs sont exécutés successivement sur l'image entière l'un après l'autre. A titre d'exemple l'opérateur de multiplication n'est exécuté que lorsque le calcul du filtre de \emph{Sobel} est achevé sur toute l'image. Ce modèle de calcul est dit \emph{data-parallel} car les données sont envoyées en parallèle sur les SPEs et traitées de manière complètement indépendante les unes des autres. Toutefois, la bande passante sur le bus mémoire centrale vers le \emph{local store} est sollicitée de manière importante, car les données sont systématiquement lues et écrites avant et après chaque opérateur.
\subsubsection{Pipeline : Fig. \ref{harris-pipeline}}
\begin{figure}[!htb]
	\centering
  \includegraphics[width= \columnwidth]{Chapter5/figures/harris-pipeline}
	\caption{Schéma de parallélisation pipeline}
	\label{harris-pipeline}
\end{figure}
Cette implémentation de l'algorithme consiste à déployer le graphe d'opérateurs sous forme de pipeline. L'image n'est pas subdivisée en régions de traitement mais chaque tuile traitée traverse le pipeline avant qu'une nouvelle tuile ne puisse le faire. L'algorithme est par conséquent fortement sérialisé car la possibilité d'exécuter des \emph{threads} concurrents y est réduite à cause des dépendances de données introduites dans le graphe. Par contre, la bande-passante inter-SPEs est bien exploitée car la majorité des transferts se font entre SPEs et par conséquent la pression exercée sur le bus précédemment est atténuée car elle est répartie sur l'ensemble de l'anneau.
\subsubsection{Chainage d'opérateurs par paires (\emph{Halfchain}) : Fig. \ref{harris-hchain}}
\begin{figure}[!htb]
	\centering
  \includegraphics[width= \columnwidth]{Chapter5/figures/harris-hchain}
	\caption{Schéma de Parallélisation Chainage d'opérateurs par paires (\emph{Halfchain})}
	\label{harris-hchain}
\end{figure}
Dans cette version deux opérateurs successifs sont placés sur le même SPE. Ainsi, les opérateurs \emph{Sobel} et \emph{Mul} sont placés sur un même SPE et \emph{Gauss} et \emph{Coarsity} sont placés sur un autre SPE. Le nombre de SPEs dans un processeur Cell étant de 8, ce schéma permet d'avoir 4 régions de l'image traitées en parallèle. Si l'on compare cette version à la précédente, on notera que la pression sur le bus d'interconnexion est plus importante car les transferts concurrents sont plus nombreux et par conséquent le risque de contention du bus est plus probable.
\subsubsection{Chainage et fusion d'opérateurs par paires (\emph{Halfchain+Halfpipe}): Fig. \ref{harris-hchainhpipe}}
\begin{figure}[!htb]
	\centering
  \includegraphics[width= 0.6\columnwidth]{Chapter5/figures/harris-hchainhpipe}
	\caption{Schéma de Parallélisation Chainage et fusion d'opérateurs par paires (\emph{Halfchain+Halfpipe})}
	\label{harris-hchainhpipe}
\end{figure}
Cette version est une variante de celle qui la précède. L'idée étant de fusionner les opérateurs présents sur un même SPE, et ce afin d'éliminer les instructions de \texttt{\textbf{load}} et \texttt{\textbf{store}} présents à la sortie du premier et à l'entrée du second. De ce fait, le nombre de cycles peut être considérablement réduit, surtout lorsque l'on sait que la latence des instructions mémoire est de 6 cycles sur les SPE \cite{cell_handbook}.
\subsubsection{Chaînage complet et fusion d'opérateur par paires (\emph{Halfchain + Fullpipe}): Fig. \ref{harris-hchainfpipe}}
\begin{figure}[htb]
	\centering
  \includegraphics[width= 0.6\columnwidth]{Chapter5/figures/harris-hchainfpipe}
	\caption{Schéma de Parallélisation Chaînage et fusion d'opérateurs par paires  (\emph{Halfchain + Fullpipe})}
	\label{harris-hchainfpipe}
\end{figure}
Dans cette implémentation tous les opérateurs sont exécutés sur le même SPE. D'une part ceci permet d'éviter les transferts DMA entre SPEs et minimise donc les transactions sur le bus d'interconnexion.
\section{\'Evaluation des performances}
Après avoir exposé les différentes techniques d'optimisation, nous procédons à la mesure des performances pour les différents modèles de déploiement qui ont été mis en oeuvre. Il s'agit ici de comparer les implémentations en terme de temps d'exécution sur le processeur Cell. Nous évaluons également l'influence des transferts sur la performance globale de chacun des modèles.
\subsection{Métriques de mesure}
La métrique que nous avons choisi d'utiliser pour la mesure de performance, est le nombre de cycles moyen par pixel ou $cpp$ :\\

\begin{equation}cpp = \frac{nombre_{cyles \mbox{ } cpu}}{nombre_{pixels}} = \frac{nombre_{cyles \mbox{ } cpu}}{H \times W} \nonumber\end{equation}
Le nombre de cycles cpu est mesuré à l'aide de primitives spécifiques $PowerPC$ et qui permettent de lire des registres des compteurs CPU. Les bancs de tests ont été conçus de sorte à mesurer la performance brute ainsi que d'autres métriques spécifiques aux architectures parallèles qui sont des métriques de passage à l'échelle ( \emph{scalability} ) notamment l'accélération ( \emph{speedup} ), et l'efficacité ( \emph{efficiency} ). Ces deux quantités ont été mesurées en se basant sur la loi d'Amdahl \cite{amdahl}. Nous avons choisi ce modèle pour sa simplicité, d'autres modèles plus précis qui prennent en compte l'équilibrage de charge et le surcout existent tels que celui de Karp et Flatt \cite{karp_flatt}, mais nous ne les avons pas utilisés dans le cadre de cette étude. 
\subsection{Méthode et plateforme de mesure}
La plate-forme sur laquelle a été menée l'évaluation des performance est un Blade Server QS20 d'IBM cadencé à 3.2 GHz. Cette lame contient deux processeurs Cell, chacun relié à une mémoire de 512 MB, ce qui donne au total 1 Go de \emph{RAM} disponible. Dans notre expérimentation, un processeur Cell sur deux est utilisé. Le compteur utilisé pour la mesure de la performance est appelé \emph{timebase} et il est cadencé à 14.318 MHz. L'OS installé est un Linux, distribution Fedora 7. Le développement à été réalisé sur le \emph{Cell SDK 3.0} et les compilateurs utilisés sont \emph{ppu-gcc} et {spu-gcc}, ce qui implique une compilation du type \emph{dual-source} un pour le SPE et un pour le PPE.\\
\indent La méthodologie de mesure adoptée est en accord avec les systèmes de vision par ordinateur. Nous avons simulé le cas d'un flux d'images continu en ajoutant une boucle externe autour de l'implémentation sur une image et avons mesuré le nombre de cycles moyen pour une image. Ceci permet de négliger le surcoût induit par la création et la synchronisation des \emph{threads}. L'interface utilisateur permet de faire varier plusieurs paramètres, notamment la taille de l'image, la taille de la tuile et le nombre de SPEs.
\subsection{Comparaison des schémas de parallélisation}
\begin{figure}[htb]
	\centering
  \includegraphics[width= 0.8\columnwidth]{Chapter5/figures/BenchCompareModels}
	\caption{Comparaison des modèles en cycles par point : SPMD correspond à la version data parallel, PIPELINE au pipeline, HCHAIN+HPIPE au chainage d'opérateurs par paires et HCHAIN+HPIPE au chaînage complet et fusion d'opérateurs par paires }
	\label{comparemodels}
\end{figure}
La figure \ref{comparemodels} donne une comparaison entre les différents modèles de déploiement de l'algorithme de Harris sur le processeur Cell décrits précédemment. Ce que l'on peut observer en premier lieu, c'est que le schéma \emph{pipeline} donne les plus mauvaises performances. Ce résultat était attendu car cette version n'exploite pas entièrement le parallélisme de données offert par l'architecture du Cell. Les autres résultats correspondent également à nos attentes : 
\begin{itemize}
\item Nos techniques d'optimisation des accès mémoires améliorent sensiblement la performance globale car la version la plus rapide est celle où les fonctions sont composées deux par deux  et où tous les opérateurs sont exécutés sur le même SPE ( remplacement des DMA par des \texttt{\textbf{load/store}} ).
\item Les versions dans lesquelles les DMA inter-SPE sont utilisés possèdent de bonnes performance : à titre d'exemple la version sans composition de fonction et en regroupant deux opérateurs par SPE est plus rapide que la version \emph{data parallel}.
\item De plus, la version avec composition de fonction où les paires d'opérateurs sont sur deux SPEs différents est presque aussi rapide que la même version où tous les opérateurs sont placés sur le même SPE. Ceci prouve que la bande-passante inter-SPE est comparable à celle des instructions \texttt{\textbf{load/store}} sur les mémoires locales.
\end{itemize}
Dans ce qui précède nous avons pu constater que le placement optimal d'un graphe de fonctions sur un processeur Cell n'était pas forcément évident lorsqu'il s'agit d'un algorithme de traitement d'image comme dans notre cas. D'une part, on a vu que le choix d'un schéma complètement \emph{data parallel} n'était pas forcément optimal, et qu'un schéma mixte avec une partie \emph{pipeline} imbriquée dans un schéma \emph{data parallel} donnait un résultat aussi bon qu'un schéma basé purement sur le parallélisme de données. Ceci est en grande partie possible, grâce à un niveau supplémentaire de parallélisme au niveau des transferts. En effet, en favorisant les communications inter-SPE on profite pleinement de la bande passante du bus interne (204.8 Go/s) qui peut gérer jusqu'à 12 transferts en parallèle, alors que dans un schéma de transfert où plusieurs SPEs communiquent avec la mémoire centrale les commandes sont forcément sérialisées, et dans ce cas là la bande passante est limitée à 25.6 Go/s. 
\subsection{Influence de la taille de la tuile}
\begin{figure}[htb]
	\centering
  \includegraphics[width= 0.8\columnwidth]{Chapter5/figures/SizeTile}
	\caption{Influence de la taille de la tuile sur la performance pour la version chainage entier et fusion d'opérateurs par paires 1 SPE (Fig. \ref{harris-hchainfpipe})}
	\label{tilesize}
\end{figure}
Comme il est démontré dans \cite{cell_bw02} et \cite{cellnoc} la taille du bloc de données transféré possède une influence sur la bande passante du bus interne. Dans notre domaine d'application, la bande passante mémoire est critique pour la performance globale de l'application, car les algorithmes de traitement d'image sont généralement caractérisés par un ratio transfert/calcul important. La figure \ref{tilesize} nous montre que la taille de tuile donnant les meilleures performances est 16Ko, et ceci pour deux raisons:
\begin{itemize}
\item Une taille de bloc de 16 Ko ou multiple de cette taille, garantit une bande-passante maximale sur le bus interne.
\item Comme nous l'avons vu dans la discussion sur la forme de la tuile, il est démontré que plus la taille de la tuile est grande moins il y a de pixels redondants, et la quantité de données totale transférées est minimisée en ce qui concerne les noyaux de convolution.
\end{itemize}
\subsection{Analyse des résultats}
\begin{table}
\begin{tabular}{|c||c|c|c|}
\hline
\rowcolor{medium-gray} \textbf{Modèle} & \textbf{Opérateur} & \textbf{Nombre de Cycles} & \textbf{Accélération} \\
\hline
\hline
\textbf{Halfchain} & Sobel+Mul+\texttt{LOAD/STORE}&119346&  1\\
\hline
\rowcolor{light-gray}\textbf{Halfchain+Halfpipe} & Gauss+Coarsity+\texttt{LOAD/STORE}&188630& 1 \\
\hline
\textbf{Halfchain} & (Sobel $\circ$ Mul)+\texttt{LOAD/STORE}&16539& 7.2 \\
\hline
\rowcolor{light-gray}\textbf{Halfchain+Halfpipe} &(Gauss $\circ$ Coarsity)+\texttt{LOAD/STORE}&504309& 3.5 \\
\hline
\end{tabular}
\caption{Apport de la composition de fonction à la performance, les versions de référence sont celles ayant une accélération de 1}
\label{tabspuperf}
\end{table}

La comparaison de la performance globale des différents schémas d'implémentation ne suffit pas à prouver que l'optimisation de l'utilisation de la mémoire est le facteur principal de l'amélioration de la performance. Dans le but de donner une analyse plus précise et donc plus complète, nous avons effectué des mesures au niveau du SPE où nous avons estimé le gain apporté par la composition des fonctions et les transferts DMA inter-SPE. Le tableau \ref{tabspuperf} donne en nombre de cycles \emph{SPU} les versions ou les opérateurs ne sont pas composés deux à deux et celle ou ils le sont. Comme on peut le constater, cette optimisation permet d'avoir une accélération pouvant atteindre $\times 7,2$. Cette technique, permet une utilisation maximale des registres au détriment de la mémoire et par conséquent elle est limitée par la taille du banc de registres du processeur (128 registres pour le SPE). D'autre part l'optimisation de la localité des données en chainant les opérateurs d'un même SPE est certe bénéfique, mais elle est limitée par le niveau de hiérarchie mémoire juste au dessus des registres qui est la mémoire locale, limité lui à 256 Ko.
\subsection{Mesure des métriques de passage à l'échelle}

\begin{figure}[htb]
	\centering
  \includegraphics[width= 0.8\columnwidth]{Chapter5/figures/Speedup}
	\caption{Mesure de l'accélération des versions data parallel (SPMD) et chaînage complet (HPIPE+FCHAIN) en fonction du nombre de SPEs}
	\label{speedup}
\end{figure}

\begin{figure}[htb]
	\centering
 \includegraphics[width= 0.8\columnwidth]{Chapter5/figures/Efficiency}
	\caption{Mesure de l'Efficacité des versions data parallel (SPMD) et chaînage complet (HPIPE+FCHAIN) en fonction du nombre de SPEs}
	\label{efficiency}
\end{figure}

Lorsque l'on mesure les performance d'une machine parallèle on ne peut se contenter de mesurer les performances temporelles brutes. Il est également intéressant de s'attarder sur d'autres paramètres qui permettent de comparer deux architectures parallèles ou alors d'isoler un goulot d'étranglement qui limite la performance. Ce que l'on veut évaluer est le passage à l'échelle ou \emph{scalability} qui permet de mesurer sur un système à plusieurs processeurs, le rendement des $N$ unités par rapport à une seul. Les métriques les plus connues sont l'accélération (\emph{Speedup}) et l'efficacité (\emph{Efficiency}), et sont définies comme suit:
\begin{equation}
Speedup = \frac{\mbox{Temps d'Exécution sur 1 Processeur}}{\mbox{Temps d'Exécution sur $P$ Processeurs}} \nonumber
\end{equation}
\begin{equation}
Efficiency = \frac{\mbox{Temps d'Exécution sur 1 Processeur}}{P \times \mbox{Temps d'Exécution sur $P$ Processeurs}}\nonumber
\end{equation}
Ces mesures sont essentielles pour l'adaptation d'un code séquentiel à une machine parallèle. Elles permettent d'une part de détecter les limitations d'un système parallèle pour la parallélisation d'un code. Par exemple, on peut évaluer la complexité due au contrôle des \emph{threads} et à leur synchronisation. D'autre part, on peut mesurer l'efficacité du réseau d'interconnexion dans la distribution des données de manière parallèle aux différents noeuds. Elles peuvent également justifier l'ajout d'une quantité supplémentaire de processeurs si cela n'altère pas la performance en induisant un coût de gestion supplémentaire. \\
\indent D'après les figures \ref{speedup} et \ref{efficiency} on peut constater que notre implémentation de l'algorithme de Harris passe bien à l'échelle sur l'architecture du Cell. En effet, on peut noter dans les deux cas étudiés à la fois, une accélération proche de $P$ le nombre de SPEs et une efficacité proche de 1. On peut tout de même noter certaines différences entre les deux implémentations étudiées, d'une part la version SPMD et d'autre part la version avec composition de fonctions et chainage sur le même SPE. La première implique une pression importante sur le bus de données car presque tous les lectures/écritures mémoire se font sur la mémoire externe. Au contraire, la seconde version minimise le trafic vers la mémoire externe. Ceci explique la différence au niveau de l'efficacité et permet de déduire que plus le ratio transfert/calcul est grand et moins le passage à l'échelle est bon.

\section{Conclusion}
Dans ce chapitre nous avons mis en oeuvre la parallélisation d'un code de traitement d'images sur le Cell. L'algorithme qui a été choisi est le détecteur de points d'intérêts de \emph{Harris} qui est à la fois représentatif des traitements bas-niveau, présent dans plusieurs traitements plus complexes et permet plusieurs schémas de parallélisation et d'optimisation. Plusieurs techniques d'optimisation on été présentées : certaines sont spécifiques au traitement du signal et des images et sont applicables à d'autres architectures. D'autres techniques sont plus spécifiques et relèvent de l'adéquation entre l'algorithme et l'architecture spécifique du processeur Cell. L'accent à été mis sur l'optimisation des transferts mémoire à plusieurs niveaux et de plusieurs manières, car elles constituent le principal facteur limitant la performance pour notre domaine d'application. Plusieurs squelettes de parallélisation ont été mis en oeuvre afin d'avoir une idée globale de squelette de parallélisation qui donne les meilleures performances. Les performances ont été mesurées de plusieurs manières et à différents niveaux : au niveau des noyaux de calcul une estimation et une mesure des gains apportés par les optimisation bas-niveau ont été effectuées. Au niveau des transferts, une mesure de la bande passante fut menée en fonction du schéma de parallélisation, et l'influence de la taille et de la forme de la tuile de calcul ont été étudiées. Enfin, des métriques de passage à l'échelle (\emph{Scalability}) ont été utilisées afin de mesurer l'efficacité de la parallélisation de notre algorithme sur le processeur Cell.
Les performances obtenues lors de cette étude ont été obtenues après un temps de développement important. Elles sont le fruit d'une méthodologie d'optimisation itérative et de l'acquisition d'une expertise sur l'architecture du Cell. Dans le cadre de développement d'applications industrielles, des contraintes de temps de livraison et de maintenabilité du code se posent. L'outil de base n'est pas adapté pour répondre à de telles contraintes. C'est pour ces raisons que des outils d'aide au portage de code sur le processeur Cell ont vu le jour.
%Une étude comparative complète à été effectué toujours en utilisant le même algorithme de référence. Les architectures parallèles émergentes du type multi-core GPP, Cell et GPU on été comparées selon plusieurs critère : modèle de programmation, performance temporelle pure et efficacité énergétique. La dernière comparaison permet de faire un choix d'un coupe architecture/modèle de programmation pour les application de traitement d'images bas-niveau.


