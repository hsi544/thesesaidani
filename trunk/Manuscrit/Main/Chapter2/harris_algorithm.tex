
Dans ce chapitre nous rentrons dans le vif du sujet, à savoir, la parallélisation de code de traitement d'images pour le processeur Cell. L'algorithme considéré est celui de la détection de points d'intérêts de Harris. Le choix de cet algorithme s'est fait selon plusieurs critères qui sont les suivants:
\begin{itemize}
\item C'est un algorithme de traitement d'images bas niveaux qu'on retrouve dans plusieurs applications plus complexes, comme la reconstructions 3D et le suivi d'objets.
\item Il est composé de blocs de traitement de base qui sont représentatifs des algorithmes bas niveau comme les opérateurs de convolution est les opérateurs point à point.
\item C'est un algorithme qui ne peux pas s'exécuter en temps réel sans optimisations spécifiques.
\end{itemize}
Etant donné les caractéristiques de l'architecture du Cell ainsi que celles de l'algorithme le but est de trouver la meilleure implémentation qui permet d'exploiter au mieux les dispositifs haute performance de l'architecture. Le processeur Cell est un vrai concentré de dispositifs accélerateurs parmi lesquels les unités SPE purement SIMD, les controleurs DMA permettant un parallélisme entre transferts mémoire et tâches de calculs ainsi que la multiplicité des coeurs qui permettent de répartir la charge de calcul de plusieurs manières possible soit sous frome de parallélisme de donnée uniquement, ou alors de parallélisme de tâches ou un mélange des deux.
\section{Algorithme de Harris}
La détection de point d'intéréts de Harris et Stephen \cite{harris_corner} est utilisée dans les systèmes de vision par ordinateur pour l'extraction de connaissance comme la détection de mouvement, correspondance d'images, suivi d'objets, reconstruction 3D et reconnaissance d'objets. Cet algorithme fut proposé pour palier aux manques de l'algorithme de Moravec \cite{moravec} qui était sensible au bruit et pas invariant à la rotation. Un coin peut être définit comme étant l'intersection de deux contours lorsque un point d'intérêt peut être définit comme un point ayant une position bien déterminée qui peut être détecté de manière robuste. Ainsi, le point d'intérêt peut être un coin mais aussi un point isolé d'intensité maximum ou minimum localement, une terminaison de ligne ou un point de courbe où la courbure est localement maximale.

\subsection{Description de l'Algorithmes}
Si l'on considère des zones de l'images de dimensions $u \times v$ (dans notre cas $3 \times 3$ dans une images 2-dimensions en niveaux de gris $I$  qui est décalée de $(x, y)$, l'opérateur de Harris est basé sur l'estimation de l'autocorrélation locale $S$ dont l'équation est la suivante:
\begin{equation}
\label{eq_00}
S(x,y) =\sum\limits_{u}\sum\limits_{v} w(u,v)\left( I(u,v) - I(u-x,v-y) \right)^{2}
\end{equation}
Par l'approximation de $S$ avec une série de Taylor du second ordre la matrice de Harris $M$ est donnée par :
\begin{equation}
\label{eq_01}
M=\sum\limits_{u}\sum\limits_{v}w(u,v)\begin{bmatrix}I_{x}^{2}& I_{x}I_{y}\\I_{x}I_{y}&I_{y}^{2}\end{bmatrix}
\end{equation}
Un point d'intérêt est caractérisé par une large variation de $S$ dans toutes les directions du vecteur $(x,y)$. En analysant les valeurs propres de $M$, cette caractérisation peut être exprimée de la manière suivante. Soit $\lambda_{1}$, $\lambda_{2}$ les valeurs propres de $M$:
\begin{enumerate}
	\item Si $\lambda_{1}$ $\approx 0$ et $\lambda_{2}$ $\approx 0$ alors il n'y a pas de point d'intérêt au pixel $(x,y)$.
	\item Si $\lambda_{1}$ $\approx 0$ and $\lambda_{2}$ a une grande valeur positive alors un contour est retrouvé.
	\item Si $\lambda_{1}$ and $\lambda_{2}$ sont deux grandes valeurs positives distinctes alors un coin est détecté.
\end{enumerate}
Harris et Stephens ont constaté que le calcul des valeurs propres est coûteux car elle requiert le calcul d'une racine carrée, et ont proposé à la place l'algorithme suivant : 

\begin{enumerate}
\item Pour chaque pixel $(x, y)$ de l'image calculer la matrice de corrélation $M$:\\
\begin{equation}
\label{eq_03}
M=\begin{bmatrix} S_{xx} & S_{xy} \\ S_{xy} & S_{yy} \end{bmatrix}; \mbox{où:} S_{xx}=\left(\frac{\partial I}{\partial x}\right)^{2}\otimes w, S_{yy}=\left(\frac{\partial I}{\partial y}\right)^{2}\otimes w, S_{xy} = \left(\frac{\partial I}{\partial x}\frac{\partial I}{\partial y}\right)\otimes w
\end{equation}

Où	$\otimes$ est l'opérateur de convolution $w$ un noyau Gaussien.\\
\item Construire la carte de coarsité en calculant la mesure de coarsité $C(x, y)$ pour chaque pixel $(x, y)$:\\
\begin{equation}
\label{eq_04}
C(x,y)=det(M)-k(trace(M))^{2}
\end{equation}
\begin{eqnarray*}
det(M)=S_{xx}.S_{yy}-S_{xy}^{2}\\
trace(M)=S_{xx}+S_{yy}
\end{eqnarray*}
et $k$ une constante empirique.\\
\end{enumerate}
Une illustration d'une détection de points d'intérêt sur une image 512$\times$512 en niveaux de gris est donné en figure \ref{fig_house}. Afin d'obtenir ce résultat, deux étapes supplémentaires sont nécessaires qui permettent d'extraire une information visuelle à partir de la matrice $C(x,y)$\footnote{Ces étapes ont pour but de visualiser le résultat et ne sont donc pas incluses dans le graphe de l'algorithme}. Ces étapes sont les suivantes :
\begin{enumerate}
\item Seuillage de la carte d'intérêt en mettant toute les valeurs de $C(x,y)$ inférieurs à un seuil donné à zéro.
\item Extraction des maxima locaux en gardant les points qui sont plus grand que tous leurs voisins dans un voisinage 3$\times$3.
\end{enumerate}
\begin{figure}[!htb]
	\centering
\begin{tabular}{cc}
	\includegraphics[width= 0.5\columnwidth]{Chapter2/figures/house1} & \includegraphics[width= 0.5\columnwidth]{Chapter2/figures/house_harris_map}
	\end{tabular}
	\caption{Illustration de la détection de points d'intérêts sur une image niveaux de gris 512$\times$512}
	\label{fig_house}
\end{figure}

\subsection{Détails de l'Implémentation}
\begin{figure}[!htb]
	\centering
	\includegraphics[width= \columnwidth]{Chapter2/figures/harris_NB}
	\caption{Implémentation de l'algorithme de Harris sous forme de graphe flot de données}
	\label{fig_HarrisAlgorithm}
\end{figure}
Les images en niveaux de gris sont typiquement des données stockées une des entiers 8-it non signés et la sortie de l'algorithme de Harris est dans ce cas là un entier 32 bit signé. Toutefois, pour des raisons de limitation du jeux d'instruction du SPU, et afin de garantir une comparaison objective avec les extension Altivec et SSE nous avons choisi le format flottant simple précision pour l'entrée et la sortie de l'algorithme. Dans notre implémentation nous avons divisé l'algorithme en 4 noyaux de traitement : un opérateur de \emph{Sobel} qui représente la dérivée dans les directions horizontale et verticale, un opérateur de multiplication, un noyau de lisage de \emph{Gauss} ($w$ dans l'équation \ref{eq_03} suivi d'un opérateur de coarsité. Nous avons fixé la constante $k$ à zéro (typiquement elle est fixée à 0.04) car ceci n'avait pas d'influence sur le résultat qualitatif. On obtient ainsi le graphe flot de données donné dans la figure \ref{fig_HarrisAlgorithm} qui est représentatif d'un algorithme de traitement d'images bas niveau car il englobe des noyaux de convolution et des opérateurs point à point. Les noyaux de convolution de Sobel ($GradX$ et $GradY$)et le noyau de $Gauss$ sont définis comme suit: 
\begin{eqnarray*}
Grad_{X} = \begin{bmatrix}-1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}; Grad_{Y} = \begin{bmatrix}-1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix}; Gauss = \frac{1}{16}\begin{bmatrix}1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \end{bmatrix}
\end{eqnarray*}
Etant donnée qu'ils consomment plus d'entrée qu'ils ne produisent de sorties, les noyaux de convolution sont le goulot d'étranglement de l'algorithme car elles augmentent considérablement le trafic mémoire. Au vu de la nature des calculs effectués dans les différents noyaux, et qui sont très simples généralement (une suite de multiplications/accumulation) on peut considérer que les instructions mémoire sont prépondérantes dans l'application et de ce fait on peut qualifier l'algorithme de \emph{memory-bounded problem} (problème limité par la mémoire). C'est pour cela que les efforts d'optimisation sur l'algorithme de Harris sont faites par l'optimisation des accès mémoire à différent niveaux de la hiérarchie mémoire du processeur Cell. 
\subsection{Exploitation du Parallélisme et Optimisations Multi-niveau}
Les techniques d'optimisation démontrées ici sont multiples et variées. Certaines sont de nature algorithmique et relèvent plutôt du domaine du traitement du signal et des images. D'autres techniques génériques relèvent plutôt du domaine de l'optimisation logicielle est qu'on retrouve par fois dans certains compilateurs optimisants. Les techniques précédentes sont générales et peuvent être appliqués à la majorité des processeurs généralistes car elle ne tiennent pas compte des aspects spécifiques d'une architecture donnée. Par contre, des optimisations de plus haut niveau et qui sont spécifiques à l'architecture particulière du Cell ont aussi été employées. Celles-ci ne sont généralement pas reproductibles sur d'autres architectures parallèles car elle relèvent plus d'une adéquation entre l'algorithme et l'architecture qui contient certains dispositifs qui n'existent que sur le Cell et des fois elles résultent de contraintes de programmation spécifiques au Cell comme la taille limitée des mémoire locale des SPEs et par conséquent la gestion logicielle de l'utilisation de la mémoire.
\subsubsection{Techniques Spécifiques au Domaine}
Ces optimisations relèvent plutôt du domaine du traitement du signal et des images. Elles peuvent donc être appliquées à plusieurs algorithmes et sur n'importe quelle architecture. Celles que nous avons utilisé concernent les noyaux de convolution et sont : la séparabilité, le chevauchement (overlapping) et la factorisation des calculs.
\paragraph{Séparabilité des Noyaux}
Cette optimisation consiste à exploiter le fait que les noyaux de convolution 2D de \emph{Sobel} \emph{Gauss} soient séparables en deux filtre de convolution 1D. Ainsi, la matrice des coefficients peut être exprimée comme un produit de deux vecteur comme l'illustre les équations suivantes :
\begin{eqnarray*}
Grad_{X} =  \frac{1}{  8} \begin{bmatrix} -1 &  0 &   1  \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix} =    \frac{1}{8} \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix} \times  \begin{bmatrix} 1 & 2 & 1 \end{bmatrix};
Grad_{Y} =  \frac{1}{  8} \begin{bmatrix} -1 & -2 & -1   \\  0 & 0 & 0 \\  1 & 2 & 1 \end{bmatrix}  =   \frac{1}{8} \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} \times  \begin{bmatrix} -1 & 0 & 1 \end{bmatrix}\\
Gauss      =  \frac{1}{16} \begin{bmatrix}  1 &  2 &   1  \\  2 & 4 & 2 \\  1 & 2 & 1 \end{bmatrix} = \frac{1}{16}\begin{bmatrix}1 \\ 2 \\ 1 \end{bmatrix} \times \begin{bmatrix}1  & 2 & 1 \end{bmatrix}
\end{eqnarray*}
Lorsqu'on sépare les noyaux de convolutions le calcul se fait en deux passes une pour chaque vecteur. Grâce à la séparabilité des noyaux on arrive à réduire le nombre d'instructions mémoire ainsi que la complexité arithmétique. La comparaison est illustrée dans le tableau \ref{tab_sepa}
\begin{table}
\begin{tabular}{|c|c|c|c|c|}
\hline
Opérateur & Nombre &\texttt{MUL} &\texttt{ADD} & Total \\
\hline
\emph{Sobel} & 2 & 3 & 5 &16 \\
\hline
\emph{Mul} & 3 & 1 & 0 &3 \\
\hline
\emph{Gauss} & 3 & 1 & 0 &3 \\
\hline
\end{tabular}
\caption{Réduction de la complexité mémoire et arithmétique par séparabilité des noyaux}
\end{table}

%Operator Number MUL ADD Total%Complexity without optimization%Sobel 2 3 5 16%Mul 3 1 0 3%Gauss 3 6 8 42%Coarsity 1 2 1 3%Total - 29 35 64%Complexity with optimizations%Sobel 2 0 5 10%Mul 3 1 0 3%Gauss 3 0 6 18%Coarsity 1 2 1 3%Total - 4 30 34

